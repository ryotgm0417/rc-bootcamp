{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "# Chapter 3. エコーステートネットワーク基礎\n",
                "\n",
                "[en]: #\n",
                "# Chapter 3. Basics of Echo State Network"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "この章では、リザバー計算 (Reservoir Computing; RC)の文脈で頻繁に使用されるエコーステートネットワーク (Echo State Network; ESN)の基礎と標準的な設定を学びます。\n",
                "\n",
                "[en]: #\n",
                "In this chapter, we will study the standard setting and basic implementation of echo state networks (ESNs), which are frequently used in the context of reservoir computing (RC)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "## 前書き\n",
                "\n",
                "[en]: #\n",
                "## Introduction"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "ESNは、H. Jaeger<sup>[1]</sup>(2001) によって提案されたリカレントニューラルネットワーク (Recurrent Neural Network; RNN)の一種です。\n",
                "RNNは深層学習で一般的に用いられるフィードフォワードネットワークとは異なり、その再帰結合によって特徴づけられます。\n",
                "またRCでは一般的なRNN (Long-Short-Term Memory; LSTMやGated Recurrent Unit; GRU等)とは異なり、その再帰結合は固定化されたまま、リードアウト層 (readout layer)と呼ばれる外部のパラメータのみが調整されます。\n",
                "このような設定で使用されるRNNはリザバー (reservoir)と呼ばれ、RCでは最も単純なセットアップとしてESNがリザバーとして用いられます。\n",
                "\n",
                "[en]: #\n",
                "\n",
                "The ESN is a type of recurrent neural network (RNN) proposed by H. Jaeger<sup>[1]</sup> (2001).\n",
                "Unlike feedforward networks commonly used in deep learning, RNNs are characterized by their recurrent connections.\n",
                "Additionally, unlike typical RNNs (e.g., long short-term memory or gated recurrent unit), the recurrent connections in ESNs remain fixed, and only the external readout layer parameters are trained.\n",
                "The RNN used in this configuration is referred to as a **reservoir**, and an ESN is primarily used in RC studies."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "RCはリザバーを用いて様々な時系列タスクを解くフレームワークで、教師あり時系列学習に分類されます。\n",
                "通常時系列は、毎ステップ入力としてリザバーの内部状態に与えられ、再帰結合と非線形関数に従ってその内部状態が時間発展します。\n",
                "またRCでは先述のとおり、再帰結合の調整を伴わず、外部のリードアウト層のみの学習だけで多様な非線形な入出力処理を実現します。\n",
                "ここで重要なポイントとして、リザバーの内部状態の高次元性と非線形性が挙げられます。\n",
                "一般にリザバーは高次元であるほど高い情報処理能力を有しますが、それは履歴として入力の情報を蓄積できるだけの(記憶)容量を、高次元な設定では確保しやすいからです。\n",
                "また非線形関数をその活性化関数として有すると、非線形変換が施された多様な入力の情報を保持します。\n",
                "他にもESNを用いたRCについて一般に以下の特長が挙げられます。\n",
                "- 非線形な活性化関数を使用 (この点はRNNと同様)\n",
                "- 通常再帰結合のパラメータをランダムに設定し**固定して**使用\n",
                "- **外側のリードアウト層のみを学習**するため誤差逆伝搬法等と比して低計算コスト\n",
                "- 同じリザバーを再利用しながら複数のリードアウト層により**マルチタスキング** (Multitasking; 文脈によってはMultiplexingとも呼ばれる)が容易に可能\n",
                "\n",
                "[en]: #\n",
                "RC is a framework that uses reservoirs to solve various time-series tasks and is a supervised learning method for time-series data.\n",
                "Normally, a time series is fed into the reservoir at each step and the internal state evolves via the recurrent connections and nonlinear activations.\n",
                "RC can realize many nonlinear input-output mappings by training only the external readout without changing the recurrent connections.\n",
                "\n",
                "An important feature is the high dimensionality and nonlinearity of the reservoir.\n",
                "In general, it can store a greater amount of input information as history in a higher-dimensional setting.\n",
                "Additionally, the nonlinear activation function produces various nonlinear transformations of the input, which are also stored and exploited as computational resources.\n",
                "\n",
                "Other important features of RC include:\n",
                "- Using an activation function for nonlinearity (similar to RNNs)\n",
                "- Randomly fixing the recurrent connection weights\n",
                "- Simply training the readout, resulting in a lower training cost compared to backpropagation\n",
                "- Multitasking (multiplexing in some contexts) by reusing the same reservoir with multiple readouts"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"text-align: center; width: 750px; margin: auto; background-color: #f8f9fa; padding: 10px; border-radius: 10px;\">\n",
                "\n",
                "![fig_3_1.webp](../assets/fig_3_1.webp)\n",
                "\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[figc]: #\n",
                "\n",
                "[ja]: #\n",
                "図1 (左)ESNの基本的構成。 (右) タコの腕を模したシリコンベースのソフトアームによる物理リザバー計算<sup>[3]</sup>。\n",
                "\n",
                "[en]: #\n",
                "Figure 1 (Left) Basic structure of ESN. (Right) An example of PRC platform with a silicone-based soft robotic arm inspired by the octopus<sup>[3]</sup>.\n",
                "\n",
                "[END]: #\n",
                "\n",
                "[/figc]: #"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "内部結合の調整を伴わないランダムニューラルネットワークを使用するため、原理的には非線形性を有する大自由度のシステム (大自由度非線形力学系)であれば、リザバーの物理的実装の候補となります。\n",
                "これらの実装は、実際に物理リザバー計算 (Physical Reservoir Computing; PRC)の分野で盛んに研究されています<sup>[3]</sup>。\n",
                "例えば水中のタコの足の動きをセンシングし、その時系列データを計算資源として活用して、パリティービットチェックタスク等の非線形処理を実現できます。\n",
                "\n",
                "[en]: #\n",
                "Since it uses a random neural network without adjusting internal connections, any sufficiently nonlinear high-dimensional system can in principle serve as a physical reservoir.\n",
                "These implementations are being actively studied in the field of physical reservoir computing (PRC)<sup>[3]</sup>.\n",
                "For example, complicated dynamics obtained by sensing the movement of an octopus's arms underwater can be used to solve various nonlinear tasks by exploiting the intrinsic computational capability."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "### ESNの定式化\n",
                "\n",
                "[en]: #\n",
                "### Typical formulation of the ESN"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "\n",
                "ここまで説明されたように、RCは入出力関係を学習する手法で、機械学習の一種である教師あり学習に分類されます。\n",
                "学習の要点をまとめると下記のとおりになります。\n",
                "\n",
                "0. 時系列入出力データの用意\n",
                "1. 適切なESNモデルの選択ならびにハイパーパラメータの調整\n",
                "2. 初期値の影響がなくなるまで系を時間発展 (ウォッシュアウト; washout)\n",
                "3. 学習アルゴリズムの選択およびリードアウト層の調整\n",
                "4. 入力データに対する出力データのサンプリング\n",
                "5. 訓練誤差ならびに汎化誤差の評価\n",
                "\n",
                "以下ESNの定式化のため、下記の表記を導入しながらより詳細を説明します。\n",
                "なおわかりやすさのため以降、離散時間上で定義される時系列や関数には角括弧 $[\\cdot]$、連続時間上で定義される時系列には丸括弧$(\\cdot)$を用います。\n",
                "教科書や論文等によっては丸括弧$(\\cdot)$ や添字表記を用いる場合もありますが、意味は同じです。\n",
                "\n",
                "- $k\\in\\mathbb{Z}$: 離散時間\n",
                "- $u[k] \\in \\mathbb{R} $: 1次元入力\n",
                "- $x[k] \\in \\mathbb{R}^{N}$: リザバーの内部状態\n",
                "- $g: \\mathbb{R}^{N}\\to \\mathbb{R}$: リードアウト層の関数\n",
                "- $y[k] \\in \\mathbb{R} $: 入力時系列$\\{u[0], u[1],~\\ldots,~u[k]\\}$ によって生成される目標出力\n",
                "\n",
                "RCでは$\\hat{y}[k]=g(x[k])\\approx y[k]$ によって目標出力$y[k]$ の近似を目指します。\n",
                "また、後の章でも改めて取り上げますが、$u[k]$ に対する**エコーステート性 (Echo State Property; ESP)** は、そのリザバーの情報処理能力を十分活用する上で重要な性質で、次の式を満たす系の性質を指します。\n",
                "\n",
                "[en]: #\n",
                "As explained so far, RC is a method of learning input-output relationships and is classified as supervised learning, a type of machine learning.\n",
                "The main points are summarized as follows:\n",
                "\n",
                "0. Prepare time-series input-output data.\n",
                "1. Select the type of ESN model and design the hyperparameters.\n",
                "2. Run the system until the effect of the initial state is washed out.\n",
                "3. Select a learning algorithm and optimize the readout.\n",
                "4. Sample the output according to the input data.\n",
                "5. Evaluate the training and validation (generalization) errors.\n",
                "\n",
                "The listed notation is used in the following description:\n",
                "- $k\\in\\mathbb{Z}$: Discrete time\n",
                "- $u[k]\\in\\mathbb{R}$: One-dimensional input\n",
                "- $x[k]\\in\\mathbb{R}^N$: Reservoir states\n",
                "- $g: \\mathbb{R}^{N}\\to \\mathbb{R}$: Readout function\n",
                "- $y[k]$: Target output generated by the input sequence $\\{u[0], u[1],~\\ldots,~u[k]\\}$\n",
                "\n",
                "We use square brackets $[\\cdot]$ for time series or functions defined in discrete time and parentheses $(\\cdot)$ for those defined in continuous time for clarity.\n",
                "The aim of RC can be stated as approximating $y[k]$ by $\\hat{y}[k]=g(x[k]).$\n",
                "It should be noted that the reservoir should have the **echo state property (ESP)** for $u[k]$: there exists an **echo function** $\\varphi$ that asymptotically determines the internal state from the input history:\n",
                "\n",
                "[END]: #\n",
                "$$\n",
                "\\begin{align*}\n",
                "x[k] = \\varphi(u[k], u[k-1],~\\ldots)\n",
                ".\\end{align*}\n",
                "$$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "換言すれば、ESPを満たす系は十分時間が経てばその内部状態の初期値に関わらず入力時系列$u[k]$によって内部状態がある関数$\\varphi$で定まる値に収束します (漸近的に初期値$x[0]$の情報を消失)。\n",
                "またこの$\\varphi$は **エコー関数 (echo function)** と呼ばれます。\n",
                "\n",
                "ここでESPはリザバーの要件として重要で、わかりやすさのためまず先に取り上げますが、必ずしも全てのRCにおいて必須ではない点に注意してください。\n",
                "例えば後の章では初期値鋭敏性を有するカオス系を活用するRCについて学びますが、カオス系ではESPは満たされません。\n",
                "また近年ではESPの拡張としてエコーインデックス (Echo Index; EI)が提案されており、全体としてESPは満たさないが、局所的に安定な応答を複数もつ系の場合にも対応しています<sup>[4]</sup>。\n",
                "このようにESPを含めリザバーの要件に関する研究は現在も継続的に進められています。\n",
                "\n",
                "[en]: #\n",
                "In other words, the system asymptotically diminishes the effect of the initial conditions.\n",
                "\n",
                "Although ESP is an important requirement for reservoirs and is introduced here for clarity, it is not necessarily essential for all RCs.\n",
                "For example, in later chapters, we will study RCs that utilize chaotic systems with sensitivity to initial conditions, which do not satisfy ESP.\n",
                "The echo index (EI) has been proposed as an extension of ESP, which can handle systems with multiple locally stable responses even if they do not satisfy ESP globally<sup>[4]</sup>.\n",
                "In this way, research on reservoir requirements, including ESP, is ongoing."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "#### 入力層\n",
                "\n",
                "[en]: #\n",
                "#### Input layer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "- $W^\\mathrm{in} \\in \\mathbb{R}^{N}$: 入力層の結合\n",
                "\n",
                "$W^\\mathrm{in}$は通常固定化され、しばしば一様分布$\\mathcal{U}([-\\sigma, \\sigma])$を用いてランダムに生成されます。\n",
                "またこの際使用される$\\sigma\\in \\mathbb{R}$はしばしば入力スケールと呼ばれます。\n",
                "\n",
                "[en]: #\n",
                "- $W^\\mathrm{in} \\in \\mathbb{R}^{N}$: Input weight (fixed)\n",
                "\n",
                "Input weights are often randomly sampled from a uniform distribution $\\mathcal{U}([-\\sigma, \\sigma])$, where $\\sigma \\in \\mathbb{R}$ is known as the **input-scale** coefficient."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "#### リザバー層\n",
                "\n",
                "[en]: #\n",
                "#### Reservoir layer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "- $x[k] \\in \\mathbb{R}^{N}$: 時刻$t$における内部状態\n",
                "- $W^\\mathrm{rec} \\in \\mathbb{R}^{N\\times N}$: 内部結合\n",
                "\n",
                "$W^\\mathrm{rec}$は多くの場合**スペクトル半径** $\\rho(W^\\mathrm{rec})$ によって制御されます。\n",
                "これは$W^\\mathrm{rec}$ の固有値の絶対値の最大値です (固有値が複素数を取りうる点に注意してください)。\n",
                "このスペクトル半径の大小により入力に対する記憶特性が著しく変化します。\n",
                "\n",
                "[en]: #\n",
                "- $x[k] \\in \\mathbb{R}^{N}$: Internal units (reservoir states)\n",
                "- $W^\\mathrm{rec} \\in \\mathbb{R}^{N \\times N}$: Internal weight matrix (fixed)\n",
                "\n",
                "$W^\\mathrm{rec}$ is often controlled by the **spectral radius** $\\rho(W^\\mathrm{rec})$, which is the maximum absolute value of the eigenvalue of $W^\\mathrm{rec}$."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "#### リードアウト層 (1タスク)\n",
                "\n",
                "[en]: #\n",
                "#### Readout layer (for a single task)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "- $\\hat{y}[k] \\in \\mathbb{R}$: 出力時系列\n",
                "- $W^\\mathrm{out} \\in \\mathbb{R}^{N + 1}$: 出力層の結合パラメータ\n",
                "\n",
                "$W^\\mathrm{out}$は学習データによって調整されます。\n",
                "なお$N$ではなく$N+1$次元となっているのは、後述しますが定数項 (バイアス)に相当する項が増えているためです。\n",
                "\n",
                "[en]: #\n",
                "- $\\hat{y}[k] \\in \\mathbb{R}$: Output time series\n",
                "- $W^\\mathrm{out} \\in \\mathbb{R}^{N + 1}$: Output weight (tuned)\n",
                "\n",
                "Note that the dimension of the reservoir state is augmented with a bias of 1 for the optimization."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "#### ESNの時間発展と出力\n",
                "\n",
                "[en]: #\n",
                "#### Time evolution and output of ESN"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "内部状態$x[k]$と出力$\\hat{y}[k]$の時間発展は以下の式で表されます。\n",
                "\n",
                "[en]: #\n",
                "The internal state and output are defined as follows:\n",
                "\n",
                "[END]: #\n",
                "$$\n",
                "\\begin{align*}\n",
                "x[k+1] &= f(W^\\mathrm{rec}x[k] + W^\\mathrm{in} u[k+1]) \\\\\n",
                "\\hat{y}[k] &= {W^\\mathrm{out}}^\\top [1 ; x[k]] \\\\\n",
                "&= {W^\\mathrm{out}}^\\top {[1 \\quad x_1[k] \\quad \\cdots \\quad x_{N}[k]]}^\\top \\\\\n",
                "&= W^\\mathrm{out}_0 + \\sum_{i=1}^{N} W^\\mathrm{out}_i x_{i}[k]\n",
                ",\\end{align*}\n",
                "$$\n",
                "\n",
                "[ja]: #\n",
                "ここで$f:\\mathbb{R}^N\\to \\mathbb{R}^N$は活性化関数と呼ばれ、ESNではよく$\\tanh$が使用されます ($f(x)=[\\tanh(x_1[k]) \\quad \\cdots \\quad \\tanh(x_N[k]) ]^\\top$)。\n",
                "\n",
                "[en]: #\n",
                "with activation function e.g., $f(x)=[\\tanh(x_1[k]) \\quad \\cdots \\quad \\tanh(x_N[k])]^\\top$."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "#### その他の形式\n",
                "\n",
                "[en]: #\n",
                "#### Variations"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "フィードバックループを明示的に導入できます。\n",
                "これは閉ループ制御の文脈で登場します。\n",
                "\n",
                "[en]: #\n",
                "The setup can be extended with feedback, which will be used for closed-loop control:\n",
                "\n",
                "[END]: #\n",
                "$$\n",
                "\\begin{align*}\n",
                "x[k+1] = f(W^\\mathrm{rec}x[k] + W^\\mathrm{in}u[k + 1] + W^\\mathrm{fb}\\hat{{y}}[k])\n",
                ".\\end{align*}\n",
                "$$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "漏れ率 (leaky rate)を導入してより内部状態を保持しやすくしたリーキーESN (leaky ESN) も存在します。\n",
                "\n",
                "[en]: #\n",
                "Leaky (integrator) ESNs with a **leaky rate** are also used to control the dynamics' time scale:\n",
                "\n",
                "[END]: #\n",
                "$$\n",
                "\\begin{align*}\n",
                "x[k+1] = (1-a)~x[k] + a~{f}(W^\\mathrm{rec}x[k] + W^\\mathrm{in}u[k+1])\n",
                ".\\end{align*}\n",
                "$$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "## 演習問題と実演\n",
                "\n",
                "[en]: #\n",
                "## Exercises and demonstrations"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "ここからは演習問題とデモンストレーションに移ります。\n",
                "(離散) ESNを構築し、NARMA10と呼ばれる典型的なベンチマークタスクに取り組みましょう。\n",
                "前章同様、まず次のセルを実行してください。\n",
                "\n",
                "[en]: #\n",
                "Now, let's build a discrete ESN and solve a typical benchmark task called NARMA10.\n",
                "First, run the following cell as in the previous section."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "import sys\n",
                "\n",
                "import numpy as np\n",
                "\n",
                "if \"google.colab\" in sys.modules:\n",
                "    from google.colab import drive  # type: ignore\n",
                "\n",
                "    if False:  # Set to True if you want to use Google Drive and save your work there.\n",
                "        drive.mount(\"/content/gdrive\")\n",
                "        %cd /content/gdrive/My Drive/[[PROJECT_NAME]]/\n",
                "        # NOTE: Change it to your own path if you put the zip file elsewhere.\n",
                "        # e.g., %cd /content/gdrive/My Drive/[PATH_TO_EXTRACT]/[[PROJECT_NAME]]/\n",
                "    else:\n",
                "        pass\n",
                "        %cd /content/\n",
                "        !git clone --branch [[BRANCH_NAME]] https://github.com/rc-bootcamp/[[PROJECT_NAME]].git\n",
                "        %cd /content/[[PROJECT_NAME]]/\n",
                "else:\n",
                "    sys.path.append(\".\")\n",
                "\n",
                "from utils.style_config import plt\n",
                "from utils.tester import load_from_chapter_name\n",
                "from utils.viewer import show_3d_coord\n",
                "\n",
                "test_func, show_solution = load_from_chapter_name(\"03_esn_basics\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "### 1. 入力層・ESNの実装\n",
                "\n",
                "[en]: #\n",
                "### 1. Implement the basic structure of ESN"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "まず初めに入力層に相当する`Linear`クラスと、リザバー層におけるESNを実装する`ESN`クラスを作成しましょう。\n",
                "\n",
                "[en]: #\n",
                "First, let's implement the `Linear` and the `ESN` classes, corresponding to the input and reservoir layers."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Q1.1.\n",
                "\n",
                "[ja]: #\n",
                "以下の穴埋めを実装し、結合行列$W\\in \\mathbb{R}^{N_\\mathrm{out}\\times N_\\mathrm{in}}$とバイアス項$b\\in\\mathbb{R}^{N_\\mathrm{out}}$を保持し、入力$u[k]\\in\\mathbb{R}^{... \\times N_\\mathrm{in}}$を線形変換して得られた多次元ベクトル$W u[k] + b \\in \\mathbb{R}^{... \\times N_\\mathrm{out}}$を出力するクラス`Linear`を完成させよ。\n",
                "ただし$W$と$b$の要素はそれぞれ$\\mathcal{U}([-\\sigma_W, \\sigma_W])$と$\\mathcal{U}([-\\sigma_b, \\sigma_b])$ (引数中の`bound`と`bias`)で初期化される。\n",
                "\n",
                "[en]: #\n",
                "Fill in the blanks in the following code to complete the class `Linear`.\n",
                "The class `Linear` holds a weight matrix $W\\in \\mathbb{R}^{N_\\mathrm{out}\\times N_\\mathrm{in}}$ and a bias term $b\\in\\mathbb{R}^{N_\\mathrm{out}}$.\n",
                "Upon receiving an input $u[k]\\in\\mathbb{R}^{... \\times N_\\mathrm{in}}$, it outputs the linear transformation $W u[k] + b \\in \\mathbb{R}^{... \\times N_\\mathrm{out}}$.\n",
                "The elements of $W$ and $b$ are initialized using uniform distributions $\\mathcal{U}([-\\sigma_W, \\sigma_W])$ and $\\mathcal{U}([-\\sigma_b, \\sigma_b])$ (denoted as `bound` and `bias` in the arguments), respectively.\n",
                "\n",
                "[END]: #\n",
                "\n",
                "- `Linear.__call__`\n",
                "  - Argument(s):\n",
                "    - `x`: `np.ndarray`\n",
                "      - `shape`: `(..., input_dim)`\n",
                "  - Return(s):\n",
                "    - `out`: `np.ndarray`\n",
                "      - `shape`: `(..., output_dim)`\n",
                "\n",
                "[tips]: #\n",
                "- [`np.swapaxes`](https://numpy.org/doc/stable/reference/generated/numpy.swapaxes.html)\n",
                "- [`np.matmul`](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html)\n",
                "\n",
                "[/tips]: #"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Module(object):\n",
                "    def __init__(self, *_args, seed=None, rnd=None, dtype=np.float64, **_kwargs):\n",
                "        if rnd is None:\n",
                "            self.rnd = np.random.default_rng(seed)\n",
                "        else:\n",
                "            self.rnd = rnd\n",
                "        self.dtype = dtype\n",
                "\n",
                "\n",
                "class Linear(Module):\n",
                "    def __init__(self, input_dim: int, output_dim: int, bound: float = None, bias: float = 0.0, **kwargs):\n",
                "        \"\"\"\n",
                "        Linear model\n",
                "\n",
                "        Args:\n",
                "            input_dim (int): input dimension\n",
                "            output_dim (int): output dimension\n",
                "            bound (float, optional): sampling scale for weight. Defaults to None.\n",
                "            bias (float, optional): sampling scale for bias. Defaults to 0.0.\n",
                "        \"\"\"\n",
                "        super(Linear, self).__init__(**kwargs)\n",
                "        self.input_dim = input_dim\n",
                "        self.output_dim = output_dim\n",
                "        if bound is None:\n",
                "            bound = math.sqrt(1 / input_dim)\n",
                "        self.weight = self.rnd.uniform(-bound, bound, (output_dim, input_dim)).astype(self.dtype)\n",
                "        self.bias = self.rnd.uniform(-bias, bias, (output_dim,)).astype(self.dtype)\n",
                "\n",
                "    def __call__(self, x: np.ndarray):\n",
                "        x = np.asarray(x)\n",
                "        out = np.matmul(x, self.weight.swapaxes(-1, -2)) + self.bias  # RIGHT Use `swapaxes` and `matmul`.\n",
                "        return out\n",
                "\n",
                "\n",
                "def solution(input_dim, output_dim, seed, bound, bias, mat):\n",
                "    # DO NOT CHANGE HERE.\n",
                "    lin = Linear(input_dim, output_dim, bound=bound, bias=bias, seed=seed)\n",
                "    return lin(mat)\n",
                "\n",
                "\n",
                "test_func(solution, \"01_01\")\n",
                "# show_solution(\"01_01\", \"Linear\")  # Uncomment it to see the solution."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Q1.2.\n",
                "\n",
                "[ja]: #\n",
                "次に以下の穴埋めを実装し、ESNの内部状態とその時間発展を制御する`ESN`クラスを完成させよ。\n",
                "ただし、`ESN`はスペクトル半径が1になるように正規化された$W^\\mathrm{rec}\\in\\mathbb{R}^{N\\times N}~(\\text{s.t.}~\\rho(W^\\mathrm{rec})=1)$と非線形パラメータ$\\rho \\in \\mathbb{R}$を保持し、入力$v[k+1]$と内部状態$x[k]$に対する時間発展は以下の式で表される。\n",
                "\n",
                "[en]: #\n",
                "Next, fill in the blanks to complete the `ESN` class, which controls the internal state and its time evolution.\n",
                "The `ESN` class holds an internal weight matrix $W^\\mathrm{rec}\\in\\mathbb{R}^{N\\times N}$ normalized so that its spectral radius equals 1 (i.e., $\\rho(W^\\mathrm{rec})=1$) and a nonlinear parameter $\\rho \\in \\mathbb{R}$.\n",
                "The time evolution of the reservoir state $x[k]$ given the input $v[k+1]$ is described by the following equation:\n",
                "\n",
                "[END]: #\n",
                "$$\n",
                "\\begin{align*}\n",
                "x[k+1] = f(\\rho W^\\mathrm{rec} x[k] + v[k+1])\n",
                ".\\end{align*}\n",
                "$$\n",
                "\n",
                "- `ESN.__call__`\n",
                "  - Argument(s):\n",
                "    - `x`: `np.ndarray`\n",
                "      - `shape`: `(..., dim)`\n",
                "    - `v`: `np.ndarray`\n",
                "      - `shape`: `(..., dim)`\n",
                "  - Return(s):\n",
                "    - `x_next`: `np.ndarray`:\n",
                "      - `shape`: `(..., dim)`\n",
                "\n",
                "[tips]: #\n",
                "\n",
                "[ja]: #\n",
                "- [`scipy.sparse.linalg.eigs`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.eigs.html)を使用せよ。\n",
                "  - 再現性のため、`v0=np.ones((dim,))`を指定せよ。\n",
                "  - 高速化のため、`which='LM', k=1`を指定せよ。\n",
                "- [Spectral radius](https://en.wikipedia.org/wiki/Spectral_radius)\n",
                "\n",
                "[en]: #\n",
                "- Use [`scipy.sparse.linalg.eigs`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.eigs.html).\n",
                "  - For reproducibility, specify `v0=np.ones((dim,))`.\n",
                "  - For speed, specify `which='LM', k=1`.\n",
                "- [Spectral radius](https://en.wikipedia.org/wiki/Spectral_radius)\n",
                "\n",
                "[END]: #\n",
                "\n",
                "[/tips]: #"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scipy.sparse.linalg import ArpackNoConvergence, eigs\n",
                "\n",
                "\n",
                "class ESN(Module):\n",
                "    def __init__(\n",
                "        self,\n",
                "        dim: int,\n",
                "        sr: float = 1.0,\n",
                "        f=np.tanh,\n",
                "        a: float | None = None,\n",
                "        p: float = 1.0,\n",
                "        init_state: np.ndarray | None = None,\n",
                "        normalize: bool = True,\n",
                "        **kwargs,\n",
                "    ):\n",
                "        \"\"\"\n",
                "        Echo state network [Jaeger, H. (2001). Bonn, Germany:\n",
                "        German National Research Center for Information Technology GMD Technical Report, 148(34), 13.]\n",
                "\n",
                "        Args:\n",
                "            dim (int): number of the ESN nodes\n",
                "            sr (float, optional): spectral radius. Defaults to 1.0.\n",
                "            f (callable, optional): activation function. Defaults to np.tanh.\n",
                "            a (float | None, optional): leaky rate. Defaults to None.\n",
                "            p (float, optional): density of connection matrix. Defaults to 1.0.\n",
                "            init_state (np.ndarray | None, optional): initial states. Defaults to None.\n",
                "            normalize (bool, optional): decide if normalizing connection matrix. Defaults to True.\n",
                "        \"\"\"\n",
                "        super(ESN, self).__init__(**kwargs)\n",
                "        self.dim = dim\n",
                "        self.sr = sr\n",
                "        self.f = f\n",
                "        self.a = a\n",
                "        self.p = p\n",
                "        if init_state is None:\n",
                "            self.x_init = np.zeros(dim, dtype=self.dtype)\n",
                "        else:\n",
                "            self.x_init = np.array(init_state, dtype=self.dtype)\n",
                "        self.x = np.array(self.x_init)\n",
                "        # Generating normalized sparse matrix\n",
                "        while True:\n",
                "            try:\n",
                "                self.weight = self.rnd.normal(size=(self.dim, self.dim)).astype(self.dtype)\n",
                "                if self.p < 1.0:  # Sparse matrix\n",
                "                    # BEGIN (Optional!) Implement sparse matrix with density `self.p`.\n",
                "                    w_con = np.full((dim * dim,), False)\n",
                "                    w_con[: int(dim * dim * self.p)] = True\n",
                "                    w_con = w_con.reshape((dim, dim))\n",
                "                    self.rnd.shuffle(w_con)\n",
                "                    self.weight = self.weight * w_con\n",
                "                    # END\n",
                "                if normalize:\n",
                "                    # RIGHT_B Calculate `spectral_radius`. Use `eigs` and specify `v0` parameter as `np.ones(self.dim)` to ensure the reproducibility.\n",
                "                    eigen_values = eigs(self.weight, return_eigenvectors=False, k=1, which=\"LM\", v0=np.ones(self.dim))\n",
                "                    spectral_radius = max(abs(eigen_values))\n",
                "                    # RIGHT_E\n",
                "                    self.weight = self.weight / spectral_radius  # RIGHT Normalize the weight matrix.\n",
                "                break\n",
                "            except ArpackNoConvergence:\n",
                "                continue\n",
                "\n",
                "    def __call__(self, x: np.ndarray, v: np.ndarray | None = None):\n",
                "        x_next = self.sr * np.matmul(x, self.weight.swapaxes(-1, -2))  # RIGHT Calculate the next state `x_next`.\n",
                "        if v is not None:\n",
                "            x_next += v  # RIGHT Add the input `v` if it is given.\n",
                "        x_next = self.f(x_next)  # RIGHT Apply the activation function.\n",
                "        if self.a is None:\n",
                "            return x_next\n",
                "        else:\n",
                "            return (1 - self.a) * x + self.a * x_next\n",
                "\n",
                "    def step(self, v: np.ndarray | None = None):\n",
                "        self.x = self(self.x, v)\n",
                "\n",
                "\n",
                "def solution(dim, sr, seed, xmat, vmat):\n",
                "    # DO NOT CHANGE HERE.\n",
                "    net = ESN(dim, sr=sr, f=np.tanh, seed=seed)\n",
                "    return net(xmat, v=vmat)\n",
                "\n",
                "\n",
                "test_func(solution, \"01_02\")\n",
                "# show_solution(\"01_02\", \"ESN\")  # Uncomment it to see the solution."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "次のコードにより`ESN.w_net`のスペクトル半径が1以下すなわち、固有値が単位円内に収まっているかどうか確認しましょう。\n",
                "\n",
                "[en]: #\n",
                "Check if the ESN is successfully normalized by the following code, which visualizes the distribution of the eigenvalues of `ESN.w_net`.\n",
                "\n",
                "[tips]: #\n",
                "\n",
                "[ja]: #\n",
                "- [円則](https://ja.wikipedia.org/wiki/%E3%83%A9%E3%83%B3%E3%83%80%E3%83%A0%E8%A1%8C%E5%88%97#%E5%86%86%E5%89%87)\n",
                "\n",
                "[en]: #\n",
                "- [Circular law](https://en.wikipedia.org/wiki/Circular_law)\n",
                "\n",
                "[/tips]: #"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def show_eigen(mat):\n",
                "    assert mat.ndim == 2\n",
                "    assert mat.shape[0] == mat.shape[1]\n",
                "    ts = np.linspace(0, 2 * np.pi, 100001)\n",
                "    es = np.linalg.eigvals(mat)\n",
                "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
                "    ax.scatter(np.real(es), np.imag(es), s=3.0)\n",
                "    ax.plot(np.cos(ts), np.sin(ts), lw=1.0, ls=\":\", color=\"k\")\n",
                "    ax.set_xlabel(\"real\")\n",
                "    ax.set_ylabel(\"imag\")\n",
                "\n",
                "\n",
                "net = ESN(500, sr=1.0, seed=1234)\n",
                "show_eigen(net.weight)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Q1.3. (Advanced)\n",
                "\n",
                "[ja]: #\n",
                "- 上記のコードで密度$p$ (`math.floor(self.dim * self.dim * p)`個の要素だけ非零) のスパース行列を作成するコードが穴埋めになっている。同様に自力での実装を試みよ。\n",
                "- `normalize=False`を指定し、固有値の分布がどうなるか確認せよ。またスパース ($p < 1$)なケースでの変化も確認せよ。\n",
                "- ノード数が非常に大きい時 (e.g., $N > 10^4$)、固有値の計算は非常に時間がかかるため、`normalize=True`の指定は現実的でない。この際、ノード数$N$と結合密度$p$から、おおよそのスペクトル半径が$1$になる定数倍の係数を求めよ。\n",
                "ただし検証は`normalize=False`を用いて行え。\n",
                "\n",
                "[en]: #\n",
                "- The above code contains a fill-in-the-blanks section for creating a sparse matrix with density $p$ (i.e. only `math.floor(self.dim * self.dim * p)` elements are non-zero).\n",
                "Try implementing this by yourself.\n",
                "- Check how the distribution of eigenvalues changes when you set `normalize=False`.\n",
                "Likewise, check the changes in the case of using a sparse matrix ($p < 1$).\n",
                "- When the number of nodes is very large (e.g., $N > 10^4$), calculating eigenvalues becomes very time-consuming, making the use of `normalize=True` impractical.\n",
                "In this case, derive a constant multiplier coefficient that scales the spectral radius to approximately $1$, based on the number of nodes $N$ and the density $p$.\n",
                "Verify this by setting `normalize=False`."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "### 2. ウォッシュアウト\n",
                "\n",
                "[en]: #\n",
                "### 2. Washout phase"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "**ウォッシュアウト** (内部状態の「洗い流し」)は、初期条件の影響を消失させるために導入されます。\n",
                "ウォッシュアウトにより、システムが同じ入力に対して同じ内部状態を再現的に取るようになり結果的に計算能力の向上が見込めます。\n",
                "\n",
                "- $T_\\mathrm{washout}$: ウォッシュアウトの時間ステップ\n",
                "- $T_\\mathrm{sample}$: その後サンプリングする時間ステップ\n",
                "- $U=\\{u[-T_\\mathrm{washout}],~\\ldots,~u[0],~\\ldots,~u[T_\\mathrm{sample} - 1] \\}$: 入力時系列\n",
                "\n",
                "先程実装した`ESN`と`Linear`を用いてウォッシュアウトの効果を確かめてみましょう。\n",
                "以下のコードは一様乱数$\\mathcal{U}([-1, 1])$からサンプルされた同じ入力$u[k]$に対する`sample_num`個の異なる初期値のESNの応答を表示するコードです。\n",
                "\n",
                "[en]: #\n",
                "The **washout phase** is introduced to diminish the effect of the initial condition, enabling the system to reproduce the same internal states and thereby enhancing computational capability.\n",
                "\n",
                "- $T_\\mathrm{washout}$: Washout length\n",
                "- $T_\\mathrm{sample}$: Sampling length\n",
                "- $U=\\{u[-T_\\mathrm{washout}],~\\ldots,~u[0],~\\ldots,~u[T_\\mathrm{sample} - 1] \\}$: Input sequence\n",
                "\n",
                "Let's check the effect of washout using the `ESN` and `Linear` classes implemented earlier.\n",
                "The following code displays the ESN responses for `sample_num` different initial values given the same input $u[k]$ sampled from the uniform distribution $\\mathcal{U}([-1, 1])$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rnd = np.random.default_rng(1234)\n",
                "\n",
                "dim_in = 1\n",
                "dim_esn = 100  # The number of ESN nodes\n",
                "sample_num = 3  # The number of randomly-sampled initial conditions\n",
                "t_washout, t_sample = 100, 1000\n",
                "t_total = t_washout + t_sample\n",
                "\n",
                "ts = np.arange(-t_washout, t_sample)\n",
                "us = rnd.uniform(-1, 1, (t_total, dim_in))  # Input sequence\n",
                "x_init = rnd.uniform(-1, 1, (sample_num, dim_esn))  # Initial conditions\n",
                "\n",
                "w_in = Linear(dim_in, dim_esn, bound=0.1, bias=0.0, rnd=rnd)\n",
                "net = ESN(dim_esn, sr=0.995, f=np.tanh, rnd=rnd, init_state=x_init)\n",
                "\n",
                "x = x_init\n",
                "xs = np.zeros((t_total, *x_init.shape))  # Shape: (t_total, sample_num, dim_esn)\n",
                "for idx, _t in enumerate(ts):\n",
                "    x = net(x, w_in(us[idx]))\n",
                "    xs[idx] = x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "`xs`を下のコードで表示してみましょう。\n",
                "\n",
                "[en]: #\n",
                "Let's visualize `xs` with the following code."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(2, 2, figsize=(16, 12))\n",
                "\n",
                "ax[0, 0].plot(ts[:t_washout], xs[:t_washout, :, 0])\n",
                "ax[1, 0].plot(ts[:t_washout], xs[:t_washout, :].mean(axis=-1))\n",
                "ax[0, 1].plot(ts[-100:], xs[-100:, :, 0])\n",
                "ax[1, 1].plot(ts[-100:], xs[-100:, :].mean(axis=-1))\n",
                "ax[0, 0].set_ylabel(r\"$x_0[k]$\")\n",
                "ax[1, 0].set_ylabel(r\"$\\bar{x}[k]$\")\n",
                "ax[0, 0].set_title(r\"$k\\in[-T_\\mathrm{washout}, 0)$\")\n",
                "ax[0, 1].set_title(r\"$k\\in[T_\\mathrm{sample} - 100, T_\\mathrm{sample})$\")\n",
                "\n",
                "None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "以下のコードは、最初の3ノード$x_0, x_1, x_2$で3次元座標を構築し、その軌跡を表示します。\n",
                "まずは$k\\in[-T_\\mathrm{washout}, 0)$の区間のみ表示します。\n",
                "\n",
                "[en]: #\n",
                "The following code constructs a 3D coordinate using the first 3 nodes $x_0, x_1, x_2$ and displays their trajectory for each initial condition.\n",
                "First, let's display the dynamics for $k\\in[-T_\\mathrm{washout}, 0)$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig = show_3d_coord(\n",
                "    xs[:t_washout, 0, :3],\n",
                "    xs[:t_washout, 1, :3],\n",
                "    xs[:t_washout, 2, :3],\n",
                "    axes=[\"x_{}[k]\".format(idx) for idx in range(3)],\n",
                ")\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "$k\\in[T_\\mathrm{sample} - 100, T_\\mathrm{sample})$の区間は以下のとおりです。\n",
                "ほとんど重なっている様子が見えます。\n",
                "\n",
                "[en]: #\n",
                "Next, let's display ones during $k\\in[T_\\mathrm{sample} - 100, T_\\mathrm{sample})$.\n",
                "You will see that they almost overlap."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig = show_3d_coord(\n",
                "    xs[-100:, 0, :3],\n",
                "    xs[-100:, 1, :3],\n",
                "    xs[-100:, 2, :3],\n",
                "    axes=[\"x_{}[k]\".format(idx) for idx in range(3)],\n",
                ")\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Q2.1. (Advanced)\n",
                "\n",
                "[ja]: #\n",
                "他のパラメータを変えて同様にウォッシュアウトの効果を確認しESPに対する理解を深めよう。\n",
                "- `sr`を変えよ。特に1以上のケースを確認せよ。\n",
                "- `bound`を変えてみよ。\n",
                "- `esn_dim`を大きくさせてみよ。\n",
                "- 異なる活性化関数`f`を試せ。\n",
                "\n",
                "[en]: #\n",
                "Check the effect of the washout phase for different parameter values and deepen your understanding of the ESP.\n",
                "- Change the value of `sr`, especially for cases where it is larger than 1.\n",
                "- Change the value of `bound`.\n",
                "- Try increasing `esn_dim`.\n",
                "- Try different activation functions `f`."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "### 3. 学習: 線形回帰によるリードアウト層の学習\n",
                "\n",
                "[en]: #\n",
                "### 3. Training phase: Training readout weight by linear regression"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "学習は線形回帰によりワンショット方式で達成されます。\n",
                "このような学習方式は**オフライン学習** (offline training)と呼ばれます。\n",
                "\n",
                "- $T_\\mathrm{train}$: 学習の時間ステップ\n",
                "- $\\{u[0],~\\ldots,~u[T_\\mathrm{train} - 1]\\}$: 入力時系列\n",
                "- $\\{y[0],~\\ldots,~y[T_\\mathrm{train}- 1]\\}$: 目標時系列\n",
                "\n",
                "[en]: #\n",
                "Training can be done in one shot using linear regression.\n",
                "This scheme of training is called **offline training**.\n",
                "\n",
                "- $T_\\mathrm{train}$: Training length\n",
                "- $\\{u[0],~\\ldots,~u[T_\\mathrm{train} - 1]\\}$: Input sequence\n",
                "- $\\{y[0],~\\ldots,~y[T_\\mathrm{train} - 1]\\}$: Target sequence"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "タスク$y[k]$のコスト関数$\\mathrm{RSS}$ は次のように定義されます。\n",
                "\n",
                "[en]: #\n",
                "The cost function $\\mathrm{RSS}$ for a single task $y[k]$ can be defined as follows:\n",
                "\n",
                "[END]: #\n",
                "\n",
                "$$\n",
                "\\mathrm{RSS} := \\sum_{k=0}^{T_\\mathrm{train}-1} \\|  y[k] - \\hat{y}[k] \\|^2,\n",
                "$$\n",
                "\n",
                "[ja]: #\n",
                "さらに以下のように展開されます。\n",
                "\n",
                "[en]: #\n",
                "which can be expanded as follows:\n",
                "\n",
                "[END]: #\n",
                "$$\n",
                "\\begin{align*}\n",
                "\\mathrm{RSS} &= \\sum_{k=0}^{T_\\mathrm{train}-1} \\| y[k] - \\hat{y}[k] \\|^2 \\\\\n",
                "&=  \\sum_{k=0}^{T_\\mathrm{train}-1} \\| y[k] - [1: x[k]]w^\\mathrm{out} \\|^2 \\\\\n",
                "&= \\| y^\\mathrm{train} - X^\\mathrm{train}w^\\mathrm{out} \\|^2\n",
                ",\\end{align*}\n",
                "$$\n",
                "\n",
                "[ja]: #\n",
                "ただし\n",
                "\n",
                "[en]: #\n",
                "where\n",
                "\n",
                "[END]: #\n",
                "$$\n",
                "\\begin{align*}\n",
                "X^\\mathrm{train} &:= \\begin{bmatrix}\n",
                "1 & x_{0}[0] &\\cdots & x_{N-1}[0] \\\\\n",
                "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
                "1 & x_{0}[T_\\mathrm{train}-1] & \\cdots & x_{N-1}[T_\\mathrm{train}-1]\n",
                "\\end{bmatrix} \\in \\mathbb{R}^{T_\\mathrm{train} \\times (N + 1)} \\\\\n",
                "y^\\mathrm{train} &:= [y[0] \\quad \\cdots \\quad y[T_\\mathrm{train}-1]]^\\top \\in \\mathbb{R}^{T_\\mathrm{train}} \\\\\n",
                "w^\\mathrm{out} &:= [{w}^\\mathrm{out}_{0} \\quad {w}^\\mathrm{out}_{1} \\quad \\cdots \\quad {w}^\\mathrm{out}_{N} ]^\\top \\in \\mathbb{R}^{N+1}\n",
                ".\\end{align*}\n",
                "$$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "${X^\\mathrm{train}}^\\top X^\\mathrm{train}$ の階数が最大 (full-rank) の場合、$\\mathrm{RSS}$ を最小化する最適解は次の式で得られます。\n",
                "\n",
                "[en]: #\n",
                "When ${X^\\mathrm{train}}^\\top X^\\mathrm{train}$ is full-rank, optimal solution minimizing $\\mathrm{RSS}$ can be obtained by the following equation:\n",
                "\n",
                "[END]: #\n",
                "$$\n",
                "\\hat{w}^\\mathrm{out} = \\arg \\min_{w^\\mathrm{out}}\\mathrm{RSS} = ({X^\\mathrm{train}}^\\top X^\\mathrm{train})^{-1}{X^\\mathrm{train}}^\\top y^\\mathrm{train},\n",
                "$$\n",
                "\n",
                "[ja]: #\n",
                "最後に、評価フェーズの出力 ($t \\in [T_\\mathrm{train}, T_\\mathrm{train}+T_\\mathrm{eval}-1] $) は次のように記述されます。\n",
                "\n",
                "[en]: #\n",
                "Finally, the output in evaluation phase ($t \\in [T_\\mathrm{train}, T_\\mathrm{train}+T_\\mathrm{eval}-1] $) is written as\n",
                "\n",
                "[END]: #\n",
                "$$\n",
                "\\begin{align*}\n",
                "\\hat{y}[k] &= [1:x[k]]\\hat{w}^\\mathrm{out} \\\\\n",
                "\\hat{y} &= X^\\mathrm{eval}\\hat{w}^\\mathrm{out}\n",
                ",\\end{align*}\n",
                "$$\n",
                "\n",
                "[ja]: #\n",
                "ただし\n",
                "\n",
                "[en]: #\n",
                "where\n",
                "\n",
                "[END]: #\n",
                "$$\n",
                "X^\\mathrm{eval} =\n",
                "\\begin{bmatrix} 1 & x_{0}[T_\\mathrm{train}] &\\cdots & x_{N-1}[T_\\mathrm{train}] \\\\\n",
                " \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
                "  1 & x_{0}[T_\\mathrm{train}+T_\\mathrm{eval}-1] & \\cdots & x_{N-1}[T_\\mathrm{train}+T_\\mathrm{eval}-1]\n",
                "\\end{bmatrix} \\in \\mathbb{R}^{T_\\mathrm{eval} \\times (N + 1)}.\n",
                "$$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Q3.1.\n",
                "\n",
                "[ja]: #\n",
                "先程実装した`Linear`を継承し、与えられた$X \\in \\mathbb{R}^{T\\times N^\\mathrm{in}}, Y \\in \\mathbb{R}^{T \\times N^\\mathrm{out}}$に対して上記の線形回帰を実行し、得られた$\\hat{w}^\\mathrm{out}$を用いて、重みとバイアスを更新する`LRReadout`を実装せよ。\n",
                "\n",
                "[en]: #\n",
                "Implement the `LRReadout` class that inherits the previously implemented class `Linear`.\n",
                "This class should perform linear regression on the given $X \\in \\mathbb{R}^{T \\times N^\\mathrm{in}}$ and $Y \\in \\mathbb{R}^{T \\times N^\\mathrm{out}}$, and update the weights and biases with the obtained $\\hat{w}^\\mathrm{out}$.\n",
                "\n",
                "[END]: #\n",
                "- `LRReadout.train`\n",
                "  - Argument(s):\n",
                "    - `x`: `np.ndarray`\n",
                "      - `shape`: `(time_steps, input_dim)`\n",
                "    - `y`: `np.ndarray`\n",
                "      - `shape`: `(time_steps, output_dim)`\n",
                "  - Return(s):\n",
                "    - `self.weight`: `np.ndarray`\n",
                "      - `shape`: `(output_dim, input_dim)`\n",
                "    - `self.bias`: `np.ndarray`\n",
                "      - `shape`: `(output_dim,)`\n",
                "\n",
                "[ja]: #\n",
                "  - Operation(s):\n",
                "      - `self.weight`の更新\n",
                "      - `self.bias`の更新\n",
                "\n",
                "[en]: #\n",
                "  - Operation(s):\n",
                "      - Update `self.weight` with the obtained weight.\n",
                "      - Update `self.bias` with the obtained bias.\n",
                "\n",
                "[END]: #\n",
                "\n",
                "[tips]: #\n",
                "- [`np.linalg.lstsq`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html)\n",
                "\n",
                "[/tips]: #"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LRReadout(Linear):\n",
                "    def train(self, x: np.ndarray, y: np.ndarray):\n",
                "        assert (x.ndim == 2) and (x.shape[-1] == self.input_dim)\n",
                "        assert (y.ndim == 2) and (y.shape[-1] == self.output_dim)\n",
                "        # BEGIN Use `lstsq` to calculate the optimal weight and bias.\n",
                "        x_biased = np.ones((x.shape[0], x.shape[1] + 1), dtype=self.dtype)\n",
                "        x_biased[:, 1:] = x\n",
                "        sol, _residuals, _rank, _s = np.linalg.lstsq(x_biased, y, rcond=None)\n",
                "        self.weight[:] = sol[1:].T\n",
                "        self.bias[:] = sol[0]\n",
                "        # END\n",
                "        return self.weight, self.bias\n",
                "\n",
                "\n",
                "def solution(dim_in, dim_out, x_train, y_train, x_eval):\n",
                "    # DO NOT CHANGE HERE.\n",
                "    readout = LRReadout(dim_in, dim_out)\n",
                "    readout.train(x_train, y_train)\n",
                "    return readout(x_eval)\n",
                "\n",
                "\n",
                "test_func(solution, \"03_01\")\n",
                "# show_solution(\"03_01\", \"LRReadout\")  # Uncomment it to see the solution."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "### 4. NARMA10―10次の非線形自己回帰移動平均<sup>[5]</sup>\n",
                "\n",
                "[en]: #\n",
                "### 4. NARMA10―The 10th-order nonlinear autogressive moving average<sup>[5]</sup>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "NARMA (Nonlinear AutoRegressive Moving Average) 10<sup>[5]</sup> は、RC で頻繁に使用される典型的なベンチマークです。\n",
                "自己回帰部分と移動平均部分で構成される非線形 ARMA (AutoRressive Moving Average)モデルであり、現在と過去値の間には非線形な関係があります。\n",
                "典型的には一様ランダム入力 $u[k]\\sim\\mathcal{U}([-1,1])$に対して、NARMA10 モデルは次のように定義されます。\n",
                "\n",
                "[en]: #\n",
                "NARMA (Nonlinear AutoRegressive Moving Average) 10<sup>[5]</sup> is a typical benchmark task frequently used in RC.\n",
                "It consists of an autoregressive part and a moving average part, meaning that the value depends on past inputs in a nonlinear manner.\n",
                "Given a uniform random input $u[k] \\sim \\mathcal{U}([-1, 1])$, the NARMA10 model is defined as follows:\n",
                "\n",
                "[END]: #\n",
                "$$\n",
                "\\begin{align*}\n",
                "y[k+1] &= \\alpha y[k] + \\beta y[k]\\sum_{i=0}^{9}y[k-i] + \\gamma \\nu[k-9]\\nu[k] + \\delta \\\\\n",
                "\\nu[k]&=\\mu u[k]+\\kappa\n",
                ",\\end{align*}\n",
                "$$\n",
                "\n",
                "[ja]: #\n",
                "ここで、$(\\alpha,\\beta,\\gamma,\\delta,\\mu,\\kappa)=(0.3,0.05,1.5,0.1,0.25,0.25)$がよく用いられます。\n",
                "値の発散を避けるために、入力 $u[k]$ は慎重に調整されます。\n",
                "特に$(\\mu,\\kappa)=(0.25,0.25)$により$\\nu[k] \\sim \\mathcal{U}([0.0, 0.5])$ にスケーリングされる場合が多いです。\n",
                "\n",
                "[en]: #\n",
                "where $(\\alpha,\\beta,\\gamma,\\delta,\\mu,\\kappa)=(0.3,0.05,1.5,0.1,0.25,0.25)$, and\n",
                "the input $u[k]$ is biased and scaled so that $\\nu[k] \\sim \\mathcal{U}([0.0,0.5])$ to avoid divergence."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Q4.1.\n",
                "\n",
                "[ja]: #\n",
                "長さ$T + 10$の入力時系列 $U=\\{u[-10],~\\ldots,~u[T-1]\\}\\in\\mathbb{R}^{(T+10)\\times d}$、\n",
                "長さ10の時系列 $Y^\\mathrm{init}=\\{y[-10],~\\ldots,~y[-1]\\}\\in \\mathbb{R}^{10 \\times d}$\n",
                "が与えられる。\n",
                "NARMA10\n",
                "上記の式に従い変換を施したNARMA10の時系列 $Y=\\{y[-10],~\\ldots,~y[0],~\\ldots,~y[T-1]\\}\\in\\mathbb{R}^{T\\times d}$ を出力する関数`narma_func`を実装せよ。\n",
                "\n",
                "[en]: #\n",
                "An input time series $U = \\{u[-10],~\\ldots,~u[T-1]\\}\\in\\mathbb{R}^{(T+10)\\times d}$ with length $T + 10$ and another time series $Y^\\mathrm{init}=\\{y[-10],~\\ldots,~y[-1]\\}\\in \\mathbb{R}^{10 \\times d}$ with length 10 are given.\n",
                "Implement the function `narma_func` that outputs the NARMA10 time series $Y=\\{y[-10],~\\ldots,~y[0],~\\ldots,~y[T-1]\\}\\in\\mathbb{R}^{T\\times d}$, following the equations defined above.\n",
                "\n",
                "[END]: #\n",
                "\n",
                "- `narma_func`\n",
                "  - Argument(s):\n",
                "    - `us`: `np.ndarray`\n",
                "      - `shape`: `(t + 10, d)`\n",
                "    - `y_init`: `np.ndarray`\n",
                "      - `shape`: `(10, d)`\n",
                "  - Return(s):\n",
                "    - `ys`: `np.ndarray`\n",
                "      - `shape`: `(t + 10, d)`\n",
                "\n",
                "[tips]: #\n",
                "- [`np.sum`](https://numpy.org/doc/stable/reference/generated/numpy.sum.html)\n",
                "\n",
                "[/tips]: #"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def narma_func(us, y_init, alpha=0.3, beta=0.05, gamma=1.5, delta=0.1, mu=0.25, kappa=0.25):\n",
                "    assert us.shape[0] > 10\n",
                "    assert y_init.shape[0] == 10\n",
                "    assert y_init.shape[1:] == us.shape[1:]\n",
                "    vs = mu * us + kappa\n",
                "    ys = np.zeros_like(vs)\n",
                "    # BEGIN Implement NARMA10 system.\n",
                "    ys[:10] = y_init\n",
                "    for idx in range(10, ys.shape[0]):\n",
                "        ys[idx] += alpha * ys[idx - 1]\n",
                "        ys[idx] += beta * ys[idx - 1] * np.sum(ys[idx - 10 : idx], axis=0)\n",
                "        ys[idx] += gamma * vs[idx - 10] * vs[idx - 1]\n",
                "        ys[idx] += delta\n",
                "    # END\n",
                "    return ys\n",
                "\n",
                "\n",
                "test_func(narma_func, \"04_01\")\n",
                "# show_solution(\"04_01\", \"narma_func\")  # Uncomment it to see the solution."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "以下のコードで実際にNARMA10の時系列を表示させてみましょう。\n",
                "\n",
                "[en]: #\n",
                "The following code displays the dynamics of NARMA10 for input $u[k]$:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rnd = np.random.default_rng(1234)\n",
                "\n",
                "length = 200\n",
                "\n",
                "us = rnd.uniform(-1.0, 1.0, length + 10)\n",
                "y_init = np.zeros(10)\n",
                "ys = narma_func(us, y_init)\n",
                "\n",
                "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
                "ax.plot(us * 0.25 + 0.25, label=r\"$v[k]$\", ls=\":\", color=\"k\")\n",
                "ax.plot(ys, label=\"NARMA10\")\n",
                "ax.legend(loc=\"upper left\", bbox_to_anchor=(1.025, 1.0), borderaxespad=0, ncol=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Q4.2. (Advanced)\n",
                "\n",
                "[ja]: #\n",
                "NARMA10は発散しやすいベンチマークタスクであり、そのパラメータの取り扱いには慎重を要する。\n",
                "- スケーリングを二倍、すなわち$u[k] \\in [-2, 2]$で値の発散を確認せよ。\n",
                "- サンプリング期間を非常に大きくする($T\\approx 10^6$)とかなりの確率で値の発散が発生する。これを確認せよ。\n",
                "- 発散しないための工夫としてクリッピングが考えられる。典型的には$\\tanh$によって$y[k]$の値を毎ステップ制限する方策が取られる。この処置を実装し実際に発散の軽減を確認せよ。\n",
                "\n",
                "[en]: #\n",
                "The NARMA10 benchmark task is prone to divergence, so its parameters must be handled carefully.\n",
                "- Check the divergence by doubling the scale of input (i.e., $u[k] \\in [-2, 2]$).\n",
                "- Divergence is more likely to occur when the sampling period is very large ($T\\approx 10^6$).\n",
                "Confirm this behavior.\n",
                "- Clipping is an effective measure to prevent divergence.\n",
                "For example, you may use $\\tanh$ to limit the values of $y[k]$ at every time step.\n",
                "Implement this measure and confirm that it reduces the occurrence of divergence."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "### 5. 精度の評価\n",
                "\n",
                "[en]: #\n",
                "### 5. Evaluation of the performance"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "- $T_\\mathrm{in}$: 評価時間ステップ\n",
                "\n",
                "平均二乗平方根誤差 (RMSE) と正規二乗平均平方根誤差 (NRMSE) は次の式から定義されます。\n",
                "\n",
                "[en]: #\n",
                "- $T_\\mathrm{in}$: Evaluation length\n",
                "\n",
                "The root mean squared error (RMSE) and normalized root mean squared error (NRMSE) can be defined as follows:\n",
                "\n",
                "[END]: #\n",
                "$$\n",
                "\\begin{align*}\n",
                "\\mathrm{NRMSE}(y^\\mathrm{eval}, \\hat{y}^\\mathrm{eval}) :&= \\dfrac{\\mathrm{RMSE}(y^\\mathrm{eval}, \\hat{y}^\\mathrm{eval}) }{\\sigma(y^\\mathrm{eval})} \\\\\n",
                "\\mathrm{RMSE}(y^\\mathrm{eval}, \\hat{y}^\\mathrm{eval}) :&=  \\sqrt{\\dfrac{\\mathrm{RSS}(y^\\mathrm{eval}, \\hat{y}^\\mathrm{eval}) }{T_\\mathrm{eval}} } \\\\\n",
                "&= \\sqrt{ \\dfrac{1}{T_\\mathrm{eval}} \\sum_{k=T_\\mathrm{train}}^{T_\\mathrm{train}+T_\\mathrm{eval}-1} ( y[k] - \\hat{y}[k])^2}\n",
                ",\\end{align*}\n",
                "$$\n",
                "\n",
                "[ja]: #\n",
                "ここで、$\\sigma^2(y^\\mathrm{eval})$ は出力 $y^\\mathrm{eval}=\\{y[T_\\mathrm{train}],~\\ldots,~y[T_\\mathrm{train}+T_\\mathrm{eval}-1]\\}$ の分散を表します。\n",
                "状況によりますが、$\\mathrm{NRMSE} < 0.2$ が精度が良好か判別する基準として一つの目安となります。\n",
                "\n",
                "[en]: #\n",
                "where $\\sigma^2(y^\\mathrm{eval})$ is the variance of $y^\\mathrm{eval}=\\{y[T_\\mathrm{train}],~\\ldots,~y[T_\\mathrm{train}+T_\\mathrm{eval}-1]\\}$ through time.\n",
                "As a rough guideline, $\\mathrm{NRMSE} < 0.2$ often indicates good performance."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Q5.1.\n",
                "\n",
                "[ja]: #\n",
                "時間$T$ステップにわたり$d$個の時系列を記録した$Y\\in\\mathbb{R}^{T\\times d}$と$\\hat{Y}\\in\\mathbb{R}^{T\\times d}$が与えられる。\n",
                "上記の式を参考に各$d$要素について$\\mathrm{NRMSE}$を計算した、$\\mathrm{NRMSE}(Y, \\hat{Y})\\in \\mathbb{R}^{d}$を出力するコードを書け。\n",
                "\n",
                "[en]: #\n",
                "Given two matrices $Y\\in\\mathbb{R}^{T\\times d}$ and $\\hat{Y}\\in\\mathbb{R}^{T\\times d}$, each containing $d$ time series measured over $T$ time steps, write code to compute the $\\mathrm{NRMSE}$ for each of the $d$ elements.\n",
                "The output $\\mathrm{NRMSE}(Y, \\hat{Y})\\in \\mathbb{R}^{d}$ should be calculated based on the above equations.\n",
                "\n",
                "[END]: #\n",
                "\n",
                "- `calc_nrmse`\n",
                "  - Argument(s):\n",
                "    - `y`: `np.ndarray`\n",
                "      - `shape`: `(t, d)`\n",
                "    - `yhat`: `np.ndarray`\n",
                "      - `shape`: `(t, d)`\n",
                "  - Return(s):\n",
                "    - `nrmse`: `np.ndarray`\n",
                "      - `shape`: `(d,)`\n",
                "\n",
                "[tips]: #\n",
                "- [`np.mean`](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)\n",
                "- [`np.var`](https://numpy.org/doc/stable/reference/generated/numpy.var.html)\n",
                "\n",
                "[/tips]: #"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calc_nrmse(y, yhat):\n",
                "    # BEGIN\n",
                "    mse = y - yhat\n",
                "    mse = (mse * mse).mean(axis=0)\n",
                "    var = y.var(axis=0)\n",
                "    return (mse / var) ** 0.5\n",
                "    # END\n",
                "\n",
                "\n",
                "test_func(calc_nrmse, \"05_01\")\n",
                "# show_solution(\"05_01\", \"calc_nrmse\")  # Uncomment it to see the solution."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "以下のサンプルコードによって、下記の典型的な設定でNARMA10を100ノードのESNに解かせてみましょう。\n",
                "これまで実装した`Linear`・`ESN`・`LRReadout`・`narma_func`・`calc_nrmse`を使用します。\n",
                "\n",
                "[en]: #\n",
                "Let's use the following sample code to let 100-node ESN solve NARMA10 with the typical settings listed below.\n",
                "The implemented modules `Linear`, `ESN`, `LRReadout`, `narma_func`, and `calc_nrmse` will be used below.\n",
                "\n",
                "[END]: #\n",
                "\n",
                "- $(N, \\rho, \\sigma) = (100, 0.9, 0.1)$\n",
                "- $(T_\\mathrm{washout}, T_\\mathrm{train}, T_\\mathrm{eval}) = (100, 2000, 1000)$\n",
                "- $(\\alpha,\\beta,\\gamma,\\delta,\\mu,\\kappa)=(0.3,0.05,1.5,0.1,0.25,0.25)$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rnd = np.random.default_rng(1234)\n",
                "dim_esn, rho, sigma = 100, 0.9, 0.1\n",
                "t_washout, t_train, t_eval = 100, 2000, 1000\n",
                "narma_parameters = dict(alpha=0.3, beta=0.05, gamma=1.5, delta=0.1, mu=0.25, kappa=0.25)\n",
                "\n",
                "t_total = t_washout + t_train + t_eval\n",
                "ts = np.arange(-t_washout, t_train + t_eval)\n",
                "us = rnd.uniform(-1, 1, (t_total, 1))\n",
                "ys = narma_func(us, np.zeros((10, 1)), **narma_parameters)\n",
                "x_init = rnd.uniform(-1, 1, (dim_esn,))\n",
                "w_in = Linear(1, dim_esn, bound=sigma, bias=0.0, rnd=rnd)\n",
                "net = ESN(dim_esn, sr=rho, f=np.tanh, init_state=x_init, rnd=rnd)\n",
                "w_out = LRReadout(dim_esn, 1)\n",
                "\n",
                "x = x_init\n",
                "xs = np.zeros((t_total, dim_esn))\n",
                "for idx in range(t_total):\n",
                "    x = net(x, w_in(us[idx]))\n",
                "    xs[idx] = x\n",
                "\n",
                "# Or, you can write as follows in the OOP style:\n",
                "# xs = np.zeros((t_total, dim_esn))\n",
                "# for idx in range(t_total):\n",
                "#     net.step(w_in(us[idx]))\n",
                "#     xs[idx] = net.x\n",
                "\n",
                "x_train, y_train = xs[t_washout:-t_eval], ys[t_washout:-t_eval]\n",
                "x_eval, y_eval = xs[-t_eval:], ys[-t_eval:]\n",
                "w_out.train(x_train, y_train)\n",
                "y_out = w_out(x_eval)\n",
                "nrmse = calc_nrmse(y_eval, y_out)\n",
                "\n",
                "plot_length = 200\n",
                "\n",
                "fig, ax = plt.subplots(3, 1, figsize=(12, 8), gridspec_kw={\"hspace\": 0.05})\n",
                "ax[0].plot(ts[-plot_length:], us[-plot_length:])\n",
                "ax[0].set_xticklabels([])\n",
                "ax[1].plot(ts[-plot_length:], xs[-plot_length:, :10], lw=1.0)\n",
                "ax[1].set_xticklabels([])\n",
                "ax[2].plot(ts[-plot_length:], y_eval[-plot_length:], color=\"k\", ls=\":\", lw=1.5)\n",
                "ax[2].plot(ts[-plot_length:], y_out[-plot_length:], lw=1.5, color=\"red\")\n",
                "ax[0].set_title(\"NRMSE={:.3e}\".format(nrmse[0]))\n",
                "ax[0].set_ylabel(r\"$u[k]$\")\n",
                "ax[1].set_ylabel(r\"$x[k]$\")\n",
                "ax[2].set_ylabel(r\"$y[k]$ & $\\hat{y}[k]$\")\n",
                "ax[2].set_xlabel(\"time steps\")\n",
                "\n",
                "None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Q5.2. (Advanced)\n",
                "\n",
                "[ja]: #\n",
                "NARMA10の精度は上記のパラメータの選択によって大きく変化する。\n",
                "「良い」(目安は$\\mathrm{NRMSE}<0.2$)となるようなパラメータセットを探すべく以下のパターンを試せ。\n",
                "- 係数$0.5$と定数項$0.5$ (`w_in(0.5 * u + 0.5)`)を採用し、実質的に非対称入力$u[k]\\sim \\mathcal{U}([0, 1])$を実現せよ。またその時の精度の変化を計測せよ。\n",
                "- ノード数を変化させよ。特に $N(=100, 200,~\\ldots,~1000)$ のケースを試し、横軸 $N$・縦軸 $\\mathrm{NRMSE}$のグラフとして描画せよ。\n",
                "- 学習用のサンプリング時間を長くせよ。特に $T_\\mathrm{train}=(2000, 4000,~\\ldots,~20000)$ と変化させ同様に、横軸 $T_\\mathrm{train}$ ・縦軸 $\\mathrm{NRMSE}$ のグラフを描画せよ。\n",
                "- $(\\mu, \\kappa)=(0.5, 0.0)$ により $\\nu[k] \\sim \\mathcal{U}([-0.5, 0.5])$ として同様に精度を計測せよ。\n",
                "\n",
                "[en]: #\n",
                "The accuracy of NARMA10 varies significantly depending on the parameter selection.\n",
                "Follow the instructions below to find a \"good\" parameter set ($\\mathrm{NRMSE}<0.2$ is a reasonable benchmark).\n",
                "\n",
                "- Adopt a coefficient of $0.5$ and a bias of $0.5$ (`w_in(0.5 * u + 0.5)`), which effectively uses an asymmetric input $u[k] \\sim \\mathcal{U}([0, 1])$.\n",
                "Observe how the accuracy changes in this setting.\n",
                "- Change the number of nodes.\n",
                "Specifically, test cases of $N = 100, 200,~\\ldots,~1000$, and plot a graph with $N$ on the x-axis and $\\mathrm{NRMSE}$ on the y-axis.\n",
                "- Change the sampling length.\n",
                "Specifically, test cases of $T_\\mathrm{train} = 2000, 4000,~\\ldots,~20000$, and plot a graph with $T_\\mathrm{train}$ on the x-axis and $\\mathrm{NRMSE}$ on the y-axis.\n",
                "- Adopt $(\\mu, \\kappa) = (0.5, 0.0)$ to use $\\nu[k] \\sim \\mathcal{U}([-0.5, 0.5])$, and observe how the accuracy changes in this setting."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Q5.3. (Advanced)\n",
                "\n",
                "[ja]: #\n",
                "次章の内容で取り扱うが、パラメータの体系的な探索は**グリッドサーチ (grid search)**と呼ばれ、RCに限らず解析や最適化の場面において幅広く登場する。\n",
                "典型的には`for`文を用いた多重ループが考えられるが、前回学んだとおりPythonではfor文の実行は遅く、探索するパラメータ数が増大するとそのオーバーヘッドが無視できなくなる。\n",
                "一方でNumPyは、扱うテンソルのサイズが大きいほど一般に並列化の恩恵を受け効率的であるといえる。\n",
                "`for`文を極力用いずに (理想的には`ESN`の時間発展のみに使用)効率的にグリッドサーチを行う方法を考えよ。\n",
                "特に以下のケースに関して考察せよ。\n",
                "- $\\sigma$: 入力スケール\n",
                "- $\\rho$: スペクトル半径\n",
                "- $\\mu, \\kappa$: NARMA10のパラメータ\n",
                "\n",
                "なおここに挙げられたパラメータに関するグリッドサーチは、今回実装した関数に変更を**加えずに**数行のコードの追加で実現できる。\n",
                "\n",
                "[en]: #\n",
                "In the next chapter, we will cover a systematic method for searching parameters called **grid search**, which appears not just in RC but in many analysis and optimization scenarios.\n",
                "Typically, this involves multiple nested loops using `for` statements.\n",
                "However, as we have already learned, `for` loops are slow in Python, and their overhead becomes significant as the number of parameters to search increases.\n",
                "On the other hand, NumPy is generally more efficient as the tensor size increases, benefiting from parallelization.\n",
                "Consider a method to perform grid search efficiently without using `for` loops as much as possible.\n",
                "Ideally, `for` loops should only be used for the time evolution of ESN states.\n",
                "Specifically, consider searching the following parameters:\n",
                "\n",
                "- $\\sigma$: input scale\n",
                "- $\\rho$: spectral radius\n",
                "- $\\mu, \\kappa$: parameters of the NARMA10 model\n",
                "\n",
                "Note that grid search for these parameters can be achieved with just a few lines of additional code, without modifying the already-implemented functions."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[ja]: #\n",
                "## 参考文献\n",
                "\n",
                "[en]: #\n",
                "## References"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[1] Jaeger, H. (2001). *The “echo state” approach to analysing and training recurrent neural networks-with an erratum note*. Bonn, Germany: German national research center for information technology gmd technical report, 148(34), 13.\n",
                "\n",
                "[2] Jaeger, H. (2002). *Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the \"echo state network\" approach*. Bonn, Germany: GMD-Forschungszentrum Informationstechnik.\n",
                "\n",
                "[3] Nakajima, K. (2020). *Physical reservoir computing—An introductory perspective*. Japanese Journal of Applied Physics, 59(6), 060501. https://doi.org/10.35848/1347-4065/ab8d4f\n",
                "\n",
                "[4] Ceni, A., Ashwin, P., Livi, L., & Postlethwaite, C. (2020). *The echo index and multistability in input-driven recurrent neural networks.* Physica D: Nonlinear Phenomena, 412, 132609. https://doi.org/10.1016/j.physd.2020.132609\n",
                "\n",
                "[5] Atiya, A. F., & Parlos, A. G. (2000). New results on recurrent network training: Unifying the algorithms and accelerating convergence. IEEE Transactions on Neural Networks, 11(3), 697–709. https://doi.org/10.1109/72.846741"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "rc-bootcamp (3.12.12)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
