{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "# Chapter 6. 記憶容量\n",
    "\n",
    "[en]: #\n",
    "# Chapter 6. Memory Capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "この章では、記憶容量、すなわち力学系が有する過去の入力を保持する能力を評価する指標の計算方法を学びます。\n",
    "特に記憶関数 (Memory Function; MF) と記憶容量 (Memory Capacity; MC) の２つを実装し、その使い方を学習します。\n",
    "\n",
    "[en]: #\n",
    "In this chapter, we introduce a measure to evaluate past inputs held in the reservoir.\n",
    "We implement two measures: memory function and memory capacity, and learn how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "## 前書き\n",
    "\n",
    "[en]: #\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "記憶関数・記憶容量はH. Jaeger<sup>[1]</sup>らによって提案された指標で、力学系が過去の入力をどれほど保持できるかを定量化します。\n",
    "以下の式で表される $N$次元の入力あり力学系 $x[k]$ とある線形写像$g: \\mathbb{R}^N \\to \\mathbb{R}$ による出力 $\\hat{y}[k]$ を考えます。\n",
    "\n",
    "[en]: #\n",
    "The memory function and memory capacity are metrics proposed by H. Jaeger<sup>[1]</sup> to quantify how well a dynamical system can retain past inputs.\n",
    "Consider an $N$-dimensional dynamical system with input $x[k]$ and an output $\\hat{y}[k]$ defined by a linear mapping $g: \\mathbb{R}^N \\to \\mathbb{R}$, expressed as follows:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\renewcommand{\\Tau}{\\mathrm{T}}\n",
    "\\renewcommand{\\Zeta}{\\mathrm{Z}}\n",
    "\\begin{align*}\n",
    "x[k+1] &= f \\left(x[k],\\zeta[k+1]\\right) \\\\\n",
    "\\hat{y}[k] &= g (x[k])\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "$g$は線形なので、ある結合パラメータ $W^\\mathrm{out} \\in \\mathbb{R}^{N+1}$ を用いて出力 $\\hat{y}[k]$ は次の式で表現できます。\n",
    "\n",
    "[en]: #\n",
    "Since $g$ is linear, the output $\\hat{y}[k]$ can be expressed using a coupling parameter $W^\\mathrm{out} \\in \\mathbb{R}^{1\\times(N+1)}$ as follows:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y}[k] &= \\hat{w} [1 ; x[k]] \\\\\n",
    "&= \\hat{w} {[1 \\quad x_1[k] \\quad \\cdots \\quad x_{N}[k]]}^\\top \\\\\n",
    "&= \\hat{w}_0 + \\sum_{i=1}^{N} \\hat{w}_i x_{i}[k]\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "入力 $\\zeta[k]$、内部状態 $x[k]$ ともに定常的、すなわちその平均や分散が時間によらず一定であると仮定します。\n",
    "このとき記憶関数 $\\mathrm{MF}[\\tau]$ は特に $\\tau~(\\geq 0)$ ステップ前の入力 $\\zeta^\\tau[k]:=\\zeta[k-\\tau]$ を内部状態 $x[k]$ からどれほど再構成できるかを評価する指標で、次の式で定義されます。\n",
    "\n",
    "[en]: #\n",
    "Assume that both the input $\\zeta[k]$ and the internal state $x[k]$ are stationary, i.e., their mean and variance are constant over time.\n",
    "The memory function $\\mathrm{MF}[\\tau]$ evaluates how well the input $\\zeta^\\tau[k]:=\\zeta[k-\\tau]$ from $\\tau~(\\geq 0)$ steps ago can be reconstructed from the internal state $x[k]$.\n",
    "It is defined as\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{MF}[\\tau] :=& \\max_{\\hat{w}} \\rho^2[\\zeta^\\tau, \\hat{y}] \\\\\n",
    "=& \\max_{\\hat{w}} \\frac{\\mathrm{Cov}^2[\\zeta^\\tau, \\hat{y}]}{\\mathrm{Var}[\\zeta^\\tau]\\mathrm{Var}[\\hat{y}]}\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "ここで $\\rho$ は相関係数、 $\\mathrm{Cov}$ は共分散、$\\mathrm{Var}$ は分散を表します。\n",
    "相関係数の絶対値は $1$ 以下であるので、以下の不等式が成り立ちます。\n",
    "\n",
    "[en]: #\n",
    "where $\\rho$ represents the correlation coefficient, $\\mathrm{Cov}$ denotes covariance, and $\\mathrm{Var}$ represents variance.\n",
    "Since the absolute value of the correlation coefficient is at most $1$, the following inequality holds:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "0 \\leq \\mathrm{MF}[\\tau] \\leq 1\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "最後に全過去入力に対する記憶関数の総和により、以下の式で記憶容量 $\\mathrm{MC}$ は定義されます。\n",
    "\n",
    "[en]: #\n",
    "Finally, the memory capacity $\\mathrm{MC}$ is defined as the sum of the memory functions over all past inputs, as expressed in the following equation:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{MC} := \\sum_{\\tau=0}^{\\infty} \\mathrm{MF}[\\tau]\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "特に入力がi.i.d.、つまり各時刻で$\\zeta[k]$の値が独立にサンプルされる場合、以下の不等式の成立が知られています (導出は発展課題)。\n",
    "\n",
    "[en]: #\n",
    "In particular, when the input is i.i.d., meaning the values of $\\zeta[k]$ are sampled independently at each time step, the following inequality is known to hold\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{MC} \\leq r \\leq N\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "ここで $r$ は $x[k]$ の階数を表し、式が示すとおり、高々線形独立な成分の数にその記憶容量が制限されます。\n",
    "\n",
    "[en]: #\n",
    "where $r$ represents the rank of $x[k]$, indicating that the memory capacity is limited by the number of linearly independent components at most (derivation is provided in the advanced exercises)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "## 演習問題と実演\n",
    "\n",
    "[en]: #\n",
    "## Exercises and demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "ここからは演習問題とデモンストレーションに移ります。\n",
    "前回と同じライブラリの他、前回の演習で実装した`ESN`・`Linear`が`import`により利用できます。\n",
    "初めに次のセルを実行してください。\n",
    "\n",
    "なお`ESN`・`Linear`の内部実装を再確認するには、`import inspect`以下の行をコメントアウトするか`...?? / ??...`を使用してください。\n",
    "\n",
    "[en]: #\n",
    "Let's move on to the exercise and demonstration.\n",
    "In addition to the basic libraries used in the previous chapter, you can also use the `ESN` and `Linear` classes/functions that we implemented previously via `import`.\n",
    "Please execute the following cell.\n",
    "\n",
    "By the way, you can check the internal implementations of `ESN` and `Linear`either by uncommenting the lines after `import inspect` or by using `...?? / ??...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import drive  # type: ignore\n",
    "\n",
    "    if False:  # Set to True if you want to use Google Drive and save your work there.\n",
    "        drive.mount(\"/content/gdrive\")\n",
    "        %cd /content/gdrive/My Drive/[[PROJECT_NAME]]/\n",
    "        # NOTE: Change it to your own path if you put the zip file elsewhere.\n",
    "        # e.g., %cd /content/gdrive/My Drive/[PATH_TO_EXTRACT]/[[PROJECT_NAME]]/\n",
    "    else:\n",
    "        pass\n",
    "        %cd /content/\n",
    "        !git clone --branch [[BRANCH_NAME]] https://github.com/rc-bootcamp/[[PROJECT_NAME]].git\n",
    "        %cd /content/[[PROJECT_NAME]]/\n",
    "else:\n",
    "    sys.path.append(\".\")\n",
    "\n",
    "from utils.reservoir import ESN, Linear\n",
    "from utils.style_config import plt\n",
    "from utils.tester import load_from_chapter_name\n",
    "from utils.tqdm import tqdm, trange\n",
    "\n",
    "test_func, show_solution = load_from_chapter_name(\"06_memory_capacity\")\n",
    "\n",
    "\n",
    "# Uncomment it to see the implementations of `Linear` and `ESN`.\n",
    "# import inspect\n",
    "# print(inspect.getsource(Linear))\n",
    "# print(inspect.getsource(ESN))\n",
    "\n",
    "# Or just use ??.../...?? (uncomment the following lines).\n",
    "# Linear??\n",
    "# ESN??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "### 1. 特異値分解を用いた記憶関数の実装\n",
    "\n",
    "[en]: #\n",
    "### 1. Implementation of the memory function using singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "まず記憶関数の実装を行いましょう。\n",
    "$\\mathrm{MF}[\\tau]$ は$\\zeta^\\tau$ と $\\hat{y}$ の相関係数の二乗の最大値として定義されます。\n",
    "一方で相関係数の二乗は、決定係数 $\\mathrm{R}^2[\\zeta^\\tau, \\hat{y}]$ と一致する (導出は発展課題) ので以下の式が成り立ちます。\n",
    "\n",
    "[en]: #\n",
    "Let us implement the memory function. $\\mathrm{MF}[\\tau]$ is defined as the maximum squared correlation coefficient between $\\zeta^\\tau$ and $\\hat{y}$.\n",
    "The squared correlation coefficient is equivalent to the coefficient of determination $\\mathrm{R}^2[\\zeta^\\tau, \\hat{y}]$ (derivation is left as an advanced exercise), so the following equation holds:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{MF}[\\tau] &= \\max_{\\hat{w}} \\mathrm{R}^2[\\zeta^\\tau, \\hat{y}] \\\\\n",
    "&= 1 - \\min_{\\hat{w}} \\frac{\\mathrm{E}[(\\zeta^\\tau - \\hat{y})^2]}{\\mathrm{Var}[\\zeta^\\tau]} \\\\\n",
    "&= 1 - \\frac{\\min_{\\hat{w}} \\mathrm{MSE}(\\zeta^\\tau, \\hat{y})}{\\mathrm{Var}[\\zeta^\\tau]}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "今線型回帰を考えているので $\\mathrm{MSE}$ を最小化する $\\hat{w}$ は線型回帰により一意に導出され、$\\mathrm{MF}[\\tau]$ は $\\hat{y}$ を用いない以下の形式で表現されます。\n",
    "\n",
    "[en]: #\n",
    "Since we are considering linear regression, the $\\hat{w}$ that minimizes $\\mathrm{MSE}$ is uniquely derived through linear regression, and $\\mathrm{MF}[\\tau]$ is expressed in the following form without using $\\hat{y}$:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{MF}[\\tau] &= \\mathrm{R}^2[\\zeta^\\tau, x]\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "この式は、明示的に過去時系列を再構成する $g$ ならびに $\\hat{w}$ を計算せずとも、特異値分解 (Singular Value Decomposition; SVD) を用いて計算できます (導出は第2章 Q5.4. 参照)。\n",
    "つまり $T$ ステップに渡る内部状態のダイナミクスを格納した説明変数行列 $X=[x[0];x[1];~\\ldots;~x[T-1]]^\\top \\in \\mathbb{R}^{T \\times N}$ と、対応する目的変数行列 $\\Zeta^\\tau = [\\zeta[-\\tau],\\zeta[-\\tau+1],~\\ldots,~\\zeta[T-\\tau-1]]^\\top \\in \\mathbb{R}^{T \\times 1}$ に関して、$X=U\\Sigma V^\\top$ と分解された後、以下の式で $\\mathrm{MF}[\\tau]$ は計算されます。\n",
    "\n",
    "[en]: #\n",
    "This equation can be calculated using singular value decomposition (SVD) without explicitly reconstructing the past time series $g$ or computing $\\hat{w}$ (see Chapter 2, Q5.4 for the derivation).\n",
    "Specifically, for the explanatory variable matrix $X = [x[0]; x[1];~\\ldots;~ x[T-1]]^\\top \\in \\mathbb{R}^{T \\times N}$, which stores the dynamics of the internal state over $T$ steps, and the corresponding target variable matrix $\\Zeta^\\tau = [\\zeta[-\\tau], \\zeta[-\\tau+1],~\\ldots,~\\zeta[T-\\tau-1]]^\\top \\in \\mathbb{R}^{T \\times 1}$, after performing SVD $X = U\\Sigma V^\\top$, $\\mathrm{MF}[\\tau]$ is calculated as follows:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{MF}[\\tau] &= \\frac{\\|U^\\top \\Zeta^\\tau\\|^2}{\\|\\Zeta^\\tau\\|^2}\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "ただし $X, \\Zeta^\\tau$ ともに正規化 (平均が零となるように変換) されているものとします。\n",
    "\n",
    "[en]: #\n",
    "where both $X$ and $\\Zeta^\\tau$ are assumed to be normalized (average transformed to zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.1.\n",
    "\n",
    "[ja]: #\n",
    "説明変数$X\\in\\mathbb{R}^{T \\times N}$ が与えられる。Xを正規化したのち、$X=U\\Sigma V^\\top$ とSVDを行い、$U\\in\\mathbb{R}^{T \\times N}$ と$X$ の階数 $r$ を出力する関数 `calc_regression_and_rank`を実装せよ。\n",
    "\n",
    "[en]: #\n",
    "Given the explanatory variable $X \\in \\mathbb{R}^{T \\times N}$, normalize $X$, perform SVD $X = U\\Sigma V^\\top$, and implement a function `calc_regression_and_rank` that outputs $U \\in \\mathbb{R}^{T \\times N}$ and the rank $r$ of $X$.\n",
    "\n",
    "[END]: #\n",
    "\n",
    "- `calc_regression_and_rank`\n",
    "  - Argument(s):\n",
    "    - `X`: `np.ndarray`\n",
    "      - `shape`: `(..., t, n)`\n",
    "      - `dtype`: `np.float64`\n",
    "  - Return(s):\n",
    "    - `U`: `np.ndarray`\n",
    "      - `shape`: `(..., t, n)`\n",
    "      - `dtype`: `np.float64`\n",
    "    - `mask`: `np.ndarray`\n",
    "      - `shape`: `(..., n)`\n",
    "      - `dtype`: `np.boolean`\n",
    "    - `rank`: `np.ndarray`\n",
    "      - `shape`: `(...,)`\n",
    "      - `dtype`: `np.int64`\n",
    "  - $10^{2} \\leq T \\leq 10^{3}$\n",
    "  - $1 \\leq N \\leq 10^{2}$\n",
    "\n",
    "[tips]: #\n",
    "- [`np.linalg.svd`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)\n",
    "- [`np.linalg.matrix_rank`](https://numpy.org/devdocs/reference/generated/numpy.linalg.matrix_rank.html)\n",
    "\n",
    "[/tips]: #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_regression_and_rank(X):\n",
    "    T, N = X.shape[-2:]\n",
    "    X = X - X.mean(axis=-2, keepdims=True)  # RIGHT Calculate centered X.\n",
    "    U, sigma, V = np.linalg.svd(X, full_matrices=False)  # RIGHT Use `np.linalg.svd` to perform SVD.\n",
    "    eps = np.finfo(X.dtype).eps\n",
    "    sigma_sq_max = np.max(sigma * sigma, axis=-1, keepdims=True)\n",
    "    eps = sigma_sq_max * (eps * max(T, N))\n",
    "    mask = sigma > eps\n",
    "    rank = mask.sum(axis=-1)  # RIGHT Calculate rank by counting number of singular values greater than `eps`.\n",
    "    return U, mask, rank\n",
    "\n",
    "\n",
    "test_func(calc_regression_and_rank, \"01_01\", multiple_output=True)\n",
    "# show_solution(\"01_01\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.2.\n",
    "\n",
    "[ja]: #\n",
    "`calc_regression_and_rank`で得られた $U \\in \\mathbb{R}^{T\\times N}$ を用いて、$\\mathrm{MF}[\\tau]$ を計算する関数 `calc_memory_function`を実装せよ。\n",
    "\n",
    "[en]: #\n",
    "Implement the function `calc_memory_function` to calculate $\\mathrm{MF}[\\tau]$ using $U \\in \\mathbb{R}^{T\\times N}$ obtained from `calc_regression_and_rank`.\n",
    "\n",
    "[END]: #\n",
    "\n",
    "- `calc_memory_function`\n",
    "  - Argument(s):\n",
    "    - `U`: `np.ndarray`\n",
    "      - `shape`: `(..., t, n)`\n",
    "      - `dtype`: `np.float64`\n",
    "    - `mask`: `np.ndarray`\n",
    "      - `shape`: `(..., n)`\n",
    "      - `dtype`: `np.boolean`\n",
    "    - `zeta`: `np.ndarray`\n",
    "      - `shape`: `(..., t, 1)`\n",
    "      - `dtype`: `np.float64`\n",
    "  - Return(s):\n",
    "    - `r2`: `np.ndarray`\n",
    "      - `shape`: `(..., t, 1)`\n",
    "      - `dtype`: `np.float64`\n",
    "  - $10^{2} \\leq T \\leq 10^{3}$\n",
    "  - $1 \\leq N \\leq 10^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_memory_function(U, mask, zeta):\n",
    "    uzeta = U.swapaxes(-2, -1) @ zeta  # RIGHT Calculate U^T * zeta.\n",
    "    dot = ((uzeta * uzeta) * mask[..., None]).sum(\n",
    "        axis=-2\n",
    "    )  # Calculate dot product considering only components where mask is True.\n",
    "    var = (zeta * zeta).sum(axis=-2)  # RIGHT Calculate variance of zeta.\n",
    "    r2 = dot / var  # RIGHT Calculate R^2 (`dot` divided by `var`).\n",
    "    return r2\n",
    "\n",
    "\n",
    "test_func(calc_memory_function, \"01_02\")\n",
    "# show_solution(\"01_02\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.3. (Advanced)\n",
    "\n",
    "[ja]: #\n",
    "- 相関係数 $\\rho^2$ と決定係数 $\\mathrm{R}^2$ の関係を導出し、それらの一致を確認せよ。\n",
    "- 線形ESN、すなわち活性化関数がなく、かつ入力時系列がi.i.d.であるとき、$\\tau$に対する$\\mathrm{MF}[\\tau]$ の単調減少性を示せ。\n",
    "- 入力時系列がi.i.d.であるとき、$\\mathrm{MC} \\leq N$ を示せ。\n",
    "\n",
    "[en]: #\n",
    "- Derive the relationship between the correlation coefficient $\\rho^2$ and the coefficient of determination $\\mathrm{R}^2$, and confirm that they are equivalent.\n",
    "- Show the monotonic decrease of $\\mathrm{MF}[\\tau]$ with respect to $\\tau$ when using a linear ESN (i.e., without an activation function) and when the input time series is i.i.d.\n",
    "- Prove that the upper bound of $\\mathrm{MC}$ is $N$ when the input is i.i.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "### 2. ESNの記憶容量の計算\n",
    "\n",
    "[en]: #\n",
    "### 2. Calculation of memory capacity of ESN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "ここまで実装した`calc_regression_and_rank`と`calc_memory_function`を用いて、記憶容量を図示化してみましょう。\n",
    "以下のセルはESNの記憶容量を計算し記憶関数をプロットします。\n",
    "入力時系列は一様乱数 $\\mathcal{U}([-1, 1])$ からサンプルし、複数のスペクトル半径 (Spectral Radius; SR) に対して図示化します。\n",
    "\n",
    "[en]: #\n",
    "Let's visualize the memory capacity using the previously implemented `calc_regression_and_rank` and `calc_memory_function`.\n",
    "The following cell calculates the memory capacity of the ESN and plots the memory function.\n",
    "The input time series is sampled from a uniform random distribution $\\mathcal{U}([-1, 1])$, and the visualization is performed for multiple spectral radii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "dim = 50\n",
    "t_washout = 1000\n",
    "t_sample = 20000\n",
    "t_total = t_washout + t_sample\n",
    "display = True\n",
    "\n",
    "rnd = np.random.default_rng(seed)\n",
    "srs = np.array([0.1, 0.5, 0.9])\n",
    "w_in = Linear(1, dim, bound=0.1, bias=0.0, rnd=rnd)\n",
    "\n",
    "net = ESN(dim, sr=srs[:, None], f=np.tanh, p=1, rnd=rnd)\n",
    "# net = ESN(dim, sr=srs[:, None], f=lambda t: t, p=1, rnd=rnd)  # Linear reservoir.\n",
    "# net.weight[:] = np.roll(np.eye(dim), 1, axis=0)  # Ring topology.\n",
    "\n",
    "x0 = np.zeros((srs.shape[0], dim))\n",
    "us = rnd.uniform(-1, 1, (t_total, 1))\n",
    "\n",
    "x = x0\n",
    "xs = np.zeros((t_total, *x0.shape))\n",
    "for idx in trange(t_total, display=display):\n",
    "    x = net(x, w_in(us[idx]))\n",
    "    xs[idx] = x\n",
    "\n",
    "taus = np.arange(0, 81)\n",
    "r2s = None\n",
    "U, mask, ranks = calc_regression_and_rank(xs[t_washout:].swapaxes(0, -2))\n",
    "for idx, tau in enumerate(tqdm(taus, display=display)):\n",
    "    zeta = us[t_washout - tau : t_total - tau]\n",
    "    r2 = calc_memory_function(U, mask, zeta)[..., 0]\n",
    "    if r2s is None:\n",
    "        r2s = np.zeros((taus.shape[0], *r2.shape))\n",
    "    r2s[idx] = r2\n",
    "mcs = np.sum(r2s, axis=0)\n",
    "labels = []\n",
    "for sr, rank, mc in zip(srs, ranks, mcs, strict=True):\n",
    "    labels.append(f\"SR={sr:.2f}, rank={rank}, MC={mc:.2f}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.plot(taus, r2s, \"o-\", label=labels)\n",
    "ax.set_xlim(taus.min() - 0.5, taus.max() + 0.5)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "ax.legend(\n",
    "    loc=\"upper right\",\n",
    "    borderaxespad=0,\n",
    "    ncol=1,\n",
    "    fontsize=12,\n",
    "    frameon=False,\n",
    ")\n",
    "ax.set_xlabel(r\"$\\tau$\", fontsize=14)\n",
    "ax.set_ylabel(r\"$\\mathrm{MF}[\\tau]$\", fontsize=14)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "今度はESNのスペクトル半径を細かく設定し、記憶容量との関係を調べてみましょう。\n",
    "以下のセルはESNの記憶容量を計算し、階数と記憶容量を同時に表示します (デフォルトではスペクトル半径を 0.0から1.5まで0.05刻みで設定されています)。\n",
    "記憶容量が階数を必ず下回る点、スペクトル半径が1に近づくほど記憶容量が大きくなる点を確認してください。\n",
    "\n",
    "[en]: #\n",
    "Let's set the spectral radius of the ESN in more detail this time and examine its relationship with memory capacity.\n",
    "The following cell calculates the memory capacity of the ESN and displays both the rank and the memory capacity simultaneously (by default, the spectral radius is set from 0.0 to 1.5 in increments of 0.05).\n",
    "Confirm that the memory capacity is always less than the rank and that the memory capacity increases as the spectral radius approaches 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "dim = 50\n",
    "t_washout = 1000\n",
    "t_sample = 20000\n",
    "t_total = t_washout + t_sample\n",
    "display = True\n",
    "\n",
    "rnd = np.random.default_rng(seed)\n",
    "srs = np.linspace(0.0, 1.5, 31)\n",
    "w_in = Linear(1, dim, bound=0.1, bias=0.0, rnd=rnd)\n",
    "\n",
    "net = ESN(dim, sr=srs[:, None], f=np.tanh, p=1, rnd=rnd)\n",
    "# net = ESN(dim, sr=srs[:, None], f=lambda t: t, p=1, rnd=rnd)\n",
    "\n",
    "x0 = np.zeros((srs.shape[0], dim))\n",
    "us = rnd.uniform(-1, 1, (t_total, 1))\n",
    "\n",
    "x = x0\n",
    "xs = np.zeros((t_total, *x0.shape))\n",
    "for idx in trange(t_total, display=display):\n",
    "    x = net(x, w_in(us[idx]))\n",
    "    xs[idx] = x\n",
    "\n",
    "taus = np.arange(0, 81)\n",
    "r2s = None\n",
    "U, mask, ranks = calc_regression_and_rank(xs[t_washout:].swapaxes(0, -2))\n",
    "for idx, tau in enumerate(tqdm(taus, display=display)):\n",
    "    zeta = us[t_washout - tau : t_total - tau]\n",
    "    r2 = calc_memory_function(U, mask, zeta)[..., 0]\n",
    "    if r2s is None:\n",
    "        r2s = np.zeros((taus.shape[0], *r2.shape))\n",
    "    r2s[idx] = r2\n",
    "mcs = np.sum(r2s, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.plot(srs, mcs, \"o-\", label=\"MC\")\n",
    "ax.plot(srs, ranks, \"o-\", color=\"k\", label=\"rank\")\n",
    "ax.set_xlim(srs.min() - 0.05, srs.max() + 0.05)\n",
    "# ax.set_ylim(-0.1, dim + 0.1)\n",
    "ax.legend(\n",
    "    loc=\"upper left\",\n",
    "    borderaxespad=0,\n",
    "    bbox_to_anchor=(1.025, 1.0),\n",
    "    ncol=1,\n",
    "    fontsize=12,\n",
    "    frameon=False,\n",
    ")\n",
    "ax.set_xlabel(r\"SR\", fontsize=14)\n",
    "ax.set_ylabel(r\"$\\mathrm{MC}$\", fontsize=14)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "前の章で学んだ最大リアプノフ指数との関連を調べてみましょう。\n",
    "次のセルはESNの最大リアプノフ指数も同時に計算し、MCとの関係を図示化します。\n",
    "スペクトル半径を0.01刻みで2.0まで200点設定しているため、環境によっては実行に若干時間がかかる点に注意してください。\n",
    "\n",
    "[en]: #\n",
    "Let's examine the relationship with the maximum Lyapunov exponent that we learned in the previous chapter.\n",
    "The following cell calculates the maximum Lyapunov exponent of the ESN simultaneously and visualizes its relationship with memory capacity.\n",
    "Note that it may take some time to execute since the spectral radius is set at 200 points in increments of 0.01 up to 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12345\n",
    "dim = 100\n",
    "eps = 1e-4\n",
    "t_washout = 1000\n",
    "t_sample = 10000\n",
    "t_total = t_washout + t_sample\n",
    "ts = np.arange(-t_washout, t_sample)\n",
    "display = True\n",
    "\n",
    "rnd = np.random.default_rng(seed)\n",
    "srs = np.linspace(0.01, 2.0, 200)\n",
    "w_in = Linear(1, dim, bound=0.1, bias=0.0, rnd=rnd)\n",
    "\n",
    "net = ESN(dim, sr=srs[:, None], f=np.tanh, p=1, rnd=rnd)\n",
    "# net = ESN(dim, sr=srs[:, None], f=lambda t: t, p=1, rnd=rnd)\n",
    "\n",
    "x0 = np.zeros((2, srs.shape[0], dim))\n",
    "us = rnd.uniform(-1, 1, (t_total, 1))\n",
    "\n",
    "x = x0\n",
    "xs = np.zeros((t_total, *x0.shape[1:]))\n",
    "lmbds = np.zeros((t_sample, srs.shape[0]))\n",
    "for idx, t in enumerate(tqdm(ts, display=display)):\n",
    "    if t == 0:\n",
    "        pert = rnd.uniform(-1, 1, x[0].shape)\n",
    "        pert = pert / np.linalg.norm(pert, axis=-1, keepdims=True)\n",
    "        x[1] = x[0] + pert * eps\n",
    "    x = net(x, w_in(us[idx]))\n",
    "    xs[idx] = x[0]\n",
    "    if t >= 0:\n",
    "        x_org, x_per = x[0], x[1]\n",
    "        x_diff = x_per - x_org\n",
    "        d_post = np.linalg.norm(x_diff, axis=-1, keepdims=True)\n",
    "        lmbd = np.log(np.abs(d_post / eps))\n",
    "        x_per[:] = x_org + x_diff * (eps / d_post)\n",
    "        lmbds[idx - t_washout] = lmbd[..., 0]\n",
    "\n",
    "taus = np.arange(0, 81)\n",
    "r2s = None\n",
    "U, mask, ranks = calc_regression_and_rank(xs[t_washout:].swapaxes(0, -2))\n",
    "for idx, tau in enumerate(tqdm(taus, display=display)):\n",
    "    zeta = us[t_washout - tau : t_total - tau]\n",
    "    r2 = calc_memory_function(U, mask, zeta)[..., 0]\n",
    "    if r2s is None:\n",
    "        r2s = np.zeros((taus.shape[0], *r2.shape))\n",
    "    r2s[idx] = r2\n",
    "mcs = np.sum(r2s, axis=0)\n",
    "\n",
    "\n",
    "def get_maxima_and_minima(xs, **kwargs):\n",
    "    id_maxima = sp.signal.find_peaks(xs, **kwargs)[0]\n",
    "    id_minima = sp.signal.find_peaks(-xs, **kwargs)[0]\n",
    "    return id_maxima, id_minima\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8, 8), gridspec_kw={\"hspace\": 0.05})\n",
    "axl = ax[0]\n",
    "axl.set_xlim(srs.min() - 0.01, srs.max() + 0.01)\n",
    "axl.set_xticklabels([])\n",
    "for idx, sr in enumerate(srs):\n",
    "    id_maxima, id_minima = get_maxima_and_minima(xs[t_washout:, idx, 0])\n",
    "    id_all = np.concatenate([id_maxima, id_minima])\n",
    "    peaks = xs[t_washout:, idx, 0][id_all]\n",
    "    axl.scatter(sr * np.ones(peaks.shape[0]), peaks, marker=\".\", s=0.01, color=\"k\")\n",
    "axl.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "axl.set_xlabel(r\"$x_0[k]$\", fontsize=14)\n",
    "axl.set_yticks([-1.0, 0.0, 1.0])\n",
    "axl.set_ylim(-1.1, 1.1)\n",
    "\n",
    "axr = ax[0].twinx()\n",
    "axr.plot(srs, lmbds.mean(axis=0), \"o-\", color=\"red\", label=\"MLE\")\n",
    "axr.set_yticks([-0.2, 0.0, 0.2])\n",
    "axr.set_ylim(-0.22, 0.22)\n",
    "axr.set_ylabel(r\"MLE: $\\lambda$\", fontsize=14)\n",
    "axr.set_xticklabels([])\n",
    "axr.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "\n",
    "ax[1].plot(srs, mcs, \"o-\", label=\"MC\")\n",
    "ax[1].set_xlim(srs.min() - 0.01, srs.max() + 0.01)\n",
    "ax[1].set_xlabel(r\"SR\", fontsize=14)\n",
    "ax[1].set_ylabel(r\"$\\mathrm{MC}$\", fontsize=14)\n",
    "ax[1].tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.1. (Advanced)\n",
    "\n",
    "[ja]: #\n",
    "- パラメータを様々に変更して挙動がどう変化するか観察せよ。特に活性化関数とMF/MCとの関係を調査せよ。\n",
    "- 入力分布は上のデモンストレーションでは一様乱数を用いているが、他の分布（例えば正規分布やベルヌーイ分布）を用いても良い。入力分布を変更した場合、MF/MCはどう変化するか観察せよ。\n",
    "\n",
    "[en]: #\n",
    "- Observe how the behavior changes by varying the parameters. In particular, investigate the relationship between the activation function and memory function/capacity.\n",
    "- In the demonstration above, the input distribution uses uniform random noise, but other distributions (e.g., normal distribution or Bernoulli distribution) can also be used. Observe how memory function/capacity changes when the input distribution is altered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "## 引用\n",
    "\n",
    "[en]: #\n",
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Jaeger, H. (2001). *Short term memory in echo state networks*. GMD Forschungszentrum Informationstechnik. https://doi.org/10.24406/publica-fhg-291107"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rc-bootcamp (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
