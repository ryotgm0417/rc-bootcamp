{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "# Chapter 7. 情報処理容量\n",
    "\n",
    "[en]: #\n",
    "# Chapter 7. Information Processing Capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "この章では、前章の記憶容量を拡張した指標である情報処理容量（Information Processing Capacity; IPC）の計算方法を学習します。\n",
    "また前章同様、階数やリアプノフ指数などの力学系の性質と、得られる情報処理容量との間の関係を学びましょう。\n",
    "\n",
    "**注意:** この章の後半部では、GPUを使用した環境が推奨されます。手元のPCにGPUがない場合はGoogle Colaboratory上での実行をお勧めします。\n",
    "\n",
    "[en]: #\n",
    "In this chapter, we will learn how to calculate the information processing capacity (IPC), an extended metric of the memory capacity introduced in the previous chapter.\n",
    "As in the previous chapter, we will also explore the relationship between the properties of dynamical systems, such as rank and dynamics, and the resulting IPC.\n",
    "\n",
    "**Note:** In the latter part of this chapter, an environment with GPU support is recommended.\n",
    "If your local PC does not have a GPU, it is recommended to run it on Google Colaboratory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "## 前書き\n",
    "\n",
    "[en]: #\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "情報処理容量はJ. Dambreら<sup>[1]</sup>が提案した記憶容量を拡張した指標で、入力時系列に対してどのような計算 (記憶と非線形性) を力学系が行っているかを評価します。\n",
    "前章と同じく以下の式で表される $1$ 入力 $N$ 次元の力学系 $x[k]$ とある線形写像 $g: \\mathbb{R}^N \\to \\mathbb{R}$ による出力 $\\hat{y}[k]$ を考えます。\n",
    "\n",
    "[en]: #\n",
    "IPC is an extended metric of memory capacity proposed by J. Dambre et al.<sup>[1]</sup>, which evaluates what kind of computations (memory and nonlinearity) a dynamical system performs on input time series.\n",
    "As in the previous chapter, we consider a single-input $N$-dimensional dynamical system $x[k]$ represented by the following equations and the output $\\hat{y}[k]$ obtained through a certain linear mapping $g: \\mathbb{R}^N \\to \\mathbb{R}$:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\renewcommand{\\Tau}{\\mathrm{T}}\n",
    "\\renewcommand{\\Zeta}{\\mathrm{Z}}\n",
    "\\begin{align*}\n",
    "x[k+1] &= f \\left(x[k],\\zeta[k+1]\\right) \\\\\n",
    "\\hat{y}[k] &= g(x[k])\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "また入力時系列 $\\zeta[k]$ は零平均で定常的かつi.i.d.であると仮定します。\n",
    "ここで新たに 以下の式で定義される容量 $\\mathrm{C}$ を導入します。\n",
    "\n",
    "[en]: #\n",
    "where the input time series $\\zeta[k]$ is assumed to be zero-mean, stationary, and i.i.d.\n",
    "Here, we introduce a new capacity $\\mathrm{C}$ defined by the following equation:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{C}[x, z] := \\mathrm{R}^2[z, x]\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "式に示されているとおり$\\mathrm{C}(x, z)$ は目標時系列 $z$ をどれほど $x$ から再構成できるかを定量化した指標で、決定係数 $\\mathrm{R}^2$ を用いて計算され、0から1の範囲を取ります。\n",
    "前章で学習した記憶関数は以下のとおり $C$ を用いて表現されます。\n",
    "\n",
    "[en]: #\n",
    "As shown in the equation, $\\mathrm{C}(x, z)$ is a metric that quantifies how well the target time series $z$ can be reconstructed from $x$, calculated using the coefficient of determination $\\mathrm{R}^2$, and it takes values in the range [0, 1].\n",
    "The memory function, learned in the previous chapter, is expressed using $\\mathrm{C}$ as follows:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{MF}[\\tau] &= \\mathrm{C}[x, \\zeta^\\tau]\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "記憶容量$\\mathrm{MC}$ は $\\tau$ を変えて全過去に対して $\\mathrm{C}$ を計算し総和を取って計算されました。\n",
    "言い換えれば記憶容量は、過去入力 $[\\zeta[k], \\zeta[k-1], \\zeta[k-2],~\\ldots]$ の**線形**な変換を、現在の状態 $x[k]$ (の線形変換) からどれほど回収できるかを定量化した指標といえます。\n",
    "一方で情報処理容量の計算では、**非線形**な範囲まで考慮・拡張してどれほど再構成できるかを評価します。\n",
    "\n",
    "さて目標として設定する過去入力の非線形な変換 $z$ をどのように構成すれば良いでしょうか？\n",
    "ここで、目標時系列の直交性と網羅性、すなわち重複なくすべてのパターンを考慮しなければならない点に注意しなければなりません。\n",
    "なぜならば線形従属な目標を許容すると、総容量はいくらでも大きくできてしまうからです。\n",
    "記憶容量の計算の際、各記憶関数の値 $\\mathrm{C}[x, \\zeta^\\tau]$ の単純な総和を取って求められたのは、入力の i.i.d.性を仮定しており、$\\tau_1 \\neq \\tau_2$ の時 $\\zeta^{\\tau_1}$ と $\\zeta^{\\tau_2}$ が線形独立で直交していたからです ($\\mathrm{E}[\\zeta^{\\tau_1} \\zeta^{\\tau_2}] = 0$)。\n",
    "\n",
    "情報処理容量の計算では **直交多項式**<sup>[2]</sup>と呼ばれる道具を用いて、目標時系列を重複なく網羅的に構成します。\n",
    "直交多項式は**次数**と呼ばれる整数のパラメータを持ちます。\n",
    "一般に $d$ の値が大きいほど非線形性が強くなります (逆に $d=0$ のときは定数、$d=1$ のときは線形変換に対応)。\n",
    "$d$ 次の直交多項式を $\\mathcal{P}_d$ と表記します。\n",
    "\n",
    "[en]: #\n",
    "The memory capacity $\\mathrm{MC}$ is calculated by summing $\\mathrm{C}$ over all past inputs by varying $\\tau$.\n",
    "In other words, memory capacity can be interpreted as a metric that quantifies how well the **linear** transformation of past inputs $[\\zeta[k], \\zeta[k-1], \\zeta[k-2],~\\ldots]$ can be recovered from the current state $x[k]$ (or its linear transformation).\n",
    "On the other hand, the calculation of IPC evaluates how well reconstruction can be achieved, extending the scope to include **nonlinear** transformations.\n",
    "\n",
    "Now, how should we construct the nonlinear transformation $z$ of past inputs that we aim to evaluate? The points to consider here are the orthogonality and completeness of the target time series, meaning that all patterns must be considered without duplication.\n",
    "This is because allowing linearly dependent targets would make the total capacity arbitrarily large.\n",
    "In the calculation of memory capacity, the simple summation of the values of each memory function $\\mathrm{C}[x, \\zeta^\\tau]$ was valid because the i.i.d. nature of the input was assumed.\n",
    "That is, for $\\tau_1 \\neq \\tau_2$, $\\zeta^{\\tau_1}$ and $\\zeta^{\\tau_2}$ were linearly independent and orthogonal ($\\mathrm{E}[\\zeta^{\\tau_1} \\zeta^{\\tau_2}] = 0$).\n",
    "\n",
    "In the calculation of IPC, a tool called **orthogonal polynomials**<sup>[2]</sup> is used to construct target time series comprehensively without duplication.\n",
    "Orthogonal polynomials have an integer parameter called **degree**.\n",
    "Generally, the larger the value of $d$, the stronger the nonlinearity (conversely, $d=0$ corresponds to a constant, and $d=1$ corresponds to a linear transformation).\n",
    "Orthogonal polynomials of degree $d$ are denoted as $\\mathcal{P}_d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "まず記憶容量のときに用いられた1次の目標時系列は、$d=1$次の直交多項式 $\\mathcal{P}_1$ を用いて改めて以下の形で表記できます。\n",
    "\n",
    "[en]: #\n",
    "First, the first-order target time series used in memory capacity can be expressed again in the following form using the first-degree orthogonal polynomial $\\mathcal{P}_1$:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{P}_1(\\zeta^0),~\\mathcal{P}_1(\\zeta^1),~\\mathcal{P}_1(\\zeta^2),~\\mathcal{P}_1(\\zeta^3),~\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_1(\\zeta^5),~\\ldots\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "次に2次の目標時系列を見ていきましょう。ここでは2次の直交多項式 $\\mathcal{P}_2$ を用いて、以下のように列挙されます。\n",
    "\n",
    "[en]: #\n",
    "Next, let us consider second-order target time series.\n",
    "Here, using the second-degree orthogonal polynomial $\\mathcal{P}_2$, they are enumerated as follows:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\mathcal{P}_2(\\zeta^0),~\\mathcal{P}_2(\\zeta^1),~\\mathcal{P}_2(\\zeta^2),~\\mathcal{P}_2(\\zeta^3),~\\mathcal{P}_2(\\zeta^4),~\\mathcal{P}_2(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^1),~\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^2),~\\mathcal{P}_{1}(\\zeta^0)\\mathcal{P}_1(\\zeta^3),~\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^2),~\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^3),~\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^3),~\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^3)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_1(\\zeta^3)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^4)\\mathcal{P}_1(\\zeta^5),\\ldots\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "ここで2次の目標時系列として、$\\mathcal{P}_2$を使った要素だけでなく、$\\mathcal{P}_{1}$同士の積 (すなわち$1+1=2$) も含まれる点に注意してください。\n",
    "直交しているのでいずれの目標時系列も線形独立で、その間の内積は $0$ になります。\n",
    "\n",
    "3次の目標時系列も同様に列挙されます。合計次数が3となる足し算のパターンは$3,~2+1, 1+1+1$ の3パターンあるため以下のとおりより考慮されるパターンが増えます。\n",
    "\n",
    "[en]: #\n",
    "Here, note that as second-order target time series, not only the elements using $\\mathcal{P}_2$ but also the products of $\\mathcal{P}_{1}$ (i.e., $1+1=2$) are included.\n",
    "Since they are orthogonal, all target time series are linearly independent, and the inner product between them is $0$.\n",
    "\n",
    "Third-order target time series are also enumerated similarly.\n",
    "Since there are three patterns of addition that result in a total degree of 3: $3,~2+1, 1+1+1$, the number of considered patterns increases as follows:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\mathcal{P}_3(\\zeta^0),~\\mathcal{P}_3(\\zeta^1),~\\mathcal{P}_3(\\zeta^2),~\\mathcal{P}_3(\\zeta^3),~\\mathcal{P}_3(\\zeta^4),~\\mathcal{P}_3(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_2(\\zeta^0)\\mathcal{P}_1(\\zeta^1),~\\mathcal{P}_2(\\zeta^0)\\mathcal{P}_1(\\zeta^2),~\\mathcal{P}_2(\\zeta^0)\\mathcal{P}_1(\\zeta^3),~\\mathcal{P}_2(\\zeta^0)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_2(\\zeta^0)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_2(\\zeta^1)\\mathcal{P}_1(\\zeta^2),~\\mathcal{P}_2(\\zeta^1)\\mathcal{P}_1(\\zeta^3),~\\mathcal{P}_2(\\zeta^1)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_2(\\zeta^1)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_2(\\zeta^2)\\mathcal{P}_1(\\zeta^3),~\\mathcal{P}_2(\\zeta^2)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_2(\\zeta^2)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_2(\\zeta^3)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_2(\\zeta^3)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_2(\\zeta^4)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^2),~\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^3),~\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^3),~\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^3)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^3)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^4)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^3),~\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^3)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^3)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^4)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^3)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^3)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^4)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^3)\\mathcal{P}_1(\\zeta^4)\\mathcal{P}_1(\\zeta^5),~\\ldots\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "$d=4$ 以降も同様かつ爆発的に組み合わせが増えていきます。\n",
    "これらを一般化し次数を表す整数の集合 $D=\\{d_1,d_2,~\\ldots,~d_m\\}$ と同じ要素数を持つ時間遅れを表す整数の集合 $\\Tau=\\{\\tau_1, \\tau_2,~\\ldots,~\\tau_m\\}$ を用いてある目標時系列 $\\zeta^{D,\\Tau}$ を以下の式で定義します。\n",
    "\n",
    "[en]: #\n",
    "The combinations continue to increase explosively for $d = 4$ and beyond as well.\n",
    "We generalize these and define a target time series $\\zeta^{D,\\Tau}$ using the following equation, with $D=\\{d_1,d_2,~\\ldots,~d_m\\}$ being a set of integers representing degrees and $\\Tau=\\{\\tau_1, \\tau_2,~\\ldots,~\\tau_m\\}$ being a set of integers representing time delays, both having the same number of elements:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\zeta^{D,\\Tau}[k] :=& \\mathcal{P}_{d_1}(\\zeta[k-\\tau_1])\\mathcal{P}_{d_2}(\\zeta[k-\\tau_2])\\cdots \\mathcal{P}_{d_m}(\\zeta[k-\\tau_m]) \\\\\n",
    "=& \\prod_{i=1}^m \\mathcal{P}_{d_i}(\\zeta[k-\\tau_i])\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "$d$ 次の情報処理容量 $\\mathrm{C}^d$ は $\\sum_i d_i = d$ となる $D$ に対する目標時系列の容量の総和で以下の式で定義されます。\n",
    "\n",
    "[en]: #\n",
    "The $d$-th order IPC $\\mathrm{C}^d$ is defined as the sum of capacities for target time series corresponding to $D$ such that $\\sum_i d_i = d$:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{C}^d[x, \\zeta] := \\sum_{\\substack{D~\\mathrm{s.t.}\\\\ \\sum_i d_i=d}} \\sum_{\\Tau} \\mathrm{C}[x, \\zeta^{D,\\Tau}]\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "定義から $\\mathrm{MC}=\\mathrm{C}^1$ であるとわかります。\n",
    "これが情報処理容量が記憶容量を拡張した指標であるとされる理由です。\n",
    "さらに総容量 $C^\\mathrm{tot}$ は以下の式で定義されます。\n",
    "\n",
    "[en]: #\n",
    "From the definition, we have $\\mathrm{MC}=\\mathrm{C}^1$.\n",
    "This is why IPC is an extended metric of memory capacity.\n",
    "The total capacity $\\mathrm{C}^\\mathrm{tot}$ is defined as:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{C}^\\mathrm{tot}[x, \\zeta] &:= \\sum_{d=1}^{\\infty} \\mathrm{C}^d[x, \\zeta]\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "J. Dambreら<sup>[1]</sup>はこの情報処理容量の上限に関して以下の不等式を示しました (導出は発展課題)。\n",
    "\n",
    "[en]: #\n",
    "J. Dambre et al.<sup>[1]</sup> demonstrated the following inequality regarding the upper limit of this IPC (derivation is an advanced exercise):\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{C}^\\mathrm{tot}[x, \\zeta] \\leq r \\leq N\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "ここで $r$ は力学系の階数を表します。\n",
    "これは記憶容量同様に情報処理容量の上限が、内部状態の線形独立な次元数に制限される点を示しています。\n",
    "\n",
    "この指標によって導かれた結果は特に物理リザバー計算 (Physical Reservoir Computing; PRC) において重要な示唆を与えます。\n",
    "PRCでは通常設置されるセンサの数がそのまま内部状態の次元数に対応し、線形独立なセンサ時系列の数がそのまま情報処理容量の上限を表すからです。\n",
    "また情報処理容量はどのような変換が行われているのか全網羅的に評価するため、物理系そのものの特性の評価にも役立ちます。\n",
    "\n",
    "[en]: #\n",
    "where $r$ represents the rank of the dynamical system.\n",
    "This indicates that, similar to memory capacity, the upper limit of IPC is constrained by the number of linearly independent dimensions of the internal states.\n",
    "\n",
    "The results derived from this metric provide particularly important insights for physical reservoir computing (PRC).\n",
    "In PRC, the number of sensors typically installed directly corresponds to the dimensionality of the internal states, and the number of linearly independent sensor time series directly represents the upper limit of IPC.\n",
    "Moreover, since IPC comprehensively evaluates what kinds of transformations are performed, it is also useful for assessing the characteristics of the physical system itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "## 演習問題と実演\n",
    "\n",
    "[en]: #\n",
    "## Exercises and demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "ここからは演習問題とデモンストレーションに移ります。\n",
    "前回と同じライブラリの他、前回の演習で実装した`ESN`・`Linear`・`narma_func`が`import`により利用できます。\n",
    "初めに次のセルを実行してください。\n",
    "\n",
    "なお`ESN`・`Linear`・`narma_func`の内部実装を再確認するには、`import inspect`以下の行をコメントアウトするか`...?? / ??...`を使用してください。\n",
    "\n",
    "[en]: #\n",
    "Let's move on to the exercises and demonstrations.\n",
    "Along with the basic libraries from the previous chapter, you can import and use the `ESN`, `Linear`, and `narma_func` we implemented earlier.\n",
    "Please run the following cell.\n",
    "\n",
    "You can view the implementations of `ESN`, `Linear`, and `narma_func` by uncommenting the lines after `import inspect` or by using `...?? / ??...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import drive  # type: ignore\n",
    "\n",
    "    if False:  # Set to True if you want to use Google Drive and save your work there.\n",
    "        drive.mount(\"/content/gdrive\")\n",
    "        %cd /content/gdrive/My Drive/[[PROJECT_NAME]]/\n",
    "        # NOTE: Change it to your own path if you put the zip file elsewhere.\n",
    "        # e.g., %cd /content/gdrive/My Drive/[PATH_TO_EXTRACT]/[[PROJECT_NAME]]/\n",
    "    else:\n",
    "        pass\n",
    "        %cd /content/\n",
    "        !git clone --branch [[BRANCH_NAME]] https://github.com/rc-bootcamp/[[PROJECT_NAME]].git\n",
    "        %cd /content/[[PROJECT_NAME]]/\n",
    "else:\n",
    "    sys.path.append(\".\")\n",
    "\n",
    "from ipc_module.helper import visualize_dataframe\n",
    "from ipc_module.profiler import UnivariateProfiler, UnivariateViewer\n",
    "from utils.reservoir import ESN, Linear\n",
    "from utils.style_config import Figure, plt\n",
    "from utils.tester import load_from_chapter_name\n",
    "from utils.tqdm import tqdm, trange\n",
    "\n",
    "test_func, show_solution = load_from_chapter_name(\"07_information_processing_capacity\")\n",
    "\n",
    "# Uncomment it to see the implementations of `Linear` and `ESN`.\n",
    "# import inspect\n",
    "# print(inspect.getsource(Linear))\n",
    "# print(inspect.getsource(ESN))\n",
    "\n",
    "# Or just use ??.../...?? (uncomment the following lines).\n",
    "# Linear??\n",
    "# ESN??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "### 1. ルジャンドル多項式と直交性の確認\n",
    "\n",
    "[en]: #\n",
    "### 1. Legendre polynomials and verification of orthogonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "ここまで直交多項式を $\\mathcal{P}$ とおいて説明しましたが、具体的な多項式を導入して議論しましょう。\n",
    "実はこの直交多項式は入力時系列が従う分布に依存します。\n",
    "例えば入力時系列 $\\zeta[k]$ が 一様乱数 $\\mathcal{U}([-1, 1])$ に従う場合、以下の漸化式で定義される[ルジャンドル多項式](https://ja.wikipedia.org/wiki/%E3%83%AB%E3%82%B8%E3%83%A3%E3%83%B3%E3%83%89%E3%83%AB%E5%A4%9A%E9%A0%85%E5%BC%8F)を使用できます。\n",
    "\n",
    "[en]: #\n",
    "So far, we have explained orthogonal polynomials denoted as $\\mathcal{P}$, but let's introduce specific polynomials for discussion.\n",
    "These orthogonal polynomials depend on the distribution of the input time series.\n",
    "For example, if the input time series $\\zeta[k]$ follows a uniform random distribution $\\mathcal{U}([-1, 1])$, we can use the [Legendre polynomials](https://en.wikipedia.org/wiki/Legendre_polynomials) defined by the following recurrence relation:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "(n+1)\\mathcal{P}_{n+1}(z) &= (2n+1)z\\mathcal{P}_n(z) - n\\mathcal{P}_{n-1}(z)\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "ただし$\\mathcal{P}_0(z)=1,~\\mathcal{P}_1(z)=z$ とします。\n",
    "これを式展開すると以下のような多項式が得られます。\n",
    "\n",
    "[en]: #\n",
    "where $\\mathcal{P}_0(z)=1$ and $\\mathcal{P}_1(z)=z$.\n",
    "Expanding this equation yields the following polynomials:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{P}_0(z) &= 1 \\\\\n",
    "\\mathcal{P}_1(z) &= z \\\\\n",
    "\\mathcal{P}_2(z) &= \\frac{1}{2}(3z^2-1) \\\\\n",
    "\\mathcal{P}_3(z) &= \\frac{1}{2}(5z^3-3z) \\\\\n",
    "\\mathcal{P}_4(z) &= \\frac{1}{8}(35z^4-30z^2+3) \\\\\n",
    "\\mathcal{P}_5(z) &= \\frac{1}{8}(63z^5-70z^3+15z) \\\\\n",
    "\\mathcal{P}_6(z) &= \\frac{1}{48}(231z^6-315z^4+105z^2-5)\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "一方で直交性は内積計算の結果により評価できます。\n",
    "いま2つの目標時系列 $\\zeta^A:=\\zeta^{D_A,\\Tau_A}, \\zeta^B:=\\zeta^{D_B,\\Tau_B}$ からそれぞれ$T$ ステップ分抽出された2つの目標時系列行列 $\\Zeta^A = [\\zeta^A[0]; \\zeta^A[1];~\\ldots;~\\zeta^A[T-1]] \\in \\mathbb{R}^{T \\times 1}$ と $\\Zeta^B = [\\zeta^B[0]; \\zeta^B[1];~\\ldots;~\\zeta^B[T-1]]  \\in \\mathbb{R}^{T \\times 1}$を用意します。 $\\Zeta^A$と$\\Zeta^B$の内積 $I(\\Zeta^A, \\Zeta^B)$ は以下のように計算できます。\n",
    "\n",
    "[en]: #\n",
    "Orthogonality, on the other hand, can be evaluated based on the result of inner product calculations.\n",
    "Now, consider two target time series $\\zeta^A := \\zeta^{D_A,\\Tau_A}$ and $\\zeta^B := \\zeta^{D_B,\\Tau_B}$, from which two target time series matrices $\\Zeta^A = [\\zeta^A[0]; \\zeta^A[1];~\\ldots;~\\zeta^A[T-1]] \\in \\mathbb{R}^{T \\times 1}$ and $\\Zeta^B = [\\zeta^B[0]; \\zeta^B[1];~\\ldots;~\\zeta^B[T-1]] \\in \\mathbb{R}^{T \\times 1}$ are extracted for $T$ steps.\n",
    "The inner product $I(\\Zeta^A, \\Zeta^B)$ of $\\Zeta^A$ and $\\Zeta^B$ can be calculated as follows:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "I(\\Zeta^A, \\Zeta^B) &= \\sum_{t=1}^{T} \\frac{\\Zeta^A_t}{\\sqrt{\\sum_{t=1}^T (\\Zeta^A_t)^2}} \\cdot \\frac{\\Zeta^B_t}{\\sqrt{\\sum_{t=1}^T (\\Zeta^B_t)^2}}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "直交性により $I$ の期待値は以下のとおり計算されます。\n",
    "\n",
    "[en]: #\n",
    "The expected value of $I$ based on orthogonality is calculated as follows:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{E}[I(\\Zeta^A, \\Zeta^B)] &= \\begin{cases}\n",
    "1 & \\mathrm{if}~\\zeta^A = \\zeta^B~(\\Leftrightarrow  D_A=D_B \\land \\Tau_A=\\Tau_B) \\\\\n",
    "0 & \\mathrm{if}~\\zeta^A \\perp \\zeta^B~(\\mathrm{otherwise}) \\\\\n",
    "\\end{cases}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "演習問題で実際にルジャンドル多項式と内積計算を実装し、これらを確認しましょう。\n",
    "\n",
    "[en]: #\n",
    "Let us implement the Legendre polynomials and inner product calculations in the exercises to verify these properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.1\n",
    "\n",
    "[ja]: #\n",
    "上記の漸化式を参考に、ルジャンドル多項式を計算するクラス`Legendre`内のメソッド`Legendre._calc`を完成させよ。\n",
    "なお`Legendre._calc`は時系列 $\\Zeta$ に対して $\\mathcal{P}_n(\\Zeta)$ を計算する。\n",
    "結果は一様乱数 $\\mathcal{U}([-1, 1])$ からサンプルされた長さ $T$ の時系列より結果は検証される。\n",
    "\n",
    "[en]: #\n",
    "Based on the above recurrence relation, complete the method `Legendre._calc` in the class `Legendre` to compute the Legendre polynomial. Note that `Legendre._calc` calculates $\\mathcal{P}_n(\\Zeta)$ for the time series $\\Zeta$. The result will be validated against a time series of length $T$ sampled from a uniform random distribution $\\mathcal{U}([-1, 1])$.\n",
    "\n",
    "[END]: #\n",
    "- `Legendre._calc`\n",
    "  - Argument(s):\n",
    "    - `n`: `int`\n",
    "      - `n >= 0`\n",
    "  - Operation(s):\n",
    "    - Update `self._cache[n]`\n",
    "- $10 \\leq T \\leq 10^{3}$, $1\\leq n \\leq 20$\n",
    "\n",
    "[tips]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "n\\mathcal{P}_{n}(z) &= (2n-1)z\\mathcal{P}_{n-1}(z) - (n-1)\\mathcal{P}_{n-2}(z) ~\\mathrm{for}~n \\geq 2 \\\\\n",
    ".\\end{align*}\n",
    "$$\n",
    "[/tips]: #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Legendre(object):\n",
    "    def __init__(self, xs):\n",
    "        self.xs = xs\n",
    "        self._caches = {}\n",
    "        self._caches[0] = 1\n",
    "        self._caches[1] = self.xs\n",
    "\n",
    "    def __getitem__(self, deg):\n",
    "        assert deg >= 0\n",
    "        if deg not in self._caches:\n",
    "            self._caches[deg] = self._calc(deg)\n",
    "        return self._caches[deg]\n",
    "\n",
    "    def _calc(self, n: int):\n",
    "        # BEGIN Use `self.xs` and `self[n-1]`, `self[n-2]` to calculate the n-th Legendre polynomial.\n",
    "        res = ((2 * n - 1) / n) * self.xs * self[n - 1]\n",
    "        res -= ((n - 1) / n) * self[n - 2]\n",
    "        return res\n",
    "        # END\n",
    "\n",
    "\n",
    "def solution(us, n):\n",
    "    # DO NOT CHANGE HERE.\n",
    "    poly = Legendre(us)\n",
    "    return poly[n]\n",
    "\n",
    "\n",
    "test_func(solution, \"01_01\")\n",
    "# show_solution(\"01_01\", \"Legendre\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.2.\n",
    "\n",
    "[ja]: #\n",
    "上の式に基づき、２つの時系列 $A \\in \\mathbb{R}^{T}$ と $B\\in \\mathbb{R}^{T}$ の内積を計算するメソッド`calc_inner_product`を完成させよ。\n",
    "\n",
    "[en]: #\n",
    "Based on the above equation, complete the method `calc_inner_product` to calculate the inner product of two time series $A \\in \\mathbb{R}^{T}$ and $B \\in \\mathbb{R}^{T}$.\n",
    "\n",
    "[END]: #\n",
    "\n",
    "- `calc_inner_product`\n",
    "  - Argument(s):\n",
    "    - `a`: `np.ndarray`\n",
    "      - `shape`: `(t,)`\n",
    "      - `dtype`: `np.float64`\n",
    "  - Return(s):\n",
    "    - `b`: `np.ndarray`\n",
    "      - `shape`: `(t,)`\n",
    "      - `dtype`: `np.float64`\n",
    "- $10 \\leq T \\leq 10^3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_inner_product(a, b):\n",
    "    # BEGIN Calculate and return the inner product between vectors a and b.\n",
    "    return a.dot(b) / np.linalg.norm(a) / np.linalg.norm(b)\n",
    "    # END\n",
    "\n",
    "\n",
    "test_func(calc_inner_product, \"01_02\")\n",
    "# show_solution(\"01_02\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "さて実際に多項式の直交性を確認しましょう。\n",
    "まずは[Wikipediaのルジャンドル多項式の図表](https://en.wikipedia.org/wiki/Legendre_polynomials#/media/File:Legendrepolynomials6.svg)を正しく再現できるか確認します。\n",
    "\n",
    "[en]: #\n",
    "Now, let's verify their orthogonality.\n",
    "First, let's check that we can reproduce the [figure of Legendre polynomials from Wikipedia](https://en.wikipedia.org/wiki/Legendre_polynomials#/media/File:Legendrepolynomials6.svg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Legendre_polynomials\n",
    "us = np.linspace(-1, 1, 1000)\n",
    "poly = Legendre(us)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "for deg in range(1, 6):\n",
    "    ax.plot(us, poly[deg], label=r\"$\\mathcal{P}_\" + f\"{{{deg}}}$\", color=f\"C{deg}\")\n",
    "ax.legend(\n",
    "    loc=\"upper left\",\n",
    "    fontsize=12,\n",
    "    bbox_to_anchor=(1.025, 1.0),\n",
    "    borderaxespad=0,\n",
    "    frameon=False,\n",
    ")\n",
    "ax.tick_params(axis=\"both\", labelsize=12)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "次に一様乱数 $\\mathcal{U}([-1, 1])$ によって時系列を生成し、時間遅れとルジャンドル多項式によって様々な目標時系列を生成し、それらの間の内積を計算してみましょう。\n",
    "`degree_delay_list` の各要素は $\\zeta^{D,\\Tau}$ における次数の集合 $D$ と時間遅れの集合 $\\Tau$ の組を指定します。\n",
    "色々変えてみて直交性が保たれているか確認してみましょう。\n",
    "\n",
    "[en]: #\n",
    "Next, generate a time series using the uniform random distribution $\\mathcal{U}([-1, 1])$, create various target time series using time delay and Legendre polynomials, and calculate the inner products between them.\n",
    "Each element of `degree_delay_list` specifies a pair of degree sequence $D$ and time delay sequence $\\Tau$ in $\\zeta^{D,\\Tau}$.\n",
    "Try changing them in various ways and check if orthogonality is maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "t_washout, t_sample = 100, 10000\n",
    "\n",
    "rnd = np.random.default_rng(seed)\n",
    "t_total = t_washout + t_sample\n",
    "us = rnd.uniform(-1, 1, t_total)\n",
    "poly = Legendre(us)\n",
    "\n",
    "degree_delay_list = [\n",
    "    ([1], [0]),\n",
    "    ([1], [10]),\n",
    "    ([1, 1], [1, 2]),\n",
    "    ([2], [0]),\n",
    "    ([3], [5]),\n",
    "]  # You can add or change more combinations if you want.\n",
    "\n",
    "\n",
    "def create_poly(args):\n",
    "    degrees, taus = args\n",
    "    out = 1\n",
    "    for deg, tau in zip(degrees, taus, strict=True):\n",
    "        out *= poly[deg][t_washout - tau : t_total - tau]\n",
    "    return out\n",
    "\n",
    "\n",
    "def create_label(args):\n",
    "    degrees, taus = args\n",
    "    out = r\"$\"\n",
    "    for d, t in zip(degrees, taus, strict=True):\n",
    "        if t == 0:\n",
    "            out += f\"P_{{{d}}}(\\\\zeta)\"\n",
    "        else:\n",
    "            out += f\"P_{{{d}}}(\\\\zeta^{{{t}}})\"\n",
    "    out += r\"$\"\n",
    "    return out\n",
    "\n",
    "\n",
    "length = len(degree_delay_list)\n",
    "polys = list(map(create_poly, degree_delay_list))\n",
    "labels = list(map(create_label, degree_delay_list))\n",
    "products = np.zeros((length, length))\n",
    "\n",
    "for idx, idy in itertools.product(range(length), range(length)):\n",
    "    products[idx, idy] = calc_inner_product(polys[idx], polys[idy])\n",
    "\n",
    "fig = Figure(figsize=(8, 6))\n",
    "ax = fig[0]\n",
    "im, cb = ax.plot_matrix(\n",
    "    products,\n",
    "    cmap=\"Blues\",\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    aspect=\"equal\",\n",
    "    colorbar=True,\n",
    ")\n",
    "ax.set_xticks(range(length))\n",
    "ax.set_yticks(range(length))\n",
    "ax.set_xticklabels(labels, fontsize=10)\n",
    "ax.set_yticklabels(labels, fontsize=10)\n",
    "cb.ax.tick_params(labelsize=12)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.3 (Advanced)\n",
    "\n",
    "[ja]: #\n",
    "- 上のデモでは時系列の長さ`t_sample`によって内積の値が変化する。時系列の長さが短いとき、直交していても$I$の値が0近くにならない点を確認せよ。\n",
    "- 一様乱数$\\mathcal{U}([-1, 1])$の代わりに標準正規分布 $\\mathcal{N}(0, 1)$を用いる時代わりに[Hermite多項式](https://en.wikipedia.org/wiki/Hermite_polynomials#Recurrence_relation)を使用しなければならない<sup>[2]</sup>。 クラス`Hermite`を実装し、Hermite多項式を計算できるようにし、同様に直交性を確認せよ。\n",
    "\n",
    "[en]: #\n",
    "- In the above demo, the value of the inner product $I$ changes depending on the time series length `t_sample`. Confirm that when the time series length is short, the value of $I$ does not approach 0 even if they are orthogonal.\n",
    "- Instead of using the uniform random distribution $\\mathcal{U}([-1, 1])$, when using the standard normal distribution $\\mathcal{N}(0, 1)$, [Hermite polynomials](https://en.wikipedia.org/wiki/Hermite_polynomials#Recurrence_relation) must be used instead<sup>[2]</sup>. Implement the class `Hermite` to calculate Hermite polynomials and similarly verify their orthogonality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "### 2. 情報処理容量の実装と確認\n",
    "\n",
    "[en]: #\n",
    "### 2. Implementation and verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "さて実際に情報処理容量を計算してみましょう。\n",
    "まず $N=10$で活性化関数 $\\tanh$ のESNと、一様乱数 $\\mathcal{U}([-1, 1])$ に従う入力時系列 $\\zeta[k]$ を用意し、その時のダイナミクス $x[k]$ をサンプルします。\n",
    "非対称性を確保するため、$\\zeta[k]$ は $[0, 1]$の範囲を取るようにスケーリングされます\n",
    "(入力を非対称にしたのは$\\tanh$が奇関数であるため、対称入力に対しては奇数次の成分の容量しか出現しないからです[確認は発展課題])。\n",
    "\n",
    "[en]: #\n",
    "Now, let's actually calculate the IPC.\n",
    "First, prepare an ESN with $N=10$ and the activation function $\\tanh$, along with an input time series $\\zeta[k]$ that follows the uniform random distribution $\\mathcal{U}([-1, 1])$, and sample its dynamics $x[k]$.\n",
    "To ensure asymmetry, $\\zeta[k]$ is scaled to take values in the range $[0, 1]$.\n",
    "(The input is made asymmetric because $\\tanh$ is an odd function, and for symmetric input, only the odd-order components of the capacity appear [verification is an advanced task])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5678\n",
    "dim = 10\n",
    "t_washout = 1000\n",
    "t_sample = 1000000\n",
    "t_total = t_washout + t_sample\n",
    "display = True\n",
    "\n",
    "rnd = np.random.default_rng(seed)\n",
    "w_in = Linear(1, dim, bound=0.1, bias=0.0, rnd=rnd)\n",
    "net = ESN(dim, sr=0.1, f=np.tanh, p=1, rnd=rnd)\n",
    "\n",
    "x0 = np.zeros((dim,))\n",
    "us = rnd.uniform(-1, 1, (t_total, 1))\n",
    "\n",
    "x = x0\n",
    "xs = np.zeros((t_total, *x0.shape))\n",
    "for idx in trange(t_total, display=display):\n",
    "    x = net(x, w_in(0.5 * us[idx] + 0.5))\n",
    "    # x = net(x, w_in(us[idx]))  # Uncomment it for the symmetric case.\n",
    "    xs[idx] = x\n",
    "\n",
    "print(\"us:\", us.shape)\n",
    "print(\"xs:\", xs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "前章で扱われた記憶関数の計算同様に、情報処理容量もSVDを用いて計算します。\n",
    "したがって同じコードを流用できます (確認していない人は前章に戻ってください) 。\n",
    "以下の`calc_regression_and_rank`と`calc_capacity`は前章の実装を流用したものです。\n",
    "\n",
    "[en]: #\n",
    "As with the memory function calculations covered in the previous chapter, IPC is also computed using SVD.\n",
    "Thus, the same code can be reused (if you haven't reviewed it, please refer back to the previous chapter).\n",
    "The following `calc_regression_and_rank` and `calc_capacity` functions are reused implementations from the previous chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_regression_and_rank(X):\n",
    "    T, N = X.shape[-2:]\n",
    "    X = X - X.mean(axis=-2, keepdims=True)\n",
    "    U, sigma, _V = np.linalg.svd(X, full_matrices=False)\n",
    "    eps = np.finfo(X.dtype).eps\n",
    "    sigma_sq_max = np.max(sigma * sigma, axis=-1, keepdims=True)\n",
    "    eps = sigma_sq_max * (eps * max(T, N))\n",
    "    mask = sigma > eps\n",
    "    rank = mask.sum(axis=-1)\n",
    "    return U, mask, rank\n",
    "\n",
    "\n",
    "def calc_capacity(U, mask, zeta):\n",
    "    uzeta = U.swapaxes(-2, -1) @ zeta\n",
    "    dot = ((uzeta * uzeta) * mask[..., None]).sum(axis=-2)\n",
    "    var = (zeta * zeta).sum(axis=-2)\n",
    "    r2 = dot / var\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "まずSVDを実行し階数 $r$ を確認しましょう。\n",
    "階数は $\\mathrm{C}^\\mathrm{tot}$ の上限となります。\n",
    "また直交多項式 $\\mathcal{P}$ も`Legendre`によって計算できるように準備しましょう。\n",
    "\n",
    "[en]: #\n",
    "First, let's perform SVD and check the rank $r$.\n",
    "The rank serves as the upper limit of $\\mathrm{C}^\\mathrm{tot}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, mask, rank = calc_regression_and_rank(xs[t_washout:])\n",
    "print(\"rank:\", rank)\n",
    "\n",
    "poly = Legendre(us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "#### 1次の容量の計算\n",
    "\n",
    "[en]: #\n",
    "#### Calculation of first-order capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "まずは1次の容量 $C^1$ を計算しましょう。\n",
    "特に $\\mathcal{P}_1(z) = z$ なので1次の目標時系列は以下のとおり計算できます。\n",
    "\n",
    "[en]: #\n",
    "First, let's calculate the first-order capacity $C^1$.\n",
    "Specifically, since $\\mathcal{P}_1(z) = z$, the first-order target time series can be calculated as follows:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\zeta^{0}, \\zeta^{1}, \\zeta^{2}, \\zeta^{3},~\\ldots\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay1_max = 10\n",
    "\n",
    "taus = np.arange(0, delay1_max + 1)\n",
    "c1 = np.zeros(len(taus))\n",
    "for idx, tau in enumerate(tqdm(taus)):\n",
    "    zeta = poly[1][t_washout - tau : t_total - tau]\n",
    "    c1[idx] = calc_capacity(U, mask, zeta)[..., 0]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.plot(taus, c1, label=r\"$\\zeta^1$\", color=\"C0\", marker=\"o\")\n",
    "ax.set_xlim(-0.5, delay1_max + 0.5)\n",
    "ax.set_xlabel(r\"$\\tau$\", fontsize=14)\n",
    "ax.set_ylabel(r\"$\\mathrm{C}[x,\\zeta^\\tau]$\", fontsize=14)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "ax.set_title(r\"$\\mathrm{C}^1$=\" + f\"{c1.sum():.3f}\", fontsize=14)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "#### 2次の容量の計算\n",
    "\n",
    "[en]: #\n",
    "#### Calculation of second-order capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "2次の場合は $D=\\{1,1\\}$ と $D=\\{2\\}$ の2パターンの組み合わせが考えられます。\n",
    "$\\tau_1 \\leq \\tau_2$となる $\\tau_1$ と $\\tau_2$ を用意し以下の形で2次の目標時系列 $z^{\\tau_1, \\tau_2} $ を網羅的に用意できます。\n",
    "\n",
    "[en]: #\n",
    "In the case of the second order, there are two possible combinations: $D=\\{1,1\\}$ and $D=\\{2\\}$.\n",
    "Using $\\tau_1$ and $\\tau_2$ such that $\\tau_1 \\leq \\tau_2$, we can comprehensively prepare the orthogonal polynomials $z^{\\tau_1, \\tau_2}$ in the following form:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "z^{\\tau_1, \\tau_2} = \\begin{cases}\n",
    "\\mathcal{P}_2(\\zeta^{\\tau_1}) &= \\frac{3}{2}(\\zeta^{\\tau_1})^2 - \\frac{1}{2} & \\mathrm{if}~\\tau_1 = \\tau_2 \\\\\n",
    "\\mathcal{P}_1(\\zeta^{\\tau_1})\\mathcal{P}_1(\\zeta^{\\tau_2}) &= \\zeta^{\\tau_1} \\zeta^{\\tau_2} & \\mathrm{if}~\\tau_1 \\leq \\tau_2 \\\\\n",
    "\\end{cases}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay2_max = 8\n",
    "\n",
    "taus = np.arange(0, delay2_max + 1)\n",
    "c2 = np.zeros((len(taus), len(taus)))\n",
    "\n",
    "cands = list(itertools.product(enumerate(taus), repeat=2))\n",
    "for (idx, tau1), (idy, tau2) in tqdm(cands):\n",
    "    if not (idx <= idy):\n",
    "        continue\n",
    "    if idx == idy:\n",
    "        zeta = poly[2][t_washout - tau1 : t_total - tau1]\n",
    "    else:\n",
    "        zeta1 = poly[1][t_washout - tau1 : t_total - tau1]\n",
    "        zeta2 = poly[1][t_washout - tau2 : t_total - tau2]\n",
    "        zeta = zeta1 * zeta2\n",
    "    c2[idx, idy] = calc_capacity(U, mask, zeta)[..., 0]\n",
    "\n",
    "fig = Figure(figsize=(8, 6))\n",
    "ax = fig[0]\n",
    "im, cb = ax.plot_matrix(\n",
    "    c2,\n",
    "    x=taus,\n",
    "    y=taus,\n",
    "    cmap=\"viridis\",\n",
    "    zscale=\"log\",\n",
    "    vmax=1,\n",
    "    vmin=1e-3,\n",
    "    aspect=\"equal\",\n",
    "    colorbar=True,\n",
    "    xticks_kws=dict(num_tick=len(taus)),\n",
    "    yticks_kws=dict(num_tick=len(taus)),\n",
    ")\n",
    "ax.grid(False)\n",
    "ax.set_xlim(-1, len(taus))\n",
    "ax.set_ylim(-1, len(taus))\n",
    "ax.set_xlabel(r\"$\\tau_2$\", fontsize=14)\n",
    "ax.set_ylabel(r\"$\\tau_1$\", fontsize=14)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "cb.ax.tick_params(labelsize=12)\n",
    "cb.set_label(r\"$\\mathrm{C}[x,z^{\\tau_1,\\tau_2}]$\", fontsize=14)\n",
    "ax.set_title(r\"$\\mathrm{C}^2$=\" + f\"{c2.sum():.3f}\", fontsize=14)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "#### 3次の容量の計算\n",
    "\n",
    "[en]: #\n",
    "#### Calculation of third-order capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "3次の場合は $D=\\{1,1,1\\}$ と $D=\\{2, 1\\}$ ならびに $D=\\{3\\}$ の3パターンの組み合わせが考えられます。\n",
    "2次の場合同様に$\\tau_1 \\leq \\tau_2 \\leq \\tau_3$となる $\\tau_1,\\tau_2,\\tau_3$ を用意し以下の形で3次の目標時系列 $z^{\\tau_1, \\tau_2,\\tau_3} $ を網羅的に用意できます。\n",
    "\n",
    "[en]: #\n",
    "For the third order, there are three possible combinations: $D=\\{1,1,1\\}$, $D=\\{2, 1\\}$, and $D=\\{3\\}$.\n",
    "Similar to the second-order case, we prepare $\\tau_1, \\tau_2, \\tau_3$ such that $\\tau_1 \\leq \\tau_2 \\leq \\tau_3$ and construct the third-order target time series $z^{\\tau_1, \\tau_2, \\tau_3}$ as follows:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "z^{\\tau_1, \\tau_2, \\tau_3} = \\begin{cases}\n",
    "\\mathcal{P}_3(\\zeta^{\\tau_1}) &= \\frac{5}{2}(\\zeta^{\\tau_1})^3 - \\frac{3}{2}\\zeta^{\\tau_1} & \\mathrm{if}~\\tau_1 = \\tau_2 = \\tau_3 \\\\\n",
    "\\mathcal{P}_2(\\zeta^{\\tau_1})\\mathcal{P}_1(\\zeta^{\\tau_3}) &= \\left(\\frac{3}{2}(\\zeta^{\\tau_1})^2 - \\frac{1}{2}\\right)\\zeta^{\\tau_3} & \\mathrm{if}~\\tau_1 = \\tau_2 < \\tau_3 \\\\\n",
    "\\mathcal{P}_1(\\zeta^{\\tau_1})\\mathcal{P}_2(\\zeta^{\\tau_2}) &= \\zeta^{\\tau_1}\\left(\\frac{3}{2}(\\zeta^{\\tau_2})^2 - \\frac{1}{2}\\right) & \\mathrm{if}~\\tau_1 < \\tau_2 = \\tau_3 \\\\\n",
    "\\mathcal{P}_1(\\zeta^{\\tau_1})\\mathcal{P}_1(\\zeta^{\\tau_2})\\mathcal{P}_1(\\zeta^{\\tau_3}) &= \\zeta^{\\tau_1}\\zeta^{\\tau_2}\\zeta^{\\tau_3} & \\mathrm{if}~\\tau_1 < \\tau_2 < \\tau_3 \\\\\n",
    "\\end{cases}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay3_max = 6\n",
    "\n",
    "taus = np.arange(0, delay3_max + 1)\n",
    "c3 = np.zeros((len(taus), len(taus), len(taus)))\n",
    "\n",
    "cands = list(itertools.product(enumerate(taus), repeat=3))\n",
    "for (idx, tau1), (idy, tau2), (idz, tau3) in tqdm(cands):\n",
    "    if not (idx <= idy <= idz):\n",
    "        continue\n",
    "    if idx == idy == idz:\n",
    "        zeta = poly[3][t_washout - tau1 : t_total - tau1]\n",
    "    elif idx == idy:\n",
    "        zeta1 = poly[2][t_washout - tau1 : t_total - tau1]\n",
    "        zeta2 = poly[1][t_washout - tau3 : t_total - tau3]\n",
    "        zeta = zeta1 * zeta2\n",
    "    elif idy == idz:\n",
    "        zeta1 = poly[1][t_washout - tau1 : t_total - tau1]\n",
    "        zeta2 = poly[2][t_washout - tau2 : t_total - tau2]\n",
    "        zeta = zeta1 * zeta2\n",
    "    else:\n",
    "        zeta1 = poly[1][t_washout - tau1 : t_total - tau1]\n",
    "        zeta2 = poly[1][t_washout - tau2 : t_total - tau2]\n",
    "        zeta3 = poly[1][t_washout - tau3 : t_total - tau3]\n",
    "        zeta = zeta1 * zeta2 * zeta3\n",
    "    c3[idx, idy, idz] = calc_capacity(U, mask, zeta)[..., 0]\n",
    "\n",
    "\n",
    "num_col = math.ceil(len(taus) / 2)\n",
    "grid_size = (2, num_col)\n",
    "fig = Figure(figsize=(grid_size[1] * 3, grid_size[0] * 3))\n",
    "fig.create_grid(*grid_size, hspace=0.35, wspace=0.3)\n",
    "\n",
    "for pos in range(len(taus)):\n",
    "    ax = fig[pos // num_col, pos % num_col]\n",
    "    res = ax.plot_matrix(\n",
    "        c3[pos],\n",
    "        x=taus,\n",
    "        y=taus,\n",
    "        cmap=\"viridis\",\n",
    "        zscale=\"log\",\n",
    "        vmax=1,\n",
    "        vmin=1e-3,\n",
    "        aspect=\"equal\",\n",
    "        colorbar=len(taus) == (pos + 1),\n",
    "        xticks_kws=dict(num_tick=len(taus)),\n",
    "        yticks_kws=dict(num_tick=len(taus)),\n",
    "    )\n",
    "    ax.grid(False)\n",
    "    ax.set_xlim(-1, len(taus))\n",
    "    ax.set_ylim(-1, len(taus))\n",
    "    if pos % num_col == 0:\n",
    "        ax.set_ylabel(r\"$\\tau_2$\", fontsize=14)\n",
    "    if (pos // num_col) == grid_size[0] - 1:\n",
    "        ax.set_xlabel(r\"$\\tau_3$\", fontsize=14)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax.set_title(r\"$\\tau_1$=\" + f\"{taus[pos]}\", fontsize=14)\n",
    "    if len(taus) == (pos + 1):\n",
    "        cb = res[1]\n",
    "        cb.ax.set_position([0.9, 0.1, 0.03, 0.8])\n",
    "        cb.ax.tick_params(labelsize=12)\n",
    "        cb.set_label(r\"$\\mathrm{C}[x,z^{\\tau_1,\\tau_2,\\tau_3}]$\", fontsize=14)\n",
    "if len(taus) < (grid_size[0] * grid_size[1]):\n",
    "    for pos in range(len(taus), grid_size[0] * grid_size[1]):\n",
    "        fig.delaxes(fig[pos // num_col, pos % num_col])\n",
    "fig.suptitle(r\"$\\mathrm{C}^3$=\" + f\"{c3.sum():.3f}\", fontsize=16)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "ここまで計算した $\\mathrm{C}^1, \\mathrm{C}^2, \\mathrm{C}^3$ を足し合わせて、全体の情報処理容量 $\\mathrm{C}^\\mathrm{tot}$ を計算します。\n",
    "これがほぼ階数 $r$ に等しくなる点を確認してください。\n",
    "\n",
    "[en]: #\n",
    "Finally, we sum up the calculated $\\mathrm{C}^1, \\mathrm{C}^2, \\mathrm{C}^3$ to compute the overall IPC $\\mathrm{C}^\\mathrm{tot}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_tot = np.sum(c1) + np.sum(c2) + np.sum(c3)\n",
    "print(\"total_capacity\", c_tot, \"rank\", rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.1. (Advanced)\n",
    "\n",
    "[ja]: #\n",
    "- 文献[1]を読み、$\\mathrm{C}^\\mathrm{tot} \\leq r$ の導出を確認せよ。\n",
    "- 入力を対称的にした場合、$\\mathrm{C}^d$ の偶数次成分 (特に $d=2$) の消失を確認せよ。またその理由も考察し説明せよ。\n",
    "- 活性化関数を偶関数に変更し、その挙動も同様に確認せよ。\n",
    "\n",
    "[en]: #\n",
    "- Read reference [1] and verify the derivation of $\\mathrm{C}^\\mathrm{tot} \\leq r$.\n",
    "- Verify that the even-order components of $\\mathrm{C}^d$ (particularly $d=2$) vanish when the input is made symmetric. Also, consider and explain the reason for this.\n",
    "- Change the activation function to an even function and similarly verify its behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.2. (Advanced)\n",
    "\n",
    "[ja]: #\n",
    "- ある整数$d$に対して$\\sum_{i} d_i = d$ となるような次数の集合$D=\\{d_i\\}_i$を効率的に出力するコードを実装せよ (Cf. [ヤング図形](https://ja.wikipedia.org/wiki/%E3%83%A4%E3%83%B3%E3%82%B0%E5%9B%B3%E5%BD%A2))。\n",
    "    - あるいは後述の`ipc-module`において実装されている[`make_degree_list`](https://github.com/rc-bootcamp/ipc-module/blob/main/src/ipc_module/helper.py#L60)を使用しても良い。\n",
    "    - `from ipc_module.helper import make_degree_list`によってインポートして使用できる。\n",
    "- ある次数の集合 $D$ と最大時間遅れ$\\tau_\\mathrm{max} \\geq 1$ が与えられた時、$\\tau_\\mathrm{max}$以下の範囲で可能な時間遅れの組み合わせ $\\Tau=\\{\\tau_i\\}_i$ をすべて列挙するコードを実装せよ。\n",
    "\n",
    "[en]: #\n",
    "- Implement code to efficiently output a set of degrees $D=\\{d_i\\}_i$ such that $\\sum_{i} d_i = d$ for a given integer $d$ (Cf. [Young tableau](https://en.wikipedia.org/wiki/Young_tableau)).\n",
    "    - Alternatively, you may use the [`make_degree_list`](https://github.com/rc-bootcamp/ipc-module/blob/main/src/ipc_module/helper.py#L60) implemented in the `ipc-module` mentioned later.\n",
    "    - It can be imported and used with `from ipc_module.helper import make_degree_list`.\n",
    "- Implement code to enumerate all possible combinations of time delays $\\Tau=\\{\\tau_i\\}_i$ within the range of $\\tau_\\mathrm{max} \\geq 1$, given a set of degrees $D$ and a maximum time delay $\\tau_\\mathrm{max}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "### 3. ライブラリを使用した高速な演算\n",
    "\n",
    "[en]: #\n",
    "### 3. Fast IPC computation using libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "#### 環境の設定\n",
    "\n",
    "[en]: #\n",
    "#### Environmental setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "ここまで確認したとおり、情報処理容量は直交多項式と時間遅れの組み合わせを網羅的に探索する必要があるため、次数が大きくなると計算量が爆発的に増加します。\n",
    "また次数が大きくなると次数の分割の仕方が指数的に増えるため、前節でのやり方のように逐一実装するのはとても大変です (分割の数の一般項は $p(n)\\sim\\frac{1}{4\\sqrt{3}n}e^{\\pi\\sqrt{\\frac{2n}{3}}}$ に漸近すると知られています<sup>[3]</sup>) 。\n",
    "そのためこの演習では研究室内で開発されたライブラリ [`ipc-module`](https://rc-bootcamp.github.io/ipc-module/) を使い、情報処理能力を計算する方法を学びましょう。\n",
    "\n",
    "`ipc-module`は`numpy`の他`pytorch`や`cupy`といったGPUを用いたテンソル演算のライブラリをサポートしているため、CPUのみでの演算と比べてより効率的に計算できます。\n",
    "また整理や描画のための関数が用意されており、簡単に所望の力学系に対してその情報処理の内実を確認できます。\n",
    "[PyPIにおいて公開](https://pypi.org/project/ipc-module/)されており、`pip install ipc-module`でインストールできますが、このノートブックでは手元で確認し変更を加えやすいようにソースコードを直接取り込んでいます。\n",
    "具体的な実装は`./ipc_module`フォルダに入っているPythonコードを参照してください。\n",
    "\n",
    "なお以下のコードはそのままCPU上でも動きますがかなり時間がかかります。\n",
    "したがってGPUを使える環境にある場合は**GPUの使用を強く推奨します** (手元にない場合はGoogle Colaboratory上での実行をおすすめします)。\n",
    "その際は以下のガイドを参考に追加の設定を行ってください。\n",
    "\n",
    "[en]: #\n",
    "As confirmed so far, calculating the IPC requires an exhaustive search over combinations of orthogonal polynomials and time delays, leading to an explosive increase in computational cost as the degree grows.\n",
    "Additionally, as the degree increases, the number of ways to partition the degree grows exponentially, making it very challenging to implement each case individually as done in the previous section (it is known that the general term for the number of partitions asymptotically approaches $p(n)\\sim\\frac{1}{4\\sqrt{3}n}e^{\\pi\\sqrt{\\frac{2n}{3}}}$<sup>[3]</sup>).\n",
    "Therefore, in this exercise, we will learn how to calculate IPC using the library [`ipc-module`](https://rc-bootcamp.github.io/ipc-module/), which was developed within the laboratory.\n",
    "\n",
    "`ipc-module` supports GPU-accelerated tensor computation libraries such as `pytorch` and `cupy` in addition to `numpy`, enabling more efficient calculations compared to CPU-only operations.\n",
    "It also provides functions for organizing and visualizing results, allowing you to easily examine the IPC of a given dynamical system.\n",
    "It is [published on PyPI](https://pypi.org/project/ipc-module/) and can be installed via `pip install ipc-module`, but in this notebook, we directly include the source code to make it easier to check and modify locally.\n",
    "For specific implementations, refer to the Python code in the `./ipc_module` folder.\n",
    "\n",
    "Note that the following code will run on a CPU as is, but it will take considerable time.\n",
    "Therefore, if you have access to a GPU environment, **using a GPU is strongly recommended** (if you don't have one locally, we recommend running it on Google Colaboratory).\n",
    "In that case, follow the guide below for additional setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "<details><summary>GPU環境で計算する場合の下準備</summary>\n",
    "\n",
    "`pytorch` がとても便利ですので、以下その導入方法を説明します。\n",
    "\n",
    "1. オンライン環境 (Google Colaboratory) の場合\n",
    "\n",
    "    無料版でもデフォルトでGPUを使用できる他、`pytorch` がすでにインストールされているので特に追加の設定をする必要はないですが、以下の手順でGPUが有効か確認できます。\n",
    "    「編集」 > 「ノートブックの設定」 > 「ハードウェア アクセラレータ」 > 「GPU」\n",
    "\n",
    "2. ローカル環境の場合 (uvの場合)\n",
    "\n",
    "    NVIDIA製のGPUの場合ドライバーをまずインストールしてください。\n",
    "    インストールされているかどうかは、`nvidia-smi` コマンドで確認できます。\n",
    "    インストールされていない場合は、公式の[配布ページ](https://www.nvidia.com/en-us/drivers/)からダウンロードできます。\n",
    "    あとは以下のコマンドでインストールできます。\n",
    "    ```bash\n",
    "    uv sync --extra gpu\n",
    "    ```\n",
    "    自動的に`pytorch` のインストールが開始されます。\n",
    "\n",
    "</details>\n",
    "\n",
    "[en]: #\n",
    "<details><summary>Preparations for calculations in a GPU environment</summary>\n",
    "\n",
    "`pytorch` is very convenient, so the following explains how to set it up.\n",
    "\n",
    "1. In an online environment (Google Colaboratory):\n",
    "\n",
    "   Even with the free version, GPU can be used by default, and `pytorch` is already installed, so no additional setup is required. However, you can verify that the GPU is enabled using the following steps:\n",
    "   \"Edit\" > \"Notebook settings\" > \"Hardware accelerator\" > \"GPU\"\n",
    "\n",
    "2. In a local environment (for uv):\n",
    "\n",
    "   For NVIDIA GPUs, first install the drivers.\n",
    "   You can check if they are installed using the `nvidia-smi` command.\n",
    "   If not installed, you can download them from the official [distribution page](https://www.nvidia.com/en-us/drivers/).\n",
    "   Then, you can install them using the following command:\n",
    "   ```bash\n",
    "   uv sync --extra gpu\n",
    "   ```\n",
    "   This will automatically start the installation of `pytorch`.\n",
    "    ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "#### 基本的な動作の説明\n",
    "\n",
    "[en]: #\n",
    "#### Explanation of basic operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "準備ができたら実際に`ipc_module`を使って計算してみましょう。\n",
    "まずは$N=50$ 次元のESNを用意し、スペクトル半径を0.1から1.7の範囲で0.1刻みでふり、同時にそのダイナミクスをサンプルしましょう。\n",
    "まずは先程同様に、一様乱数 $\\mathcal{U}([-1, 1])$ から入力時系列 $\\zeta$ を用意し、$[0, 1]$の範囲にスケーリングして非対称にしたものをESNに入力します (メモリ要求量が大きいので、メモリ不足のエラーが出たら適宜`t_sample`や`dim`を小さくしてください。ただし一般にサンプルの長さが大きいほど計算の精度が上がります)。\n",
    "\n",
    "[en]: #\n",
    "Once ready, let's perform calculations using `ipc_module`.\n",
    "First, prepare an ESN with $N=50$ dimensions, vary the spectral radius from 0.1 to 1.7 in increments of 0.1, and simultaneously sample its dynamics.\n",
    "As before, prepare an input time series $\\zeta$ from a uniform random distribution $\\mathcal{U}([-1, 1])$, scale it to the range $[0, 1]$ to make it asymmetric, and input it into the ESN.\n",
    "(The memory requirement is large, so if you encounter a memory shortage error, reduce `t_sample` or `dim` as needed.\n",
    "In general, longer sample lengths lead to higher calculation accuracy.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5678\n",
    "dim = 50\n",
    "t_washout = 10000\n",
    "t_sample = 100000\n",
    "srs = np.linspace(0.1, 1.7, 17)\n",
    "t_total = t_washout + t_sample\n",
    "display = True\n",
    "\n",
    "rnd = np.random.default_rng(seed)\n",
    "w_in = Linear(1, dim, bound=0.1, bias=0.0, rnd=rnd)\n",
    "net = ESN(dim, sr=srs[:, None], f=np.tanh, p=1, rnd=rnd)\n",
    "\n",
    "x0 = np.zeros((srs.shape[0], dim))\n",
    "us = rnd.uniform(-1, 1, (t_total, 1))\n",
    "\n",
    "x = x0\n",
    "xs = np.zeros((t_total, *x0.shape))\n",
    "for idx in trange(t_total, display=display):\n",
    "    x = net(x, w_in(0.5 * us[idx] + 0.5))\n",
    "    xs[idx] = x\n",
    "\n",
    "print(\"us:\", us.shape)\n",
    "print(\"xs:\", xs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "[`UnivariateProfiler`](https://rc-bootcamp.github.io/ipc-module/profiler/#ipc_module.profiler.UnivariateProfiler)は情報処理容量を計算する様々なメソッドを備えたクラスです。\n",
    "1次元の入力時系列と対応する状態時系列を渡し、その後[`UnivariateProfiler.calc`](https://rc-bootcamp.github.io/ipc-module/profiler/#ipc_module.profiler.UnivariateProfiler.calc)に指定された次数と時間遅れの範囲で情報処理容量を計算します。\n",
    "\n",
    "<details><summary> 引数の詳細</summary>\n",
    "\n",
    "- `us`: `np.ndarray | torch.Tensor | cupy.ndarray`\n",
    "    - 入力時系列\n",
    "    - 形は `(t, ..., 1)` である必要がある\n",
    "- `xs`: `np.ndarray | torch.Tensor | cupy.ndarray`\n",
    "    - 対応する状態時系列\n",
    "    - 形は `(t, ..., N)` である必要がある\n",
    "- `poly_name`: `str`\n",
    "    - 使用する多項式の名前\n",
    "        - [`Legendre`](https://rc-bootcamp.github.io/ipc-module/polynomial/#ipc_module.polynomial.Legendre): ルジャンドル多項式\n",
    "        - [`Hermite`](https://rc-bootcamp.github.io/ipc-module/polynomial/#ipc_module.polynomial.Hermite): エルミート多項式\n",
    "        - [`GramSchmidt`](https://rc-bootcamp.github.io/ipc-module/polynomial/#ipc_module.polynomial.GramSchmidt): グラム・シュミット法による多項式展開\n",
    "- `offset`: `int`\n",
    "    - 時間遅れのオフセット ($t=0$ となるインデックスの指定)\n",
    "    - デフォルトは0\n",
    "- `surrogate_num`: `int`\n",
    "    - サロゲートサンプルの数\n",
    "    - デフォルトは1000\n",
    "- `surrogate_seed`: `int`\n",
    "    - サロゲートサンプルのシード\n",
    "    - デフォルトは0\n",
    "- `axis1`: `int`\n",
    "    - 時間軸に対応するaxis\n",
    "    - `us` と `xs` で同じものが使用される\n",
    "    - デフォルトは0\n",
    "- `axis2`: `int`\n",
    "    - 状態に対応するaxis\n",
    "    - `us` と `xs` で同じものが使用される\n",
    "    - デフォルトは-1\n",
    "</details>\n",
    "\n",
    "実際に使ってみましょう。\n",
    "まず `UnivariateProfiler`クラスのインスタンス`profiler`を作成します。\n",
    "GPUが使えない環境の場合は`use_gpu`を`False`にしてください。\n",
    "\n",
    "[en]: #\n",
    "The [`UnivariateProfiler`](https://rc-bootcamp.github.io/ipc-module/profiler/#ipc_module.profiler.UnivariateProfiler) is a class equipped with various methods for calculating IPC.\n",
    "You pass a one-dimensional input time series and the corresponding state time series, and then calculate the IPC within the specified range of degrees and time delays using [`UnivariateProfiler.calc`](https://rc-bootcamp.github.io/ipc-module/profiler/#ipc_module.profiler.UnivariateProfiler.calc).\n",
    "\n",
    "<details><summary> Details of the arguments</summary>\n",
    "\n",
    "- `us`: `np.ndarray | torch.Tensor | cupy.ndarray`\n",
    "    - Input time series\n",
    "    - Must have the shape `(t, ..., 1)`\n",
    "- `xs`: `np.ndarray | torch.Tensor | cupy.ndarray`\n",
    "    - Corresponding state time series\n",
    "    - Must have the shape `(t, ..., N)`\n",
    "- `poly_name`: `str`\n",
    "    - Name of the polynomial to use\n",
    "        - [`Legendre`](https://rc-bootcamp.github.io/ipc-module/polynomial/#ipc_module.polynomial.Legendre): Legendre polynomial\n",
    "        - [`Hermite`](https://rc-bootcamp.github.io/ipc-module/polynomial/#ipc_module.polynomial.Hermite): Hermite polynomial\n",
    "        - [`GramSchmidt`](https://rc-bootcamp.github.io/ipc-module/polynomial/#ipc_module.polynomial.GramSchmidt): Polynomial expansion using the Gram-Schmidt method\n",
    "- `offset`: `int`\n",
    "    - Time delay offset (specifies the index where $t=0$)\n",
    "    - Default is 0\n",
    "- `surrogate_num`: `int`\n",
    "    - Number of surrogate samples\n",
    "    - Default is 1000\n",
    "- `surrogate_seed`: `int`\n",
    "    - Seed for surrogate samples\n",
    "    - Default is 0\n",
    "- `axis1`: `int`\n",
    "    - Axis corresponding to the time dimension\n",
    "    - The same axis is used for both `us` and `xs`\n",
    "    - Default is 0\n",
    "- `axis2`: `int`\n",
    "    - Axis corresponding to the state dimension\n",
    "    - The same axis is used for both `us` and `xs`\n",
    "    - Default is -1\n",
    "</details>\n",
    "\n",
    "Let’s try using it.\n",
    "First, create an instance of the `UnivariateProfiler` class, named `profiler`.\n",
    "If you are in an environment where a GPU cannot be used, set `use_gpu` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True  # NOTE: Set it to False to run on CPU.\n",
    "\n",
    "if use_gpu:\n",
    "    import torch\n",
    "\n",
    "    assert torch.cuda.is_available(), \"CUDA is not available\"\n",
    "    us_c = torch.from_numpy(us).cuda()\n",
    "    xs_c = torch.from_numpy(xs).cuda()\n",
    "    args = (us_c, xs_c)\n",
    "else:\n",
    "    args = (us, xs)\n",
    "\n",
    "profiler = UnivariateProfiler(\n",
    "    *args,\n",
    "    \"Legendre\",\n",
    "    offset=t_washout,\n",
    "    surrogate_num=1000,\n",
    "    axis1=0,\n",
    "    axis2=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "以下簡単なアルゴリズムの説明をします。\n",
    "1. `UnivariateProfiler`クラスのインスタンス作成時に、第2引数に与えられた状態時系列 (今回の場合 `xs`) を正規化した後、SVDを実行し同時に階数を計測します (実装は`calc_regression_and_rank`とほぼ同じ)。\n",
    "2. その直後に、サロゲートデータ (後述) に用いるシャッフルされたインデックスを生成します (`surrogate_num` で指定)。\n",
    "3. `UnivariateProfiler.calc`メソッドを実行し、SVDの計算結果を基に指定された次数と時間遅れの範囲で情報処理容量を計算します。目標時系列 (の構成に必要な直交多項式) の計算は遅延評価、すなわち必要になった際に計算されかつ結果がキャッシュされます。\n",
    "\n",
    "1と2はすでに前のセルで完了したので以下のセルで `UnivariateProfiler.calc`メソッドを実行して、まず1次の容量 $C^1$ を計算してみましょう。\n",
    "\n",
    "[en]: #\n",
    "Here is a simple explanation of the algorithm:\n",
    "1. When creating an instance of the `UnivariateProfiler` class, the state time series provided as the second argument (in this case, `xs`) is normalized, followed by performing SVD and simultaneously measuring the rank (the implementation is almost the same as `calc_regression_and_rank`).\n",
    "2. Immediately afterward, shuffled indices to be used for surrogate data (described later) are generated (as specified by `surrogate_num`).\n",
    "3. The `UnivariateProfiler.calc` method is executed to calculate the IPC within the specified range of degrees and time delays based on the SVD results. The calculation of the target time series (and the orthogonal polynomials required for its construction) is done lazily, meaning it is computed only when needed, and the results are cached.\n",
    "\n",
    "Since steps 1 and 2 have already been completed in the previous cell, let’s execute the `UnivariateProfiler.calc` method in the following cell to calculate the first-order capacity $C^1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler.calc(1, 1001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "このセルでは $\\Tau \\in \\{\\{0\\}, \\{1\\}, \\{2\\},~\\ldots,~\\{1000\\}\\}$ すなわち 目標時系列 $z \\in \\{\\zeta^0, \\zeta^1, \\zeta^2,~\\ldots,~\\zeta^{1000} \\}$ に対してそれぞれ $\\mathrm{C}[x,z]$ を計算しています (`zero_offset=False`は 1始まりを指定するオプション)。\n",
    "計算結果は `profiler[key]` の形で、次数の $D$ を指定して取得できます (`key`の中身が`tuple`である点に注意)。\n",
    "\n",
    "[en]: #\n",
    "In this cell, $\\Tau \\in \\{\\{0\\}, \\{1\\}, \\{2\\},~\\ldots,~\\{1000\\}\\}$, that is, for each target time series $z \\in \\{\\zeta^0, \\zeta^1, \\zeta^2,~\\ldots,~\\zeta^{1000}\\}$, $\\mathrm{C}[x,z]$ is being calculated (`zero_offset=False` is an option specifying 1-based indexing).\n",
    "The calculation results can be retrieved in the form of `profiler[key]` by specifying the degree $D$ (note that the content of `key` is a `tuple`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays, ipc, surr = profiler[(1,)]\n",
    "\n",
    "print(\"delays:\", *delays[:3], \"...\", *delays[-3:])\n",
    "print(\"ipc:\", ipc.shape)\n",
    "print(\"surr:\", surr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "このように`profiler`各 `key` に対して3つの情報を保持しています。\n",
    "一番目の`delays` は $\\Tau$ のリストです。\n",
    "`ipc`は計算された $\\mathrm{C}[x,z]$ が `np.array` の形で格納されておりSRが2軸目に対応します。\n",
    "`surr`は同時に求められたサロゲートデータに対する $\\mathrm{C}[x,z]$です。\n",
    "まず`ipc`の中身を確認してみましょう。\n",
    "\n",
    "[en]: #\n",
    "In this way, `profiler` holds three pieces of information for each `key`.\n",
    "The first, `delays`, is a list of $\\Tau$.\n",
    "`ipc` stores the calculated $\\mathrm{C}[x,z]$ in the form of a `np.array`, where the SR corresponds to the second axis.\n",
    "`surr` represents $\\mathrm{C}[x,z]$ for the surrogate data calculated simultaneously.\n",
    "First, let's check the contents of `ipc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = 301\n",
    "\n",
    "fig = Figure(figsize=(8, 6))\n",
    "ax = fig[0]\n",
    "im, cb = ax.plot_matrix(\n",
    "    ipc[:time_step, :, 0],\n",
    "    y=np.array(delays)[:time_step, 0],\n",
    "    x=srs,\n",
    "    aspect=\"auto\",\n",
    "    cmap=\"jet\",\n",
    "    vmin=1e-4,\n",
    "    vmax=1,\n",
    "    zscale=\"log\",\n",
    "    yticks_kws=dict(num_tick=4),\n",
    "    xticks_kws=dict(num_tick=3),\n",
    ")\n",
    "ax.set_xlabel(\"SR\", fontsize=14)\n",
    "ax.set_ylabel(r\"$\\tau$\", fontsize=14)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "cb.ax.tick_params(labelsize=12)\n",
    "cb.set_label(r\"$\\mathrm{C}[x,\\zeta^\\tau]$\", fontsize=14)\n",
    "ax.set_title(r\"$D=\\{1\\}$\", fontsize=14)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "これは前章で学習した記憶関数に他なりません。\n",
    "このグラフは色のレンジを最小値 $10^{-4}$ の対数スケールで表示しています。\n",
    "$\\tau$ が十分に大きいときも値が完全に0にはならず、わずかに小さい値を有し続ける様子がわかります。\n",
    "これは **疑似相関** と呼ばれる現象で、有限のデータしか扱えない数値計算の制約からしばしば生じるものです。\n",
    "疑似相関と思われる領域での容量$\\mathrm{C}$の値は非常に小さいですが、情報処理容量の計算では膨大な種類の $\\mathrm{C}$ を足し合わせる必要があるため、無視できない影響を与える場合があります (例えば $\\mathrm{C}^\\mathrm{tot}>r$ となってしまう)。\n",
    "\n",
    "そこで「有意」な成分と、「有意でない」疑似相関を区別するのに使用されるのが**サロゲートデータ**です。\n",
    "サロゲートデータは元の時系列データをシャッフルして生成されるデータで、元のデータの統計的性質が保持されつつも時間的な依存関係が消失しています。\n",
    "今回は`surrogate_num` で指定された数だけサロゲートデータを用意し同様に容量 $\\mathrm{C}$ を計測し、その最大値をしきい値として使用します。\n",
    "こうして得られたしきい値を超える成分だけをランダムから区別されるものとして加味します。\n",
    "この手法はRandom-shuffle法<sup>[4]</sup>として呼ばれる検定法で、もともとは文献[5]で導入されました。\n",
    "1000データの場合はおおよそ有意水準 $1/1000=0.001$ の検定とみなせます。\n",
    "試しにSR=1.0の成分に関してサロゲートデータを見てみましょう。\n",
    "\n",
    "[en]: #\n",
    "This is none other than the memory function learned in the previous chapter.\n",
    "This graph displays the color range on a logarithmic scale with a minimum value of $10^{-4}$.\n",
    "Even when $\\tau$ is sufficiently large, the values do not become completely zero but remain slightly small.\n",
    "This phenomenon is called **spurious correlation**, which often arises due to the constraints of numerical computations that can only handle finite data.\n",
    "The values of capacity $\\mathrm{C}$ in regions suspected to be spurious correlation are very small, but they can have a non-negligible impact in calculating IPC, since a large number of $\\mathrm{C}$ types must be summed (e.g., $\\mathrm{C}^\\mathrm{tot}>r$ may occur).\n",
    "\n",
    "**Surrogate data** is used to distinguish between \"significant\" components and \"insignificant\" spurious correlations.\n",
    "Surrogate data is generated by shuffling the original time series, preserving the statistical properties while eliminating temporal dependencies.\n",
    "Here, surrogate data is prepared in the number specified by `surrogate_num`, and the capacity $\\mathrm{C}$ is measured in the same way, with the maximum value used as the threshold.\n",
    "Only components exceeding this threshold are considered distinguishable from random noise.\n",
    "This method is known as the random-shuffle method<sup>[4]</sup>, originally introduced in reference [5].\n",
    "For 1000 data points, this can be regarded as a test with a significance level of $1/1000=0.001$.\n",
    "Let's examine the surrogate data for the component with SR=1.0 as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_id = 9  # 9 is the index of the SR = 1.0 in `srs`.\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "for value in surr[:, sr_id, :]:\n",
    "    ax.line_y(value, color=\"#333333\", alpha=0.5, lw=0.1)\n",
    "ax.line_y(surr[:, sr_id, :].max(), color=\"red\", lw=1)\n",
    "ax.plot(np.arange(0, 1001), ipc[:, 9, 0], lw=1)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_ylim([None, 1e-2])  # Comment it out to zoom out.\n",
    "ax.set_xlim([0, 1000])\n",
    "ax.set_ylabel(r\"$\\mathrm{C}[x,\\zeta^\\tau]$\", fontsize=14)\n",
    "ax.set_xlabel(r\"$\\tau$\", fontsize=14)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "ax.set_title(f\"SR={srs[sr_id]:.2f}\", fontsize=14)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "灰色の線は各サロゲートデータに対する容量 $\\mathrm{C}$ を、赤色の線はそのうちの最大値を示しています。\n",
    "`ipc_module`ではサロゲートデータの最大値を基準にその定数倍をしきい値として設定し有意な成分を抽出します (スケールできるようにしているのはあまりに目標時系列の数が多いため、より厳しい基準がしばしば必要だからです)。\n",
    "サロゲートデータを用いてしきい値を設定し、先程のグラフをもういちど描画してみましょう。\n",
    "しきい値以下の成分が白抜きになっているはずです。\n",
    "同時に定数倍 `max_scale` を変化させて、しきい値の大きさでどのように変化するか確認しましょう。\n",
    "\n",
    "[en]: #\n",
    "The gray lines represent the capacity $\\mathrm{C}$ for each surrogate data, and the red line shows the maximum value among them.\n",
    "In `ipc_module`, the maximum value of the surrogate data is used as a reference, and a constant multiple of it is set as the threshold to extract significant components (the scaling is often needed because stricter criteria are required when the number of target time series is large).\n",
    "Let's set the threshold using the surrogate data and redraw the previous graph.\n",
    "The components below the threshold should appear as hollow.\n",
    "At the same time, vary the constant multiplier `max_scale` to see how the graph changes with the threshold size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = 301\n",
    "max_scale = 1.0\n",
    "\n",
    "ipc_trunc = ipc * (ipc > surr.max(axis=0, keepdims=True) * max_scale)\n",
    "fig = Figure(figsize=(8, 6))\n",
    "ax = fig[0]\n",
    "im, cb = ax.plot_matrix(\n",
    "    ipc_trunc[:time_step, :, 0],\n",
    "    y=np.array(delays)[:time_step, 0],\n",
    "    x=srs,\n",
    "    aspect=\"auto\",\n",
    "    cmap=\"jet\",\n",
    "    vmin=1e-4,\n",
    "    vmax=1,\n",
    "    zscale=\"log\",\n",
    "    yticks_kws=dict(num_tick=4),\n",
    "    xticks_kws=dict(num_tick=3),\n",
    ")\n",
    "ax.set_xlabel(\"SR\", fontsize=14)\n",
    "ax.set_ylabel(r\"$\\tau$\", fontsize=14)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "cb.ax.tick_params(labelsize=12)\n",
    "cb.set_label(r\"$\\mathrm{C}[x,\\zeta^\\tau]$\", fontsize=14)\n",
    "ax.set_title(r\"$D=\\{1\\}$\" + f\", scale={max_scale:.2f}\", fontsize=14)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "さてここまで説明のため $d=1$ について計算し詳細を確認しましたが、他の次数についても同様に計算してみましょう。\n",
    "次のセルは $d=2,3,4,5$ に対する容量を計算します。\n",
    "それぞれ $\\tau_\\mathrm{max}=300,50,30,15$ を指定しています。\n",
    "例えば$D=\\{1,1\\}$の時は、$\\Tau$の候補となる$\\{\\tau_1, \\tau_2\\}$ の組み合わせは $\\{0,1\\}, \\{0,2\\},~\\ldots,~\\{0,300\\}, \\{1,2\\}, \\{1,3\\}, \\{1,4\\},~\\ldots,~\\{1,300\\},~\\ldots,~\\{299,300\\}$ のように$\\tau_\\mathrm{max}=300$ 以下の時間遅れの全ての組み合わせを網羅的に計算します。\n",
    "組み合わせの数が大きく、少々計算に時間がかかるのでそのまま待ってください。\n",
    "計算が完了すると計算された $D$ のリストが表示されます (`profiler.keys()` で確認できます)。\n",
    "\n",
    "[en]: #\n",
    "Now that we have calculated and examined the details for $d=1$ as an example, let’s perform similar calculations for other degrees.\n",
    "The next cell calculates the capacity for $d=2,3,4,5$.\n",
    "For each, $\\tau_\\mathrm{max}=300,50,30,15$ is specified.\n",
    "For example, when $D=\\{1,1\\}$, the combinations of $\\{\\tau_1, \\tau_2\\}$ that are candidates for $\\Tau$ are $\\{0,1\\}, \\{0,2\\},~\\ldots,~\\{0,300\\}, \\{1,2\\}, \\{1,3\\}, \\{1,4\\},~\\ldots,~\\{1,300\\},~\\ldots,~\\{299,300\\}$.\n",
    "In this way, all combinations of time delays below $\\tau_\\mathrm{max}=300$ are exhaustively calculated.\n",
    "Since the number of combinations is large, this may take some time, so please wait patiently.\n",
    "Once the calculation is complete, the list of calculated $D$ will be displayed (you can check it with `profiler.keys()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [2, 3, 4, 5]\n",
    "taus = [300, 50, 30, 15]\n",
    "for deg, tau in zip(degrees, taus, strict=True):\n",
    "    profiler.calc(deg, tau + 1)\n",
    "\n",
    "print(profiler.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "上のセルの実行が完了したら計算結果を一度保存しておきましょう。\n",
    "一般に情報処理容量の計算はとても時間がかかるので、計算結果が失われないように適宜保存するのが得策です。\n",
    "[`UnivariateProfiler.save`](https://rc-bootcamp.github.io/ipc-module/profiler/#ipc_module.profiler.UnivariateViewer.save)メソッドを使うと、計算結果を`npz`形式、もしくは`pkl`形式でファイルに保存できます (圧縮性と確認の容易さの観点の観点から`npz`を推奨します)。\n",
    "形式は指定されたファイルの拡張子によって決まります。\n",
    "`**kwargs`に別途保存しておきたい情報を入れて保存できます。\n",
    "ここではスペクトル半径 `srs` も保存しておきましょう。\n",
    "\n",
    "[en]: #\n",
    "Once the above cell completes, let's save the calculation results.\n",
    "In general, calculating IPC takes considerable time, so it is advisable to save results periodically to avoid losing them.\n",
    "Using the [`UnivariateProfiler.save`](https://rc-bootcamp.github.io/ipc-module/profiler/#ipc_module.profiler.UnivariateViewer.save) method, results can be saved to a file in either `npz` or `pkl` format (`npz` is recommended for better compression and easier verification).\n",
    "The format is determined by the file extension.\n",
    "You can include additional information to save using `**kwargs`.\n",
    "Here, let's also save the spectral radii `srs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler.save(\"./result/ipc_asym.npz\", srs=srs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "データの回収には[`UnivariateViewer`](https://rc-bootcamp.github.io/ipc-module/profiler/#ipc_module.profiler.UnivariateViewer)クラスを使用します。\n",
    "`UnivariateViwer`は`UnivariateProfiler`の親クラスで、`UnivariateProfiler`と同じように結果を回収できますが、もとの時系列やSVDの結果は保持していないため、追加で計算できない点に注意してください (`calc`関数を呼び出せない)。\n",
    "下のセルでは、`UnivariateViewer`のインスタンス`viewer`によって、`profiler`を用いた時と同様の結果が得られる点を確認してください。\n",
    "\n",
    "[en]: #\n",
    "The [`UnivariateViewer`](https://rc-bootcamp.github.io/ipc-module/profiler/#ipc_module.profiler.UnivariateViewer) class is used to retrieve data.\n",
    "`UnivariateViewer` is the parent class of `UnivariateProfiler` and can retrieve results in the same way as `UnivariateProfiler`.\n",
    "However, note that it does not retain the original time series or the results of the SVD, so additional calculations cannot be performed (the `calc` function cannot be called).\n",
    "In the cell below, confirm that the same results can be obtained using the `UnivariateViewer` instance `viewer` as when using `profiler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = UnivariateViewer(\"./result/ipc_asym.npz\")\n",
    "srs = viewer.info[\"srs\"]  # NOTE: Keyword options are stored on `info`.\n",
    "print(viewer.keys())\n",
    "delays, ipc, surr = viewer[(1,)]\n",
    "\n",
    "print(\"delays:\", *delays[:3], \"...\", *delays[-3:])\n",
    "print(\"ipc:\", ipc.shape)\n",
    "print(\"surr:\", surr.shape)\n",
    "print(\"srs:\", srs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "#### データの可視化と解析\n",
    "\n",
    "[en]: #\n",
    "#### Data visualization and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "$d=1$のときと異なり高次の場合は可視化は容易ではありません。\n",
    "$d=2$の際は前節で扱ったように2つの時間遅れ$(\\tau_1, \\tau_2)$平面上のカラーマップとして $C$ を描画できましたが、高次だとより多くのグラフが必要になり大変です。\n",
    "そこで `ipc_module`ではいくつかの可視化のための関数が用意されています。\n",
    "次のセルで使用される [`visualize_dataframe`](https://rc-bootcamp.github.io/ipc-module/helper/#ipc_module.helper.visualize_dataframe)は総容量を棒グラフとして描画する関数です。\n",
    "\n",
    "<details><summary> 引数の詳細</summary>\n",
    "\n",
    "- `ax`: `Axes`\n",
    "    - 描画先のAxes\n",
    "- `df`: `polars.DataFrame`\n",
    "    - 描画するデータフレーム\n",
    "- `ranks`: `Any | None`\n",
    "    - 階数のリスト\n",
    "    - 与えられた場合はその値で切られる\n",
    "- `xticks`: `Any | None`\n",
    "    - x軸の値\n",
    "- `group_by`: `str`\n",
    "    - 描画する方法\n",
    "        - `degree`: 次数 $d$ でグループ化\n",
    "        - `component`: 次数の集合 $D$ でグループ化\n",
    "        - `detail`: 次数の集合 $D$ と時間遅れの集合 $\\Tau$ でグループ化 (`threhold`を指定しないと描画に時間がかかるので注意！)\n",
    "- `threshold`: `float`\n",
    "    - `rest` としてまとめられる成分のしきい値\n",
    "- `sort_by`: `Any`\n",
    "    - ソートする方法\n",
    "        - `np.nanmax`: 最大値でソート\n",
    "        - `np.nanmean`: 平均値でソート\n",
    "        - `np.nansum`: 合計値でソート\n",
    "- `cmap`: `str`\n",
    "    - カラーマップ\n",
    "</details>\n",
    "\n",
    "[en]: #\n",
    "\n",
    "Unlike the case of $d=1$, visualization for higher orders is not straightforward.\n",
    "For $d=2$, as handled in the previous section, $C$ could be visualized as a color map on the $(\\tau_1, \\tau_2)$ plane.\n",
    "However, for higher orders, more graphs are required, making it challenging.\n",
    "To address this, `ipc_module` provides several functions for visualization.\n",
    "The [`visualize_dataframe`](https://rc-bootcamp.github.io/ipc-module/helper/#ipc_module.helper.visualize_dataframe) function, used in the next cell, is a function that visualizes the total capacity as a bar graph (using degree/component as groups).\n",
    "\n",
    "<details><summary> Details of the arguments</summary>\n",
    "\n",
    "- `ax`: `Axes`\n",
    "    - The Axes to draw on\n",
    "- `df`: `polars.DataFrame`\n",
    "    - The DataFrame to visualize\n",
    "- `ranks`: `Any | None`\n",
    "    - A list of ranks\n",
    "    - If provided, the values are filtered accordingly\n",
    "- `xticks`: `Any | None`\n",
    "    - Values for the x-axis\n",
    "- `group_by`: `str`\n",
    "    - The grouping method for visualization\n",
    "        - `degree`: Grouped by degree $d$\n",
    "        - `component`: Grouped by the set of degrees $D$\n",
    "        - `detail`: Grouped by the set of degrees $D$ and the set of time delays $\\Tau$ (Note: visualization may take time if `threshold` is not specified!)\n",
    "- `threshold`: `float`\n",
    "    - Threshold for components summarized as `rest`\n",
    "- `sort_by`: `Any`\n",
    "    - Sorting method\n",
    "        - `np.nanmax`: Sort by maximum value\n",
    "        - `np.nanmean`: Sort by mean value\n",
    "        - `np.nansum`: Sort by total value\n",
    "- `cmap`: `str`\n",
    "    - Color map\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, rank = viewer.to_dataframe(max_scale=2.0)  # NOTE: Threshold is scaled by max_scale.\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), gridspec_kw=dict(hspace=0.5))\n",
    "\n",
    "for idx, group_by in enumerate([\"degree\", \"component\"]):\n",
    "    ax = axes[idx]\n",
    "    visualize_dataframe(\n",
    "        ax,\n",
    "        df,\n",
    "        xticks=srs,\n",
    "        threshold=0.1,\n",
    "        cmap=\"tab10\",\n",
    "        group_by=group_by,  # NOTE: Either \"degree\" or \"component\" are available.\n",
    "        fontsize=12,\n",
    "    )\n",
    "    ax.legend(\n",
    "        loc=\"upper right\",\n",
    "        fontsize=12,\n",
    "        bbox_to_anchor=(0.99, 0.9),\n",
    "        borderaxespad=0,\n",
    "        frameon=False,\n",
    "    )\n",
    "    ax.plot(srs, rank, ls=\":\", color=\"k\")\n",
    "    ax.set_xticks([0.0, 0.5, 1.0, 1.5])\n",
    "    ax.set_xlabel(\"SR\", fontsize=14)\n",
    "    ax.set_ylabel(r\"$\\mathrm{C}$\", fontsize=14)\n",
    "axes[0].set_title(r\"group by $d$: degree\")\n",
    "axes[1].set_title(r\"group by $D$: the set of degree\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "上のセルで先に登場しましたが[`UnivariateViewer.to_dataframe`](https://rc-bootcamp.github.io/ipc-module/profiler/#ipc_module.profiler.UnivariateViewer.to_dataframe)メソッドを使うと、計算結果を[`polars.DataFrame`](https://docs.pola.rs/py-polars/html/reference/dataframe/)形式で計算結果を取得できます。\n",
    "[`polars`](https://pola.rs/)は[`pandas`](https://pandas.pydata.org/)と同じデータ解析のためのライブラリですが、より高速に動作します。\n",
    "\n",
    "[en]: #\n",
    "As mentioned earlier in the above cell, the [`UnivariateViewer.to_dataframe`](https://rc-bootcamp.github.io/ipc-module/profiler/#ipc_module.profiler.UnivariateViewer.to_dataframe) method can be used to retrieve the calculation results in the [`polars.DataFrame`](https://docs.pola.rs/py-polars/html/reference/dataframe/) format.\n",
    "[`polars`](https://pola.rs/) is a data analysis library similar to [`pandas`](https://pandas.pydata.org/), but it operates much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, rank = viewer.to_dataframe(max_scale=2.0)  # NOTE: Threshold is scaled by max_scale.\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "`polars`の詳細な使い方は[公式のドキュメント](https://docs.pola.rs/user-guide/getting-started/)を参照してください。\n",
    "次の節からは代表的な描画方法を紹介します。\n",
    "\n",
    "[en]: #\n",
    "Refer to the [official documentation](https://docs.pola.rs/user-guide/getting-started/) for detailed usage of `polars`.\n",
    "The next section introduces representative visualization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "##### $|D|=1$ の描画\n",
    "\n",
    "[en]: #\n",
    "##### Visualization for $|D|=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "$D$ の要素数が1、すなわち$D\\in\\{\\{1\\}, \\{2\\}, \\{3\\},~\\ldots\\} $ の場合はそのまま容量 $\\mathrm{C}$ を一次元のグラフとして描画できます。\n",
    "`viewer.to_dataframe`の引数に負の値を指定すると、$D$ の要素数がその絶対値のものだけを抽出できます (例 `df = viewer.to_dataframe(-1)`)。\n",
    "以下のグラフでは各スペクトル半径のデータに対して、$\\mathrm{C}[x, \\mathcal{P}_d(\\zeta^\\tau)]$ を描画します。\n",
    "また`DataFrame`内の要素を足し合わせる以外にも、`viewer.total`メソッドを用いて総容量を計算できます。\n",
    "\n",
    "[en]: #\n",
    "If the number of elements in $D$ is 1, i.e., $D \\in \\{\\{1\\}, \\{2\\}, \\{3\\},~\\ldots\\}$, the capacity $\\mathrm{C}$ can be visualized as a one-dimensional graph.\n",
    "By specifying a negative value as an argument to `viewer.to_dataframe`, you can extract only those with the absolute value of the number of elements in $D$ (e.g., `df = viewer.to_dataframe(-1)`).\n",
    "In the graph below, $\\mathrm{C}[x, \\mathcal{P}_d(\\zeta^\\tau)]$ is visualized for the data of each spectral radius.\n",
    "Additionally, instead of summing the elements in the `DataFrame`, you can use the `viewer.total` method to calculate the total capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_scale = 2.0\n",
    "degrees = [1, 2, 3]\n",
    "sr_ids = [4, 9, 14]\n",
    "\n",
    "df, rank = viewer.to_dataframe(-1, max_scale=0.0)  # NOTE: No truncation.\n",
    "\n",
    "grid_size = (len(sr_ids), len(degrees))\n",
    "fig, axes = plt.subplots(\n",
    "    *grid_size, figsize=(grid_size[1] * 4, grid_size[0] * 3), gridspec_kw=dict(hspace=0.1, wspace=0.1)\n",
    ")\n",
    "\n",
    "for (idy, sr_id), (idx, degree) in itertools.product(enumerate(sr_ids), enumerate(degrees)):\n",
    "    scale = viewer.calc_surr_max((degree,), max_scale=max_scale)[sr_id, 0]\n",
    "    capacity = viewer.total((degree,), max_scale=max_scale)[sr_id]\n",
    "    ax = axes[idy, idx]\n",
    "    df_sub = df.filter(df[\"degree\"] == degree).sort(\"del_0\")  # NOTE: Filter by degree.\n",
    "    delays = df_sub[\"del_0\"]\n",
    "    ipc = df_sub[f\"ipc_{sr_id}\"]\n",
    "    ax.plot(\n",
    "        delays,\n",
    "        ipc,\n",
    "        color=f\"C{idx}\",\n",
    "        lw=1,\n",
    "        label=f\"SR={srs[sr_id]:.2f}\",\n",
    "    )\n",
    "    ax.line_y(scale, color=\"red\", lw=1, ls=\"--\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_ylim([1e-4, 1.1])\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    if idy == 0:\n",
    "        ax.set_title(f\"$d={degree}$\", fontsize=14)\n",
    "    if idx == 0:\n",
    "        ax.set_ylabel(\"SR=\" + f\"{srs[sr_id]:.2f}\", fontsize=14)\n",
    "    else:\n",
    "        ax.set_yticklabels([])\n",
    "    if idy < len(sr_ids) - 1:\n",
    "        ax.set_xticklabels([])\n",
    "    else:\n",
    "        ax.set_xlabel(r\"$\\tau$\", fontsize=14)\n",
    "    ax.text(\n",
    "        0.95,\n",
    "        0.95,\n",
    "        \"C={:.2f}\".format(capacity),\n",
    "        fontsize=12,\n",
    "        ha=\"right\",\n",
    "        va=\"top\",\n",
    "        transform=ax.transAxes,\n",
    "    )\n",
    "fig.suptitle(r\"$\\mathrm{C}[x,\\mathcal{P}_d(\\zeta^\\tau)]$\" + f\", scale={max_scale:.2f}\", fontsize=16)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "##### $D=\\{1,1\\}$ の描画\n",
    "\n",
    "[en]: #\n",
    "##### Visualization for $D=\\{1,1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "今度は$D=\\{1,1\\}$ の場合を考えます。\n",
    "$|D|=1$ の時とは異なり、$D$ の要素数が2つあるため、2次元の平面上に描画できます。\n",
    "`df = viewer.to_dataframe((1, 1))`とすると、$D=\\{1,1\\}$ の成分のみを抽出した`DataFrame`を取得できます。\n",
    "\n",
    "[en]: #\n",
    "Now, let us consider the case where $D=\\{1,1\\}$.\n",
    "Unlike when $|D|=1$, since $D$ has two elements, it can be visualized on a two-dimensional plane.\n",
    "By using `df = viewer.to_dataframe((1, 1))`, you can obtain a `DataFrame` that extracts only the components for $D=\\{1,1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_scale = 2.0\n",
    "\n",
    "df, rank = viewer.to_dataframe((1, 1), max_scale=max_scale)\n",
    "capacities = viewer.total((1, 1), max_scale=max_scale)\n",
    "grid_size = (4, 5)\n",
    "fig = Figure(figsize=(grid_size[1] * 3, grid_size[0] * 3))\n",
    "fig.create_grid(*grid_size, hspace=0.3, wspace=0.3)\n",
    "\n",
    "pos = len(srs)\n",
    "ax_last = fig[pos // grid_size[1], pos % grid_size[1]]\n",
    "ax_last.create_grid(1, 2, width_ratios=[1, 20])\n",
    "cax = ax_last[0]\n",
    "cax.tick_params(labelsize=12)\n",
    "for idx, sr in enumerate(srs):\n",
    "    ax = fig[idx // grid_size[1], idx % grid_size[1]]\n",
    "    df_pivot = df.sort(\"del_0\", \"del_1\").pivot(\n",
    "        \"del_1\",\n",
    "        index=\"del_0\",\n",
    "        values=f\"ipc_{idx}\",\n",
    "    )\n",
    "    mat = df_pivot[:, 1:]\n",
    "    index = df_pivot[:, 0]\n",
    "    columns = list(map(int, df_pivot.columns[1:]))\n",
    "    ax.plot_matrix(\n",
    "        mat,\n",
    "        index=index,\n",
    "        column=columns,\n",
    "        cmap=\"viridis\",\n",
    "        zscale=\"log\",\n",
    "        vmin=1e-4,\n",
    "        vmax=1,\n",
    "        aspect=\"equal\",\n",
    "        xticks_kws=dict(num_tick=7),\n",
    "        yticks_kws=dict(num_tick=7),\n",
    "        colorbar=(idx == len(srs) - 1),\n",
    "        cax=cax if (idx == len(srs) - 1) else None,\n",
    "    )\n",
    "    ax.set_xlim(-0.5, 50.5)\n",
    "    ax.set_ylim(-0.5, 50.5)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax.text(\n",
    "        0.95,\n",
    "        0.95,\n",
    "        \"C={:.2f}\".format(capacities[idx]),\n",
    "        fontsize=12,\n",
    "        ha=\"right\",\n",
    "        va=\"top\",\n",
    "        transform=ax.transAxes,\n",
    "    )\n",
    "    ax.set_title(f\"SR={sr:.2f}\", fontsize=14)\n",
    "\n",
    "for idx in range(len(srs), grid_size[0] * grid_size[1]):\n",
    "    fig.delaxes(fig[idx // grid_size[1], idx % grid_size[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "#### 応用例: 入力の対称性の影響の確認\n",
    "\n",
    "[en]: #\n",
    "#### Application example: confirming the effect of input symmetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "最後に情報処理容量の有効性を示す例として、対称入力に対する影響を確認しましょう。\n",
    "先ほどと全く同じ条件のESNですが、入力のスケールを $[-1, 1]$ のままにしてESNに与えてみます。\n",
    "\n",
    "[en]: #\n",
    "Finally, as an example to demonstrate the effectiveness of IPC, let’s examine the impact on symmetric input.\n",
    "Using the same ESN conditions as before, provide the input to the ESN while keeping the scale at $[-1, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5678\n",
    "dim = 50\n",
    "t_washout = 10000\n",
    "t_sample = 100000\n",
    "srs = np.linspace(0.1, 1.7, 17)\n",
    "t_total = t_washout + t_sample\n",
    "display = True\n",
    "\n",
    "rnd = np.random.default_rng(seed)\n",
    "w_in = Linear(1, dim, bound=0.1, bias=0.0, rnd=rnd)\n",
    "net = ESN(dim, sr=srs[:, None], f=np.tanh, p=1, rnd=rnd)\n",
    "\n",
    "x0 = np.zeros((srs.shape[0], dim))\n",
    "us = rnd.uniform(-1, 1, (t_total, 1))\n",
    "\n",
    "x = x0\n",
    "xs = np.zeros((t_total, *x0.shape))\n",
    "for idx in trange(t_total, display=display):\n",
    "    x = net(x, w_in(us[idx]))  # NOTE: Use us[idx] for the symmetric case\n",
    "    xs[idx] = x\n",
    "\n",
    "print(\"us:\", us.shape)\n",
    "print(\"xs:\", xs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "先ほどと全く同じ条件で情報処理容量の計測を行います。\n",
    "同じく時間がかかるのでしばらくお待ちください。\n",
    "結果を`./output/ipc_symm.npz`として保存します。\n",
    "\n",
    "[en]: #\n",
    "We will measure the IPC under the same conditions as before.\n",
    "This will also take some time, so please wait.\n",
    "The results will be saved as `./output/ipc_symm.npz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True  # NOTE: Set it to False to run on CPU.\n",
    "\n",
    "if use_gpu:\n",
    "    import torch\n",
    "\n",
    "    assert torch.cuda.is_available(), \"CUDA is not available\"\n",
    "    us_c = torch.from_numpy(us).cuda()\n",
    "    xs_c = torch.from_numpy(xs).cuda()\n",
    "    args = (us_c, xs_c)\n",
    "else:\n",
    "    args = (us, xs)\n",
    "\n",
    "profiler = UnivariateProfiler(\n",
    "    *args,\n",
    "    \"Legendre\",\n",
    "    offset=t_washout,\n",
    "    surrogate_num=1000,\n",
    "    axis1=0,\n",
    "    axis2=-1,\n",
    ")\n",
    "\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "taus = [1000, 300, 50, 30, 15]\n",
    "for deg, tau in zip(degrees, taus, strict=True):\n",
    "    profiler.calc(deg, tau + 1)\n",
    "\n",
    "print(profiler.keys())\n",
    "\n",
    "profiler.save(\"./result/ipc_symm.npz\", srs=srs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "以下のセルは`ipc_asym.npz`と`ipc_symm.npz`両方のデータを読み込み、図表として比較します。\n",
    "後から計算された`ipc_symm.npz`の方は対称入力で活性化関数$\\tanh$ は奇関数であるため、偶数次数の成分の消失が期待されます。\n",
    "果たしてどうなるでしょうか？\n",
    "\n",
    "[en]: #\n",
    "The following cell loads data from both `ipc_asym.npz` and `ipc_symm.npz` and compares them in a figure.\n",
    "For `ipc_symm.npz`, which uses symmetric input, the even-order components are expected to vanish because the activation function $\\tanh$ is an odd function.\n",
    "What will the result be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"./result/ipc_asym.npz\", \"./result/ipc_symm.npz\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(files), figsize=(16, 6), gridspec_kw=dict(hspace=0.5))\n",
    "for idx, file in enumerate(files):\n",
    "    viewer = UnivariateViewer(file)\n",
    "    srs = viewer.info[\"srs\"]\n",
    "    df, rank = viewer.to_dataframe(max_scale=2.0)  # NOTE: Threshold is scaled by max_scale.\n",
    "    ax = axes[idx]\n",
    "    visualize_dataframe(\n",
    "        ax,\n",
    "        df,\n",
    "        xticks=srs,\n",
    "        threshold=0.1,\n",
    "        cmap=\"tab10\",\n",
    "        group_by=\"component\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "    ax.legend(\n",
    "        loc=\"upper right\",\n",
    "        fontsize=12,\n",
    "        bbox_to_anchor=(0.99, 0.9),\n",
    "        borderaxespad=0,\n",
    "        frameon=False,\n",
    "    )\n",
    "    ax.plot(srs, rank, ls=\":\", color=\"k\")\n",
    "    ax.set_xticks([0.0, 0.5, 1.0, 1.5])\n",
    "    ax.set_xlabel(\"SR\", fontsize=14)\n",
    "    ax.set_ylabel(r\"$\\mathrm{C}$\", fontsize=14)\n",
    "    ax.set_title(file, fontsize=16)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "前章同様に分岐図と条件付きLyapunov指数の計算と合わせて描画して比較してみましょう。\n",
    "\n",
    "[en]: #\n",
    "As in the previous chapter, let’s plot and compare the bifurcation diagram along with the calculation of the conditional Lyapunov exponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-4\n",
    "net.sr = np.linspace(0.1, 1.7, 161)[:, None]\n",
    "\n",
    "x0 = np.zeros((2, net.sr.shape[0], dim))\n",
    "us = rnd.uniform(-1, 1, (t_total, 1))\n",
    "\n",
    "t_washout, t_sample = 1000, 20000\n",
    "ts = np.arange(-t_washout, t_sample)\n",
    "\n",
    "x = x0\n",
    "xs = np.zeros((t_total, *x0.shape[1:]))\n",
    "lmbds = np.zeros((t_sample, net.sr.shape[0]))\n",
    "for idx, t in enumerate(tqdm(ts, display=display)):\n",
    "    if t == 0:\n",
    "        pert = rnd.uniform(-1, 1, x[0].shape)\n",
    "        pert = pert / np.linalg.norm(pert, axis=-1, keepdims=True)\n",
    "        x[1] = x[0] + pert * eps\n",
    "    x = net(x, w_in(us[idx]))\n",
    "    xs[idx] = x[0]\n",
    "    if t >= 0:\n",
    "        x_org, x_per = x[0], x[1]\n",
    "        x_diff = x_per - x_org\n",
    "        d_post = np.linalg.norm(x_diff, axis=-1, keepdims=True)\n",
    "        lmbd = np.log(np.abs(d_post / eps))\n",
    "        x_per[:] = x_org + x_diff * (eps / d_post)\n",
    "        lmbds[idx - t_washout] = lmbd[..., 0]\n",
    "\n",
    "\n",
    "def get_maxima_and_minima(xs, **kwargs):\n",
    "    id_maxima = sp.signal.find_peaks(xs, **kwargs)[0]\n",
    "    id_minima = sp.signal.find_peaks(-xs, **kwargs)[0]\n",
    "    return id_maxima, id_minima\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, sharex=True, figsize=(8, 10), gridspec_kw=dict(hspace=0.05))\n",
    "axl = axes[0]\n",
    "axl.set_xticklabels([])\n",
    "for idx, sr in enumerate(net.sr):\n",
    "    id_maxima, id_minima = get_maxima_and_minima(xs[t_washout:, idx, 0])\n",
    "    id_all = np.concatenate([id_maxima, id_minima])\n",
    "    peaks = xs[t_washout:, idx, 0][id_all]\n",
    "    axl.scatter(sr * np.ones(peaks.shape[0]), peaks, marker=\".\", s=0.01, color=\"k\")\n",
    "axl.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "axl.set_ylabel(r\"$x_0[k]$\", fontsize=14)\n",
    "axl.set_yticks([-1.0, 0.0, 1.0])\n",
    "axl.set_ylim(-1.1, 1.1)\n",
    "\n",
    "axr = axes[0].twinx()\n",
    "axr.plot(net.sr, lmbds.mean(axis=0), \"o-\", color=\"red\", label=\"MLE\")\n",
    "axr.set_yticks([-0.2, 0.0, 0.2])\n",
    "axr.set_ylim(-0.22, 0.22)\n",
    "axr.set_ylabel(r\"MLE: $\\lambda$\", fontsize=14)\n",
    "axr.set_xticklabels([])\n",
    "axr.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "\n",
    "viewer = UnivariateViewer(\"./result/ipc_symm.npz\")\n",
    "srs = viewer.info[\"srs\"]\n",
    "df, rank = viewer.to_dataframe(max_scale=2.0)  # NOTE: Threshold is scaled by max_scale.\n",
    "ax = axes[1]\n",
    "visualize_dataframe(\n",
    "    ax,\n",
    "    df,\n",
    "    xticks=srs,\n",
    "    threshold=0.1,\n",
    "    cmap=\"tab10\",\n",
    "    group_by=\"component\",\n",
    "    fontsize=12,\n",
    ")\n",
    "ax.legend(\n",
    "    loc=\"upper right\",\n",
    "    fontsize=12,\n",
    "    bbox_to_anchor=(0.99, 0.9),\n",
    "    borderaxespad=0,\n",
    "    frameon=False,\n",
    ")\n",
    "ax.set_xlabel(\"SR\", fontsize=14)\n",
    "ax.set_ylabel(r\"$\\mathrm{C}$\", fontsize=14)\n",
    "\n",
    "for ax in [axl, axr, axes[1]]:\n",
    "    ax.plot(srs, rank, ls=\":\", color=\"k\")\n",
    "    ax.set_xticks([0.0, 0.5, 1.0, 1.5, 2.0])\n",
    "    ax.set_xlim(srs.min() - 0.1, srs.max() + 0.1)\n",
    "fig.align_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "Q3.1. (Advanced)\n",
    "- `narma_func`を用い、NARMA10に対して情報処理容量を計測し、[5]のFigure 3の結果を再現せよ。\n",
    "- NARMAとESNの情報処理容量を比較し、NARMAが解けるESNの条件を考察せよ。\n",
    "\n",
    "[en]: #\n",
    "- Use `narma_func` to measure the IPC for NARMA10 and reproduce the results in Figure 3 of [5].\n",
    "- Compare the IPC of NARMA and ESN, and discuss the conditions under which the ESN can solve NARMA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "## 参考文献\n",
    "\n",
    "[en]: #\n",
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Dambre, J., Verstraeten, D., Schrauwen, B., & Massar, S. (2012). *Information Processing Capacity of Dynamical Systems*. Scientific Reports, 2(1), 514. https://doi.org/10.1038/srep00514\n",
    "\n",
    "[2] Xiu, D., & Karniadakis, G. E. (2002). *The Wiener--Askey Polynomial Chaos for Stochastic Differential Equations*. SIAM Journal on Scientific Computing, 24(2), 619–644. https://doi.org/10.1137/S1064827501387826\n",
    "\n",
    "[3] Hardy, G. H., & Ramanujan, S. (1918). *Asymptotic Formulaæ in Combinatory Analysis*. Proceedings of the London Mathematical Society, s2-17(1), 75–115. https://doi.org/10.1112/plms/s2-17.1.75\n",
    "\n",
    "[4] Theiler, J., Eubank, S., Longtin, A., Galdrikian, B., & Doyne Farmer, J. (1992). *Testing for Nonlinearity in Time Series: The Method of Surrogate Data*. Physica D: Nonlinear Phenomena, 58(1), 77–94. https://doi.org/10.1016/0167-2789(92)90102-S\n",
    "\n",
    "[5] Kubota, T., Takahashi, H., & Nakajima, K. (2021). *Unifying Framework for Information Processing in Stochastically Driven Dynamical Systems*. Physical Review Research, 3(4), 043135. https://doi.org/10.1103/PhysRevResearch.3.043135"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rc-bootcamp (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
