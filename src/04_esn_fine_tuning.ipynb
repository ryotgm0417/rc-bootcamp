{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "# Chapter 4. ESNの最適化\n",
    "\n",
    "[en]: #\n",
    "# Chapter 4. Fine-Tuning the Echo State Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "この章では、ESNや学習セットアップに登場するパラメータやハイパーパラメータの調整方法を学び、その計算能力を最大限に引き出す手法を説明します。\n",
    "\n",
    "[en]: #\n",
    "In this chapter you will learn how to tune ESN parameters and hyperparameters to maximize ESN performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "## 前書き\n",
    "\n",
    "[en]: #\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "前回説明したESNでNARMA10を解くセットアップとそのパラメータを整理しましょう。\n",
    "一様乱数からサンプルされた入力 $u[k]\\sim\\mathcal{U}([-1, 1])$ をESNに線形変換で投射し、ESNの状態 $x[k]$ からNARMA10時系列 $y[k]$ を構成するタスクを考えます。\n",
    "この際、以下のパラメータが登場します。\n",
    "\n",
    "[en]: #\n",
    "Let's revisit the ESN setup for solving NARMA10 and list its parameters.\n",
    "We consider the task where an input $u[k] \\sim \\mathcal{U}([-1,1])$ is linearly projected into an ESN and the NARMA10 series $y[k]$ is reconstructed from the reservoir state $x[k]$.\n",
    "The following parameters appear in this setup:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "- $v[k]=\\sigma u[k] + \\phi$\n",
    "  - $\\sigma \\in \\mathbb{R}$ : 入力スケール\n",
    "  - $\\phi \\in \\mathbb{R}$ : バイアス\n",
    "- $x[k+1] = (1-a)~x[k]+a~\\tanh\\left(\\rho W^\\mathrm{rec} x[k] + W^\\mathrm{in} v[k+1]\\right)$\n",
    "  - $N \\in \\mathbb{Z}^+$ : ESNのノード数\n",
    "  - $\\rho \\in \\mathbb{R}^+$ : スペクトル半径\n",
    "  - $a \\in \\mathbb{R}^+$ : 漏れ率 (leaky rate)\n",
    "  - $x[0]$ : ESNの初期値\n",
    "- $y[k+1] = \\mathrm{NARMA10}(\\nu[k],\\nu[k-9],~y[k],~\\ldots,~y[k-9];\\alpha,\\beta,\\gamma,\\delta),~\\nu[k] = \\mu u[k] + \\kappa$\n",
    "  - $\\alpha,\\beta,\\gamma,\\delta \\in \\mathbb{R}$ : 関数パラメータ\n",
    "  - $\\mu,\\kappa\\in \\mathbb{R}$ : スケーリングパラメータ\n",
    "- その他\n",
    "  - $T_\\mathrm{washout}\\in\\mathbb{Z}^+$ : ウォッシュアウトの時間ステップ\n",
    "  - $T_\\mathrm{train}\\in\\mathbb{Z}^+$ : 学習用データの時間ステップ\n",
    "  - $T_\\mathrm{eval}\\in\\mathbb{Z}^+$ : 評価用データの時間ステップ\n",
    "\n",
    "[en]: #\n",
    "- $v[k] = \\sigma u[k] + \\phi$\n",
    "  - $\\sigma \\in \\mathbb{R}$: Input scale\n",
    "  - $\\phi \\in \\mathbb{R}$: Bias\n",
    "- $x[k+1] = (1-a)~x[k] + a~\\tanh\\left(\\rho W^\\mathrm{rec} x[k] + W^\\mathrm{in} v[k+1]\\right)$\n",
    "  - $N \\in \\mathbb{Z}^+$: Number of ESN nodes\n",
    "  - $\\rho \\in \\mathbb{R}^+$: Spectral radius\n",
    "  - $a \\in \\mathbb{R}^+$: Leaky rate\n",
    "  - $x[0]$: Initial state of ESN\n",
    "- $y[k+1] = \\mathrm{NARMA10}(\\nu[k], \\nu[k-9],~y[k],~\\ldots,~y[k-9]; \\alpha, \\beta, \\gamma, \\delta),~\\nu[k] = \\mu u[k] + \\kappa$\n",
    "  - $\\alpha, \\beta, \\gamma, \\delta \\in \\mathbb{R}$: Function parameters\n",
    "  - $\\mu, \\kappa \\in \\mathbb{R}$: Scaling parameters\n",
    "- Miscellaneous\n",
    "  - $T_\\mathrm{washout} \\in \\mathbb{Z}^+$: Total time steps of washout\n",
    "  - $T_\\mathrm{train} \\in \\mathbb{Z}^+$: Total time steps of training data\n",
    "  - $T_\\mathrm{eval} \\in \\mathbb{Z}^+$: Total time steps of evaluation (validation) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "今フェアな比較のため、ターゲットとなるNARMA10のパラメータを典型的なもの $(\\alpha,\\beta,\\gamma,\\delta,\\mu,\\kappa)=(0.3,0.05,1.5,0.1,0.25,0.25)$ に固定し、 $u[k]$ とそれに対応するNARMA10の時系列 $y[k]$ は同じものを使います。\n",
    "ここでは残りのうち重要なパラメータである、入力スケーリング $(\\sigma, \\phi)$ 、ESNパラメータ $(N,\\rho,a)$ 、そして学習データの長さ $T_\\mathrm{train}$ をそれぞれ個別に、もしくはまとめて最適化してみましょう。\n",
    "\n",
    "[en]: #\n",
    "For a fair comparison, we fix the parameters of the target NARMA10 to typical values $(\\alpha,\\beta,\\gamma,\\delta,\\mu,\\kappa)=(0.3,0.05,1.5,0.1,0.25,0.25)$, and use the same input $u[k]$ and the corresponding time series $y[k]$ in the following runs.\n",
    "Below, we will optimize the remaining important parameters individually or collectively: input scaling $(\\sigma, \\phi)$, ESN parameters $(N, \\rho, a)$, and the length of training data $T_\\mathrm{train}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "## 演習問題と実演\n",
    "\n",
    "[en]: #\n",
    "## Exercises and demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "ここからは演習問題とデモンストレーションに移ります。\n",
    "前回と同じライブラリの他、前回の演習で実装した`ESN`・`Linear`・`narma_func`が`import`により利用できます。\n",
    "初めに次のセルを実行してください。\n",
    "\n",
    "なお`ESN`・`Linear`・`narma_func`の内部実装を再確認するには、`import inspect`以下の行をコメントアウトするか`...?? / ??...`を使用してください。\n",
    "\n",
    "[en]: #\n",
    "We now move to exercises and demonstrations.\n",
    "You can import the previously implemented `ESN`, `Linear`, and `narma_func` here as before.\n",
    "Please run the next cell first.\n",
    "If you want to inspect their implementations, uncomment the lines after `import inspect` or use `...?? / ??...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import drive  # type: ignore\n",
    "\n",
    "    if False:  # Set to True if you want to use Google Drive and save your work there.\n",
    "        drive.mount(\"/content/gdrive\")\n",
    "        %cd /content/gdrive/My Drive/[[PROJECT_NAME]]/\n",
    "        # NOTE: Change it to your own path if you put the zip file elsewhere.\n",
    "        # e.g., %cd /content/gdrive/My Drive/[PATH_TO_EXTRACT]/[[PROJECT_NAME]]/\n",
    "    else:\n",
    "        pass\n",
    "        %cd /content/\n",
    "        !git clone --branch [[BRANCH_NAME]] https://github.com/rc-bootcamp/[[PROJECT_NAME]].git\n",
    "        %cd /content/[[PROJECT_NAME]]/\n",
    "else:\n",
    "    sys.path.append(\".\")\n",
    "\n",
    "from utils.reservoir import ESN, Linear\n",
    "from utils.style_config import Figure, plt\n",
    "from utils.task import narma_func\n",
    "from utils.tester import load_from_chapter_name\n",
    "from utils.tqdm import tqdm, trange\n",
    "\n",
    "test_func, show_solution = load_from_chapter_name(\"04_esn_fine_tuning\")\n",
    "\n",
    "# Uncomment it to see the implementations of `Linear` and `ESN`.\n",
    "# import inspect\n",
    "# print(inspect.getsource(Linear))\n",
    "# print(inspect.getsource(ESN))\n",
    "# print(inspect.getsource(narma_func))\n",
    "\n",
    "# Or just use ??.../...?? (uncomment the following lines).\n",
    "# Linear??\n",
    "# ESN??\n",
    "# narma_func??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "### 1. 前準備とバッチ処理への拡張\n",
    "\n",
    "[en]: #\n",
    "### 1. Preparation and batch processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "ここからは類似する大量データセットに対して類似の操作を繰り返します。\n",
    "その前準備として、前回実装した`LRReadout`と`calc_nrmse`を改造し、複数のデータセットに対してまとめて線型回帰とNRMSEの計算が可能になった`BatchLRReadout`と`calc_batch_nrmse`を実装しましょう。\n",
    "なおこのような一括処理はバッチ処理 (batch processing)と呼ばれ、NumPyではコツさえつかめば比較的容易に実装できます。\n",
    "また前章で扱われたように、NumPy内部の並列演算によりfor loopを回すよりも効率的な演算が期待できます。\n",
    "\n",
    "[en]: #\n",
    "From here, we will repeat similar operations over many related datasets.\n",
    "As preparation, we extend `LRReadout` and `calc_nrmse` so they can perform linear regression and calculation of NRMSE on multiple datasets at once, giving `BatchLRReadout` and `calc_batch_nrmse`.\n",
    "This kind of batch processing is straightforward in NumPy once you get the broadcasting patterns right.\n",
    "It often outperforms explicit Python loops due to vectorized operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.1.\n",
    "\n",
    "[ja]: #\n",
    "前回学んだとおり、1列目にバイアス項を加えた予測変数 $\\tilde{X}=[1 : X]\\in\\mathbb{R}^{T\\times (N+1)}$ と予測対象変数 $Y\\in \\mathbb{R}^{T \\times D}$ に対して、 $\\|Xw - Y\\|^2$ を最小化する $\\hat{w}\\in\\mathbb{R}^{(N+1)\\times D}$ は下記式により求められる。\n",
    "\n",
    "[en]: #\n",
    "As learned previously, for the predictor matrix $\\tilde{X}=[1 : X]\\in\\mathbb{R}^{T\\times (N+1)}$ (with an added bias column) and the target sequence $Y\\in \\mathbb{R}^{T \\times D}$, the weights $\\hat{w}\\in\\mathbb{R}^{(N+1)\\times D}$ that minimize $\\|Xw - Y\\|^2$ are given by\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\renewcommand{\\Rho}{\\mathrm{P}}\n",
    "\\begin{align*}\n",
    "\\hat{w} &= (\\tilde{X}^\\top \\tilde{X})^{-1}{\\tilde{X}}^\\top Y \\\\\n",
    "&=\\tilde{X}^+ Y\n",
    ".\\end{align*}\n",
    "$$\n",
    "\n",
    "[ja]: #\n",
    "しかし[`np.linalg.lstsq`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html)の仕様上、前回実装した`LRReadout`では引数に多次元配列 $\\tilde{X}\\in\\mathbb{R}^{... \\times T \\times (N+1)},~Y\\in \\mathbb{R}^{... \\times T \\times D}$ を与えて、まとめて $\\hat{w}^\\mathrm{out} \\in \\mathbb{R}^{...\\times (N+1) \\times D}$ を得る操作は実現できない。\n",
    "[`np.linalg.pinv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html)では多次元配列 $A\\in\\mathbb{R}^{...\\times M \\times N}$ の入力が許容され、その返り値として疑似逆行列 $A^+\\in\\mathbb{R}^{...\\times N \\times M}$ が得られる。\n",
    "上記の式と前章の回答を参考にしながら以下の穴埋めを実装し、線型回帰のバッチ処理が可能になった`BatchLRReadout`を完成させよ。\n",
    "\n",
    "[en]: #\n",
    "However, due to the specifications of [`np.linalg.lstsq`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html), the previously implemented `LRReadout` cannot take multidimensional arrays $X\\in\\mathbb{R}^{... \\times T \\times (N+1)},~Y\\in \\mathbb{R}^{... \\times T \\times D}$ as arguments to obtain $\\hat{w}^\\mathrm{out} \\in \\mathbb{R}^{...\\times (N+1) \\times D}$ in batch processing.\n",
    "On the other hand, [`np.linalg.pinv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html) allows for input of multidimensional arrays $A\\in\\mathbb{R}^{...\\times M \\times N}$, returning the pseudo-inverse $A^+\\in\\mathbb{R}^{...\\times N \\times M}$.\n",
    "Based on the above equation and the answers from the previous chapter, fill in the blanks in the following code to complete `BatchLRReadout`, which enables batch processing for linear regression.\n",
    "\n",
    "[END]: #\n",
    "\n",
    "- `BatchLRReadout.train`\n",
    "  - Argument(s):\n",
    "    - `x`: `np.ndarray`\n",
    "      - `shape`: `(..., time_steps, input_dim)`\n",
    "    - `y`: `np.ndarray`\n",
    "      - `shape`: `(..., time_steps, output_dim)`\n",
    "  - Return(s):\n",
    "    - `self.weight`: `np.ndarray`\n",
    "      - `shape`: `(..., output_dim, input_dim)`\n",
    "    - `self.bias`: `np.ndarray`\n",
    "      - `shape`: `(..., 1, output_dim)`\n",
    "\n",
    "[ja]: #\n",
    "  - Operation(s):\n",
    "      - `self.weight`の更新\n",
    "      - `self.bias`の更新\n",
    "\n",
    "[en]: #\n",
    "  - Operation(s):\n",
    "      - Update `self.weight` with the obtained weight.\n",
    "      - Update `self.bias` with the obtained bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLRReadout(Linear):\n",
    "    def train(self, x: np.ndarray, y: np.ndarray):\n",
    "        assert (x.ndim > 1) and (x.shape[-1] == self.input_dim)\n",
    "        assert (y.ndim > 1) and (y.shape[-1] == self.output_dim)\n",
    "        x_biased = np.ones((*x.shape[:-1], x.shape[-1] + 1), dtype=self.dtype)\n",
    "        x_biased[..., 1:] = x\n",
    "        # RIGHT_B\n",
    "        sol = np.matmul(np.linalg.pinv(x_biased), y)\n",
    "        self.weight = sol[..., 1:, :].swapaxes(-2, -1)\n",
    "        self.bias = sol[..., :1, :]\n",
    "        # RIGHT_E\n",
    "        return self.weight, self.bias\n",
    "\n",
    "\n",
    "def solution(dim_in, dim_out, x_train, y_train, x_eval):\n",
    "    # DO NOT CHANGE HERE.\n",
    "    readout = BatchLRReadout(dim_in, dim_out)\n",
    "    readout.train(x_train, y_train)\n",
    "    return readout(x_eval)\n",
    "\n",
    "\n",
    "test_func(solution, \"01_01\")\n",
    "# show_solution(\"01_01\", \"BatchLRReadout\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.2.\n",
    "\n",
    "[ja]: #\n",
    "同様に長さ $T$ の多次元配列 $Y, \\hat{Y}\\in\\mathbb{R}^{...\\times T \\times d}$ に対してNRMSEをまとめて計算できる`calc_batch_nrmse`を実装せよ。\n",
    "ただしNRMSEは以下の式より与えられる。\n",
    "\n",
    "[en]: #\n",
    "Similarly, implement batch processing of NRMSE calculation as `calc_batch_nrmse`, taking multidimensional arrays $Y, \\hat{Y}\\in\\mathbb{R}^{...\\times T \\times d}$ of length $T$ as arguments.\n",
    "The NRMSE is given by the following equation:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{NRMSE}(y, \\hat{y}) :&= \\dfrac{\\mathrm{RMSE}(y, \\hat{y})}{\\sigma(y)} \\\\\n",
    "\\mathrm{RMSE}(y, \\hat{y}) :&=  \\sqrt{\\dfrac{\\mathrm{RSS}(y, \\hat{y}) }{T} }\n",
    ".\\end{align*}\n",
    "$$\n",
    "\n",
    "- `calc_batch_nrmse`\n",
    "  - Argument(s):\n",
    "    - `y`: `np.ndarray`\n",
    "      - `shape`: `(..., t, d)`\n",
    "    - `yhat`: `np.ndarray`\n",
    "      - `shape`: `(..., t, d)`\n",
    "  - Return(s):\n",
    "    - `nrmse`: `np.ndarray`\n",
    "      - `shape`: `(..., d)`\n",
    "\n",
    "[tips]: #\n",
    "- [`np.mean`](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)\n",
    "- [`np.var`](https://numpy.org/doc/stable/reference/generated/numpy.var.html)\n",
    "\n",
    "[/tips]: #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_batch_nrmse(y, yhat):\n",
    "    # BEGIN\n",
    "    mse = y - yhat\n",
    "    mse = (mse * mse).mean(axis=-2)\n",
    "    var = y.var(axis=-2)\n",
    "    nrmse = np.sqrt(mse / var)\n",
    "    return nrmse\n",
    "    # END\n",
    "\n",
    "\n",
    "test_func(calc_batch_nrmse, \"01_02\")\n",
    "# show_solution(\"01_02\", \"calc_batch_nrmse\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.3.\n",
    "\n",
    "[ja]: #\n",
    "$x[k] \\in \\mathbb{R}^{... T_\\mathrm{train}\\times N} $に対して、サンプリングならびに線形回帰をまとめて実行しNRMSEを計算する`train_and_eval`を完成させよ。\n",
    "ただし前章最後に記載した下記のコードを参考にせよ。\n",
    "\n",
    "[en]: #\n",
    "Implement the `train_and_eval` function that performs sampling, batch linear regression, and batch calculation of NRMSE for ESN state sequences $x[k] \\in \\mathbb{R}^{... T_\\mathrm{train}\\times N} $.\n",
    "Refer to the code provided below, which originally appeared at the end of the previous chapter.\n",
    "\n",
    "[END]: #\n",
    "\n",
    "```python\n",
    "x = x_init\n",
    "xs = np.zeros((t_total, dim_esn))\n",
    "for idx in range(t_total):\n",
    "    x = net(x, w_in(us[idx]))\n",
    "    xs[idx] = x\n",
    "x_train, y_train = xs[t_washout:-t_eval], ys[t_washout:-t_eval]\n",
    "x_eval, y_eval = xs[-t_eval:], ys[-t_eval:]\n",
    "w_out.train(x_train, y_train)\n",
    "```\n",
    "\n",
    "- `train_and_eval`\n",
    "  - Argument(s):\n",
    "    - `x0`: `np.ndarray`\n",
    "      - `shape`: `(..., n)`\n",
    "    - `ys`: `np.ndarray`\n",
    "      - `shape`: `(...., t_washout + t_train + t_eval, d)`\n",
    "  - Return(s):\n",
    "    - `nrmse`: `np.ndarray`\n",
    "      - `shape`: `(..., d)`\n",
    "    - `xs`: `np.ndarray`\n",
    "      - `shape`: `(..., t_washout + t_train + t_eval, n)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_setup(seed, dim, rho, a=None, f=np.tanh, bound=1.0, bias=0.0, cls=BatchLRReadout):\n",
    "    rnd = np.random.default_rng(seed)\n",
    "    w_in = Linear(1, dim, bound=bound, bias=bias, rnd=rnd)\n",
    "    net = ESN(dim, sr=rho, f=f, a=a, rnd=rnd)\n",
    "    w_out = cls(dim, 1)\n",
    "    return w_in, net, w_out\n",
    "\n",
    "\n",
    "def sample_dataset(\n",
    "    seed,\n",
    "    t_washout=1000,\n",
    "    t_train=2000,\n",
    "    t_eval=1000,\n",
    "    narma_parameters=None,\n",
    "):\n",
    "    narma_parameters = (\n",
    "        narma_parameters\n",
    "        if narma_parameters is not None\n",
    "        else dict(alpha=0.3, beta=0.05, gamma=1.5, delta=0.1, mu=0.25, kappa=0.25)\n",
    "    )\n",
    "    rnd = np.random.default_rng(seed)\n",
    "    t_total = t_washout + t_train + t_eval\n",
    "    ts = np.arange(-t_washout, t_train + t_eval)\n",
    "    us = rnd.uniform(-1, 1, (t_total, 1))\n",
    "    ys = narma_func(us, np.zeros((10, 1)), **narma_parameters)\n",
    "    time_info = dict(t_washout=t_washout, t_train=t_train, t_eval=t_eval)\n",
    "    return ts, us, ys, time_info\n",
    "\n",
    "\n",
    "def sample_dynamics(x0, w_in, net, ts, vs, display=False):\n",
    "    assert vs.shape[-2] == ts.shape[0]\n",
    "    x = x0\n",
    "    xs = np.zeros((*x.shape[:-1], ts.shape[0], x.shape[-1]))\n",
    "    for idx in trange(ts.shape[0], display=display):\n",
    "        x = net(x, w_in(vs[..., idx, :]))  # RIGHT Iterate over `ts` to sample the dynamics.\n",
    "        xs[..., idx, :] = x  # RIGHT Store the state `x` at each time step.\n",
    "    return xs\n",
    "\n",
    "\n",
    "def eval_nrmse(xs, ys, w_out, time_info, return_out=False, **kwargs):\n",
    "    t_washout, t_eval = time_info[\"t_washout\"], time_info[\"t_eval\"]\n",
    "    x_train, y_train = xs[..., t_washout:-t_eval, :], ys[..., t_washout:-t_eval, :]  # RIGHT Specify training range.\n",
    "    x_eval, y_eval = xs[..., -t_eval:, :], ys[..., -t_eval:, :]  # RIGHT Specify evaluation range.\n",
    "    out = w_out.train(x_train, y_train, **kwargs)\n",
    "    y_out = w_out(x_eval)\n",
    "    nrmse = calc_batch_nrmse(y_eval, y_out)\n",
    "    if return_out:\n",
    "        return nrmse, *out\n",
    "    else:\n",
    "        return nrmse\n",
    "\n",
    "\n",
    "def train_and_eval(x0, w_in, net, w_out, ts, vs, ys, time_info, display=False):\n",
    "    assert vs.shape[-2] == ts.shape[0]\n",
    "    assert ys.shape[-2] == ts.shape[0]\n",
    "    xs = sample_dynamics(x0, w_in, net, ts, vs, display=display)\n",
    "    nrmse = eval_nrmse(xs, ys, w_out, time_info)\n",
    "    return nrmse, xs\n",
    "\n",
    "\n",
    "test_func(train_and_eval, \"01_03\", multiple_output=True)\n",
    "# show_solution(\"01_03\", \"sample_dynamics\")  # Uncomment it to see the solution.\n",
    "# show_solution(\"01_03\", \"eval_nrmse\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "### 2. 入力スケーリング\n",
    "\n",
    "[en]: #\n",
    "### 2. Input scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "入力スケーリングのパラメータ $(\\sigma, \\phi)$ の効果を確認しましょう。\n",
    "\n",
    "[en]: #\n",
    "Let's examine the effect of the input scaling parameters $(\\sigma, \\phi)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "まずこれらのパラメータの機能的意義を説明します。\n",
    "前述のとおり今 $(\\sigma, \\phi)$ を用いて入力 $u[k]$ を以下のように変換します。\n",
    "\n",
    "[en]: #\n",
    "First, we explain the role of these parameters.\n",
    "As stated earlier, the input $u[k]$ is transformed using $(\\sigma,\\phi)$ as follows:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "v[k]= \\sigma u[k] + \\phi\n",
    ".\\end{align*}\n",
    "$$\n",
    "\n",
    "[ja]: #\n",
    "$u[k]\\sim\\mathcal{U}([-1, 1])$ なので $v[k]\\sim\\mathcal{U}([-\\sigma + \\phi, \\sigma + \\phi])$ となります。\n",
    "分布の範囲より、 $\\sigma$ は入力の分散、 $\\phi$ は平均を調整します (分散 $\\mathrm{Var}[v]=\\frac{1}{3}\\sigma^2$ 、平均 $\\mathrm{E}[v]=\\phi$) 。\n",
    "これらの効果は活性化関数の「形状」との関連で議論できます。\n",
    "例えば図1に示されるとおり、 $\\tanh$ は奇関数 (すなわち $\\tanh\\left(-y\\right)=-\\tanh\\left(y\\right)$ )です。\n",
    "\n",
    "[en]: #\n",
    "Because $u[k]\\sim\\mathcal{U}([-1, 1])$, the transformed input $v[k]$ follows $v[k]\\sim\\mathcal{U}([-\\sigma + \\phi, \\sigma + \\phi])$.\n",
    "Therefore, $\\sigma$ adjusts the variance of the input, and $\\phi$ adjusts the mean (variance $\\mathrm{Var}[v]=\\frac{1}{3}\\sigma^2$, mean $\\mathrm{E}[v]=\\phi$).\n",
    "The effect of these parameters can be discussed in relation to the \"shape\" of the activation function.\n",
    "For example, $\\tanh$ is an odd function (i.e., $\\tanh\\left(-y\\right)=-\\tanh\\left(y\\right)$), as shown in Figure 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; width: 750px; margin: auto; background-color: #f8f9fa; padding: 10px; border-radius: 10px;\">\n",
    "\n",
    "![fig_4_1.webp](../assets/fig_4_1.webp)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[figc]: #\n",
    "\n",
    "[ja]: #\n",
    "図1 $f(x)=\\tanh(x)$ とその導関数 $\\dfrac{df}{dx}(x)=1-\\tanh^2(x)$ の形状。\n",
    "\n",
    "[en]: #\n",
    "Figure 1 \"Shape\" of $f(x)=\\tanh(x)$ and its derivative $\\dfrac{df}{dx}(x)=1-\\tanh^2(x)$\n",
    "\n",
    "[END]: #\n",
    "\n",
    "[/figc]: #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "一方で $v[k]$ を展開すると $x[k]$ の時間発展は以下のとおり表現されます。\n",
    "\n",
    "[en]: #\n",
    "By expanding $v[k]$, the temporal evolution of $x[k]$ can be expressed as follows:\n",
    "\n",
    "[END]: #\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x[k+1] = \\tanh\\left(\\rho W^\\mathrm{rec} x[k] + \\sigma W^\\mathrm{in} u[k+1] + \\phi W^\\mathrm{in}\\right)\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "まず $\\phi=0$ に固定して $\\sigma$ の特性を考察します。\n",
    "式と図からわかるとおり $\\sigma$ が大きいほど $\\tanh$ の「平ら」な範囲に定義域がおよびます。\n",
    "一方で $\\sigma$ が小さいほど $y=x$ に $\\tanh$ が近似できる領域に収まります。\n",
    "このように $\\sigma$ はESNの非線形性に大きな影響を与えます。\n",
    "\n",
    "[en]: #\n",
    "We begin by focusing on the characteristics of $\\sigma$, by fixing $\\phi=0$.\n",
    "As can be seen from the equation and the figure, the larger $\\sigma$ is, the more the domain of $\\tanh$ extends into its \"flat\" range.\n",
    "Conversely, the smaller $\\sigma$ is, the more $\\tanh$ argument stays in its near-linear region, where $\\tanh\\approx x$.\n",
    "Therefore, $\\sigma$ has a significant impact on the nonlinearity of the ESN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "次に $\\phi$ の効果を確認しましょう。\n",
    "$\\phi$ は、後の情報処理能力 (Information Processing Capacity; IPC)の章でより詳細に説明されますが、変換に含まれる成分の次数に影響をあたえます。\n",
    "今入力が対称 (平均0) であるので、 $x[k]$ を $\\{ u[k],~u[k-1].~\\ldots\\}$ を用いて多項式展開すると、$\\phi=0$のケースでは、奇数次数の要素 ( $u^3[\\cdot], u^5[\\cdot], u[\\cdot]u^2[\\cdot]$ 等) のみ登場します。\n",
    "対照的に$\\phi\\neq 0$の場合、0平均でなくなるので、その要素に偶数次数 ( $u^2[\\cdot], u^4[\\cdot]$ 等)の要素も加わります。\n",
    "\n",
    "[en]: #\n",
    "Next, let's examine the effect of $\\phi$.\n",
    "This will be explained in more detail in a later chapter on information processing capacity (IPC), but in short, $\\phi$ influences the order of components included in the transformation.\n",
    "Given that the input is symmetric (mean 0), if we polynomially expand $x[k]$ using $\\{ u[k],~u[k-1].~\\ldots\\}$, only odd-order elements (such as $u^3[\\cdot], u^5[\\cdot], u[\\cdot]u^2[\\cdot]$) will appear in the case of $\\phi=0$.\n",
    "In contrast, the input will no longer have a zero mean when $\\phi \\neq 0$, and so even-order elements (such as $u^2[\\cdot], u^4[\\cdot]$) will appear as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "これらの関数の次数の分布はタスク性能に大きな影響を及ぼします。\n",
    "例えば今回使うパラメータセットにおけるNARMA10タスクは、解析によりそのほとんどが以下に記される成分で占められると判明しています<sup>[1]</sup>。\n",
    "- 1次: $u[k-1],~u[k-2],~u[k-3],~u[k-10],~u[k-11],~u[k-12]$\n",
    "- 2次: $u[k-1]u[k-10],~u[k-2]u[k-11],~u[k-3]u[k-12]$\n",
    "\n",
    "したがって$\\phi=0$と$\\phi\\neq0$のケースで性能に大きな差が生じます。\n",
    "以下の演習問題においてここまで確認した$(\\sigma, \\phi)$の影響を確認しましょう。\n",
    "\n",
    "[en]: #\n",
    "Such distributions of components have a significant impact on task performance.\n",
    "For example, analysis has shown that the NARMA10 task with the current setting of parameters is largely composed of the following components<sup>[1]</sup>:\n",
    "- 1st-order: $u[k-1],~u[k-2],~u[k-3],~u[k-10],~u[k-11],~u[k-12]$\n",
    "- 2nd-order: $u[k-1]u[k-10],~u[k-2]u[k-11],~u[k-3]u[k-12]$\n",
    "\n",
    "Therefore, there is a significant performance difference between the cases of $\\phi=0$ and $\\phi\\neq0$.\n",
    "In the following exercise, let's verify the effects of $(\\sigma, \\phi)$ that we have discussed so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.1.\n",
    "\n",
    "[ja]: #\n",
    "長さ $k$ の配列としてパラメータ $\\Sigma = (\\sigma_0$, $\\sigma_1,~\\ldots,~\\sigma_{k-1})$ ならびに $\\Phi = (\\phi_0$, $\\phi_1,~\\ldots,~\\phi_{k-1})$ が与えられる。\n",
    "各 $(\\sigma_i, \\phi_i)$ に対するNRMSEをまとめて評価するための $v_i[k] = \\sigma_i u[k] + \\phi_i$ を出力する関数`convert_us_into_vs`を完成させよ。\n",
    "ただし与えられる`us`として与えられる $U=\\{u[0],~\\ldots,~u[k-1]\\} \\in \\mathbb{R}^{T \\times 1}$ の`shape` に留意せよ。\n",
    "\n",
    "[en]: #\n",
    "Parameters $\\Sigma = (\\sigma_0$, $\\sigma_1,~\\ldots,~\\sigma_{k-1})$ and $\\Phi = (\\phi_0$, $\\phi_1,~\\ldots,~\\phi_{k-1})$ are given as arrays of length $k$.\n",
    "Fill in the blanks in the following code to complete the function `convert_us_into_vs`, which outputs $v_i[k] = \\sigma_i u[k] + \\phi_i$ for each $(\\sigma_i, \\phi_i)$ at once.\n",
    "Note that you should pay attention to the `shape` of the given input `us`, which is $U=\\{u[0],~\\ldots,~u[k-1]\\} \\in \\mathbb{R}^{T \\times 1}$.\n",
    "\n",
    "[END]: #\n",
    "\n",
    "- `convert_us_into_vs`\n",
    "  - Argument(s):\n",
    "    - `us`: `np.ndarray`\n",
    "      - `shape`: `(t, 1)`\n",
    "    - `sigma`: `np.ndarray`\n",
    "      - `shape`: `(k,)`\n",
    "    - `phi`: `np.ndarray`\n",
    "      - `shape`: `(k,)`\n",
    "  - Return(s):\n",
    "    - `vs`: `np.ndarray`\n",
    "      - `shape`: `(k, t, 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_us_into_vs(us, sigma, phi):\n",
    "    assert len(sigma) == len(phi)\n",
    "    vs = sigma[:, None, None] * us + phi[:, None, None]  # RIGHT Use broadcasting to convert `us` into `vs`.\n",
    "    return vs\n",
    "\n",
    "\n",
    "test_func(convert_us_into_vs, \"02_01\")\n",
    "# show_solution(\"02_01\", \"convert_us_into_vs\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "実装した`convert_us_into_vs`を用いて入力が対称なケース ( $v[k]\\sim \\mathcal{U}([-\\sigma, \\sigma])$ ) と非対称なケース ( $v[k]\\sim \\mathcal{U}([0, \\sigma])$ )で比較してみましょう。\n",
    "$\\sigma$ が小さいケースで非対称なケースが精度の面で対称入力を上回っています。\n",
    "\n",
    "[en]: #\n",
    "Using the implemented `convert_us_into_vs`, let's compare the symmetric case ($v[k]\\sim \\mathcal{U}([-\\sigma, \\sigma])$) with the asymmetric case ($v[k]\\sim \\mathcal{U}([0, \\sigma])$).\n",
    "When $\\sigma$ is small, the asymmetric case outperforms the symmetric case in terms of task accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_setup, seed_dataset = 1234, 5678  # you can freely change here\n",
    "dim, rho = 100, 0.9\n",
    "dataset_info = dict(t_washout=100, t_train=2000, t_eval=1000)\n",
    "\n",
    "w_in, net, w_out = create_setup(seed_setup, dim, rho, f=np.tanh)\n",
    "ts, us, ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "\n",
    "\n",
    "sigmas = np.logspace(-2, 0, 21)  # 10^{-2.0}, 10^{-1.9}, ... 10^{0.0}\n",
    "x0 = np.zeros((sigmas.shape[0], net.dim))\n",
    "\n",
    "# Symmetrical case (phi=0).\n",
    "vs_sym = convert_us_into_vs(us, sigmas, np.zeros_like(sigmas))\n",
    "nrmse_sym, _xs_sym = train_and_eval(x0, w_in, net, w_out, ts, vs_sym, ys, time_info)\n",
    "best_sym = np.argmin(nrmse_sym[:, 0])\n",
    "\n",
    "# Asymmetrical case (phi=sigma).\n",
    "vs_asym = convert_us_into_vs(us, 0.5 * sigmas, 0.5 * sigmas)\n",
    "nrmse_asym, _xs_asym = train_and_eval(x0, w_in, net, w_out, ts, vs_asym, ys, time_info)\n",
    "best_asym = np.argmin(nrmse_asym[:, 0])\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.plot(sigmas, nrmse_sym[:, 0], label=r\"sym: $[-\\sigma, \\sigma]$\")\n",
    "ax.plot(sigmas, nrmse_asym[:, 0], label=r\"asym: $[0, \\sigma]$\")\n",
    "ax.scatter(sigmas[best_sym], nrmse_sym[best_sym, 0], s=100.0, marker=\"*\")\n",
    "ax.scatter(sigmas[best_asym], nrmse_asym[best_asym, 0], s=100.0, marker=\"*\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(r\"$\\sigma$\")\n",
    "ax.set_ylabel(r\"NRMSE (best: $\\bigstar$)\")\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.2. (Advanced)\n",
    "\n",
    "[ja]: #\n",
    "- 他の非線形関数に関しても試してみよ。特に $f$ を偶関数にするとどうなるか実験し、結果を考察せよ。\n",
    "- $\\sigma$ が大きくなると急激に精度が悪化する。$x[k]$ の時系列を描画しながら比較し、その理由を考察せよ。\n",
    "\n",
    "[en]: #\n",
    "- Try the same experiment for other nonlinear activation functions. In particular, try an even activation function and observe the effect.\n",
    "- The accuracy deteriorates quickly when $\\sigma$ is large. Discuss why this happens, by comparing the time series of $x[k]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "### 3. ESNパラメータ\n",
    "\n",
    "[en]: #\n",
    "### 3. ESN parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "次にESN内のパラメータ $(N, \\rho, a)$ の役割を確認しましょう。\n",
    "\n",
    "[en]: #\n",
    "Next, let's examine the role of the ESN's internal parameters $(N, \\rho, a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "#### ESNのノード数\n",
    "\n",
    "[en]: #\n",
    "#### Number of ESN nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "まず一般的にリザバーの性能は次元数 $N$ が大きいほど高くなります。\n",
    "これは、後のIPCの章で議論されますが、IPCの上限がリザバーの次元数 $N$ によって決定されるからです。\n",
    "また$N$の大きさは**高次元性**と呼ばれリザバーの性能を決定づける重要な特性といえます。\n",
    "\n",
    "次のセルは $N$ を変化させたときの NRMSEの変化を描画します。\n",
    "\n",
    "[en]: #\n",
    "In general, the performance of RC improves as the **dimensionality** $N$ increases.\n",
    "As will be discussed in the IPC chapter later, this is because the upper limit of IPC is determined by the dimensionality $N$ of the reservoir.\n",
    "Therefore, the size of $N$ is a significant parameter that determines the performance of the reservoir.\n",
    "\n",
    "Executing the next cell will plot the change in NRMSE according to the change in $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_setup, seed_dataset = 1234, 5678\n",
    "dims = [10, 20, 50, 100, 200, 500, 1000]\n",
    "rho = 0.9\n",
    "dataset_info = dict(t_washout=100, t_train=2000, t_eval=1000)\n",
    "\n",
    "w_in, net, w_out = create_setup(seed_setup, dim, rho, f=np.tanh)\n",
    "ts, us, ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "vs = 0.05 * us + 0.05  # vs \\in [0.0, 0.1]\n",
    "\n",
    "nrmses = []\n",
    "for dim in tqdm(dims):\n",
    "    x0 = np.zeros(dim)\n",
    "    w_in, net, w_out = create_setup(seed_setup, dim, rho, f=np.tanh)\n",
    "    nrmse = train_and_eval(x0, w_in, net, w_out, ts, vs, ys, time_info)\n",
    "    nrmses.append(nrmse[0])\n",
    "nrmses = np.asarray(nrmses)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.plot(dims, nrmses, marker=\".\", markersize=10.0)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"# of ESN nodes\")\n",
    "ax.set_ylabel(\"NRMSE\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "より厳密にはリザバーが有するIPCの上限は 階数 $r (\\leq N)$ によって決定されます。\n",
    "これは線形独立な状態時系列の数を表し、一般的に $X$ (定数成分を除外しない場合) または分散共分散行列$C(X):=\\mathrm{E}[(X-\\mathrm{E}[X])^\\top (X-\\mathrm{E}[X])]$ (定数成分を除外する場合) の階数 から計算されます。\n",
    "例えば、以下の次元数 $N=3$ の状態行列 $X=[{x}_0; {x}_1; {x}_2]^{}\\in\\mathbb{R}^{T\\times 3}$を考えます。\n",
    "\n",
    "[en]: #\n",
    "More precisely, the upper limit of a reservoir's IPC is determined by its rank $r (\\leq N)$.\n",
    "This represents the number of linearly independent states and is usually calculated as the rank of $X$ (when we include the constant bias component) or its covariance matrix $C(X):=\\mathrm{E}[(X-\\mathrm{E}[X])^\\top (X-\\mathrm{E}[X])]$ (when we exclude the constant bias component).\n",
    "For example, consider the following state matrix $X=[{x}_0; {x}_1; {x}_2]^{}\\in\\mathbb{R}^{T\\times 3}$ with dimensionality $N=3$:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align}\n",
    "{x}_0 &= [u[0],~u[1],~\\ldots,~u[T-1]]^\\top, \\\\\n",
    "{x}_1 &= [u^2[0],~u^2[1],~\\ldots,~u^2[T-1]]^\\top, \\\\\n",
    "{x}_2 &= [3u^2[0]-4u[0],~3u^2[1]-4u[1],~\\ldots,~3u^2[T-1]-4u[T-1]] ^\\top\n",
    ".\\end{align}\n",
    "$$\n",
    "\n",
    "[ja]: #\n",
    "$x_2=3x_1 - 4x_0$ より $x_2$ は線形従属で $X$ の階数は $2$となります。\n",
    "\n",
    "[en]: #\n",
    "Since $x_2=3x_1 - 4x_0$, $x_2$ is linearly dependent and therefore the rank of $X$ is 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "In [*]: rnd = np.random.default_rng(1234)\n",
    "   ...: us = rnd.uniform(-1, 1, (100, 1))\n",
    "   ...: xs = np.concatenate([us, us**2, 3 * us**2 - 4 * us], axis=1)\n",
    "   ...: xs_m = xs - xs.mean(axis=0)\n",
    "   ...: print('rank: {}'.format(np.linalg.matrix_rank(xs_m.T @ xs_m)))\n",
    "rank: 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "線型回帰とその出力 $\\hat{Y}$ は、$X$ によって張られる部分空間に対する $Y$ の射影として解釈され、射影行列 $P_X=X X^+$を用いて $\\hat{Y}=P_X Y$と表されます。\n",
    "そしてその部分空間の次元は $X$ の階数 $r$ によって規定されます。\n",
    "上記の例では線形従属な成分 $x_2$ は部分空間の次元を増やさず、したがって残差誤差 $\\|P_X Y - Y \\|^2$ の低減には寄与しません。\n",
    "IPC的な意味では、$x_2$はIPCの増加に寄与しない余分な成分といえます。\n",
    "このように階数 $r$ は冗長な成分を定量的に評価する重要な指標で、RCの文脈で頻繁に登場します。\n",
    "\n",
    "階数が最大でないケースは、活性化関数に恒等写像を採用した線形ESNの場合に簡単に構成できます。\n",
    "以下は線形ESNと $\\tanh$ を活性化関数に有する非線形ESNの階数を比較するコードです。\n",
    "\n",
    "[en]: #\n",
    "The output of linear regression, $\\hat{Y}$, can be interpreted as the projection of $Y$ onto the subspace spanned by $X$, expressed as $\\hat{Y}=P_X Y$ using the projection matrix $P_X=X X^+$.\n",
    "The dimension of this subspace is determined by the rank $r$ of $X$.\n",
    "In the $N=3$ example above, the linearly dependent component $x_2$ does not increase the dimension of the subspace and thus does not contribute to the reduction of the residual error $\\|P_X Y - Y \\|^2$.\n",
    "In terms of IPC, $x_2$ is an extra component that does not contribute to an increase in IPC.\n",
    "In this way, the rank $r$ is an indicator for quantitatively evaluating redundant components and appears frequently in the context of RC.\n",
    "\n",
    "A case where the rank is not maximal can be easily constructed using a linear ESN with the identity map as the activation function.\n",
    "The code below compares the rank of a linear ESN with that of a nonlinear ESN using $\\tanh$ as its activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_setup, seed_dataset = 1234, 5678\n",
    "sigma, phi = 0.05, 0.05  # vs \\in [0.0, 0.1]\n",
    "dims = np.arange(1, 11) * 10\n",
    "rho = 0.9\n",
    "dataset_info = dict(t_washout=100, t_train=2000, t_eval=0)\n",
    "\n",
    "ts, us, _ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "vs = sigma * us + phi\n",
    "\n",
    "rank_nlin, rank_lin = [], []\n",
    "for dim in tqdm(dims):\n",
    "    x0 = np.zeros(dim)\n",
    "    t_washout = dataset_info[\"t_washout\"]\n",
    "    w_in, net, _w_out = create_setup(seed_setup, dim, rho, f=np.tanh)\n",
    "    # Non-linear case\n",
    "    xs_nlin = sample_dynamics(x0, w_in, net, ts, vs)[t_washout:]\n",
    "    xs_nlin -= xs_nlin.mean(axis=0)\n",
    "    rank_nlin.append(np.linalg.matrix_rank(xs_nlin.T @ xs_nlin))\n",
    "    # Linear case\n",
    "    net.f = lambda val: val  # Identity function\n",
    "    xs_lin = sample_dynamics(x0, w_in, net, ts, vs)[t_washout:]\n",
    "    xs_lin -= xs_lin.mean(axis=0)\n",
    "    rank_lin.append(np.linalg.matrix_rank(xs_lin.T @ xs_lin))\n",
    "rank_nlin = np.asarray(rank_nlin)\n",
    "rank_lin = np.asarray(rank_lin)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.plot(dims, dims, color=\"k\", label=r\"$r=N$\")\n",
    "ax.plot(dims, rank_nlin, marker=\".\", markersize=10.0, color=\"red\", ls=\":\", label=r\"$f=\\tanh$\")\n",
    "ax.plot(\n",
    "    dims,\n",
    "    rank_lin,\n",
    "    marker=\".\",\n",
    "    markersize=10.0,\n",
    "    color=\"blue\",\n",
    "    ls=\"--\",\n",
    "    label=r\"$f=\\mathrm{id}_\\mathbb{R}$\",\n",
    ")\n",
    "ax.set_xlabel(\"# of ESN nodes\")\n",
    "ax.set_ylabel(r\"rank $r$\")\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.1. (Advanced)\n",
    "\n",
    "[ja]: #\n",
    "- 線形なケースだと$N$が大きい時に上の例に示されるように階数が最大にならない。この原因を直感的に理解するため、各$N$に対して分散共分散行列 $C(X)$ の固有値 $\\{\\lambda_i \\}_{i=0}^{N-1}$ の絶対値 $|\\lambda_{i}|$ を昇順に並べ対数グラフで描画し、線形ESNと非線形ESNを比較せよ。\n",
    "- 固有値の絶対値の分布を基に考察し、線形ESNでも階数を最大にするための$W^\\mathrm{rec}$の上手い初期化の戦略を考察せよ。\n",
    "\n",
    "[en]: #\n",
    "- In the linear case, the rank does not become maximal when $N$ is large, as shown in the above example. To understand this cause intuitively, plot the absolute values $|\\lambda_{i}|$ of the eigenvalues $\\{\\lambda_i \\}_{i=0}^{N-1}$ of the covariance matrix $C(X)$ in ascending order on a logarithmic graph. Compare this between the linear and nonlinear ESNs.\n",
    "- Based on the distribution of the absolute values of the eigenvalues, devise a method of initializing $W^\\mathrm{rec}$ that maximizes the rank even in a linear ESN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "#### スペクトル半径\n",
    "\n",
    "[en]: #\n",
    "#### Spectral radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "次にスペクトル半径 $\\rho$ の影響を調査します。\n",
    "前章で扱ったように、スペクトル半径 $\\rho$ は $W^\\mathrm{rec}$ の固有値 $\\{\\lambda_i \\}_{i=0}^{N-1}$ のうち最大の絶対値として以下の式を用いて定義されます。\n",
    "\n",
    "[en]: #\n",
    "Next, we will examine the effect of the spectral radius $\\rho$.\n",
    "As introduced in the previous chapter, the spectral radius $\\rho$ is defined as the maximum absolute value among the eigenvalues $\\{\\lambda_i \\}_{i=0}^{N-1}$ of $W^\\mathrm{rec}$ using the following formula:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\rho(W^\\mathrm{rec}):=\\max_{i} |\\lambda_{i}|\n",
    ".\\end{align*}\n",
    "$$\n",
    "\n",
    "[ja]: #\n",
    "スペクトル「半径」と呼ばれる理由は、前回紹介されたとおり、固有値を複素平面上に並べると視覚的に理解できます。\n",
    "特に正規分布 $\\mathcal{N}\\left(0, \\frac{1}{N}\\right)$ より $W^\\mathrm{rec} \\in \\mathbb{R}^{N\\times N}$ の各要素をサンプリングする場合、その固有値は半径1の単円板上におおよそ一様分布する点が知られています (より厳密には複素平面上で半径1の単位円板上の一様測度に $N\\to \\infty$ でほとんど確実に分布収束)。\n",
    "またこれは[円則](https://ja.wikipedia.org/wiki/%E3%83%A9%E3%83%B3%E3%83%80%E3%83%A0%E8%A1%8C%E5%88%97#%E5%86%86%E5%89%87)として知られるランダム行列の重要な特性です。\n",
    "\n",
    "`ESN`では、`normalize=True`によってスペクトル半径 $\\rho$ を分離しハイパーパラメータとして扱えます。\n",
    "まずは様々な $\\rho$ に対して一度に $x[t]$ のサンプリングを可能にする`reshape_rho`を以下の設問で実装しましょう。\n",
    "\n",
    "[en]: #\n",
    "You can visually understand why it is called the spectral \"radius\" by arranging the eigenvalues on a complex plane.\n",
    "Especially when each element of $W^\\mathrm{rec} \\in \\mathbb{R}^{N\\times N}$ is sampled from the normal distribution $\\mathcal{N}\\left(0, \\frac{1}{N}\\right)$, the eigenvalues are approximately uniformly distributed on a unit disk of radius 1 (more precisely, as $N\\to \\infty$, they almost surely converge to a uniform distribution on the unit disk, a phenomenon known as the [*circular law*](https://en.wikipedia.org/wiki/Circular_law) in random matrix theory).\n",
    "\n",
    "In the class `ESN`, the spectral radius $\\rho$ can be separated and treated as a hyperparameter by setting the option `normalize = True`.\n",
    "To begin with, let's implement the function `reshape_rho` that allows the simultaneous sampling of $x[t]$ for multiple values of $\\rho$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.2.\n",
    "\n",
    "[ja]: #\n",
    "長さ$k$ の配列としてスペクトル半径 $\\Rho = \\left(\\rho_0, \\rho_1,~\\ldots,~\\rho_{k-1} \\right)$ が与えられる。\n",
    "同じ入力 $u[k]$ と $W^\\mathrm{rec}$ に対してスペクトル半径のみを $\\rho_{i}$ に変えたときの $x_{i}[k]$ をまとめてサンプリングできるように、`reshape_rho`を完成させよ。\n",
    "\n",
    "[en]: #\n",
    "An array of spectral radii $\\Rho = \\left(\\rho_0, \\rho_1,~\\ldots,~\\rho_{k-1} \\right)$ with length $k$ is given.\n",
    "Fill in the blanks in the following code to complete `reshape_rho`, which enables the simultaneous sampling of $x_{i}[k]$ when only the spectral radius $\\rho_i$ is changed for the same input $u[k]$ and $W^\\mathrm{rec}$.\n",
    "\n",
    "[END]: #\n",
    "\n",
    "- `reshape_rho`\n",
    "  - Argument(s):\n",
    "    - `rho`: `np.ndarray`\n",
    "      - `shape`: `(k,)`\n",
    "  - Return(s):\n",
    "    - `rho_new`: `np.ndarray`\n",
    "      - `shape`: `(k, 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_rho(rho):\n",
    "    rho_new = rho[:, None]  # RIGHT\n",
    "    return rho_new\n",
    "\n",
    "\n",
    "test_func(reshape_rho, \"03_02\")\n",
    "# show_solution(\"03_02\", \"reshape_rho\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "スペクトル半径 $\\rho$ の効果を直感的に理解するため、$u[k]=0, f=\\mathrm{id}_\\mathbb{R}$ の無入力線形ESNのケースを考えます。\n",
    "この際、ESNの時間発展は以下の式で表現されます。\n",
    "\n",
    "[en]: #\n",
    "To intuitively understand the effect of the spectral radius $\\rho$, let's consider the case of a linear ESN with no input (i.e., $u[k]=0, f=\\mathrm{id}_\\mathbb{R}$).\n",
    "In this case, the temporal evolution of the ESN can be represented by the following equation:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "x[t+1] = \\rho W^\\mathrm{rec} x[t]\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "この際、$x[k]$の解析解は以下の式で表されます。\n",
    "\n",
    "[en]: #\n",
    "where the analytical solution for $x[k]$ is given as follows:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "x[k] = \\rho^k \\left({W^\\mathrm{rec}}\\right)^k x[0]\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "$W^\\mathrm{rec}$ は正規化されているため、$\\left({W^\\mathrm{rec}}\\right)^k$ はそのものは発散しません。\n",
    "一方で $\\rho^k$ は $\\rho > 1$ の時無限大に発散、$\\rho < 1$ の時収束します。\n",
    "以下のセルは $\\rho$ の違いが与える $x[k]$ の応答を図示化します (比較のために $\\tanh$ を活性化関数に有する非線形ESNのケースを右側に描画しています)\n",
    "\n",
    "[en]: #\n",
    "Because $W^\\mathrm{rec}$ is normalized, $\\left({W^\\mathrm{rec}}\\right)^k$ itself does not diverge.\n",
    "However, $\\rho^k$ diverges to infinity when $\\rho > 1$ and converges when $\\rho < 1$.\n",
    "The following cell visualizes the response of $x[k]$ according to differences in $\\rho$.\n",
    "For comparison, the case of nonlinear ESN with $\\tanh$ is shown on the right side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_setup, seed_dataset = 1234, 5678\n",
    "dim, rhos = 50, np.array([0.99, 1.0, 1.01])\n",
    "dataset_info = dict(t_washout=300, t_train=0, t_eval=0)\n",
    "\n",
    "w_in, net, _w_out = create_setup(seed_setup, dim, reshape_rho(rhos))  # Use `reshape_rho`.\n",
    "ts, us, _ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "vs = np.zeros_like(us)  # Zero input\n",
    "x0 = net.rnd.uniform(low=-0.5, high=0.5, size=(1, dim))\n",
    "x0 = np.broadcast_to(x0, (len(rhos), dim))  # Using the same initial conditions\n",
    "\n",
    "xs_nlin = sample_dynamics(x0, w_in, net, ts, vs)\n",
    "net.f = lambda val: val  # Identity function\n",
    "xs_lin = sample_dynamics(x0, w_in, net, ts, vs)\n",
    "\n",
    "fig, ax = plt.subplots(len(rhos), 2, figsize=(12, 10), gridspec_kw=dict(hspace=0.05, wspace=0.2))\n",
    "for idx, rho in enumerate(rhos):\n",
    "    ax[idx, 0].set_ylabel(r\"$\\rho = {:.2f}$\".format(rho))\n",
    "    for idy, xs in enumerate([xs_lin, xs_nlin]):\n",
    "        ax[idx, idy].plot(xs[idx], lw=1.0)\n",
    "        ax[idx, idy].axhline(1.0, ls=\"--\", color=\"k\", lw=1.0)\n",
    "        ax[idx, idy].axhline(-1.0, ls=\"--\", color=\"k\", lw=1.0)\n",
    "        ax[idx, idy].set_yticks([-1.0, 1.0])\n",
    "        if idx < len(rhos) - 1:\n",
    "            ax[idx, idy].set_xticklabels([])\n",
    "ax[-1, 0].set_xlabel(\"time steps\")\n",
    "ax[-1, 1].set_xlabel(\"time steps\")\n",
    "ax[0, 0].set_title(r\"$f=\\mathrm{id}_\\mathbb{R}$\")\n",
    "ax[0, 1].set_title(r\"$f=\\tanh$\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "まず同じ初期値 $x[k]$ にもかかわらず、 $\\rho = 1.01$ で $x[k]$ の値の範囲が $[-1, 1]$ を大きく逸脱しています。\n",
    "また非線形ESNの場合、 $\\rho\\geq 1$ でも $x[k]$ は $\\tanh$ により $[-1, 1]$ の範囲に収まり無限に発散する状況は発生しませんが、$\\rho<1$ のケースのように0に収束しません。\n",
    "この際、ESPは成立せず、$x[0]$ の影響が残り続けます。\n",
    "後の章でも扱いますが、$\\rho$ を大きくすると非周期的で初期値鋭敏性を有する**カオス**が生じる場合があります。\n",
    "\n",
    "[en]: #\n",
    "First, despite having the same initial value $x[0]$, the range of $x[k]$ for $\\rho = 1.01$ deviates significantly from $[-1, 1]$.\n",
    "In the case of a nonlinear ESN, $x[k]$ remains within the range $[-1, 1]$ even when $\\rho \\geq 1$ because the $\\tanh$ function prevents it from diverging to infinity.\n",
    "However, it does not converge to zero as it does when $\\rho < 1$.\n",
    "Here, the ESP does not hold, and the influence of $x[0]$ continues to persist.\n",
    "As will be discussed in later sections, increasing $\\rho$ can lead to *chaos*, a phenomenon characterized by aperiodicity and sensitivity to initial conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "したがってNARMA10等の通常の用途では $\\rho < 1$ に限定して探索される場合が多いです。\n",
    "再び入力ありのケースを考えましょう。\n",
    "今簡単のため入力スケール$(\\sigma, \\phi)=(1, 0)$、$(f, W^\\mathrm{rec})=(\\mathrm{id}_\\mathrm{R}, I) $となる線形ESNを考えます。\n",
    "この際、状態の時間発展は以下の式で表現されます。\n",
    "\n",
    "[en]: #\n",
    "Therefore, for typical use cases such as NARMA10, the parameter search for $\\rho$ is often restricted to $\\rho < 1$.\n",
    "Now, let's return to considering the case with input.\n",
    "For simplicity, consider a linear ESN with input scaling $(\\sigma, \\phi)=(1, 0)$ and other settings $(f, W^\\mathrm{rec})=(\\mathrm{id}_\\mathrm{R}, I)$.\n",
    "In this case, the temporal evolution of the state is expressed by the following equation:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "x[k+1] = \\rho x[k] + W^\\mathrm{in} u[k+1]\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "$x[k]$の解析解は以下の式で表現されます。\n",
    "\n",
    "[en]: #\n",
    "where the analytical solution for $x[k]$ can be expressed as follows:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "x[k] = W^\\mathrm{in} \\sum_{j=0}^\\infty \\rho^{j}  u[k-j]\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "式から推測されるとおり、 $\\rho$ は内部状態 $x[k]$ をどれほど保持するか制御するパラメータで $\\rho$ が 1に近いほど 過去の入力を保持しやすくなります。\n",
    "この効果を確かめるため、NARMA10精度の $\\rho$ 依存性を調べてみましょう。\n",
    "次のコードは $\\rho$ を0.02刻みで 0から1まで変化させたときの NARMA10タスクの NRMSEと $x[k]$ の階数を描画します。\n",
    "\n",
    "[en]: #\n",
    "As inferred from the equation, $\\rho$ is a parameter that controls how long the reservoir holds internal states $x[k]$.\n",
    "The closer $\\rho$ is to 1, the more it retains past inputs.\n",
    "To check this effect, let's now investigate the dependency of NARMA10 accuracy on $\\rho$.\n",
    "The following code plots the NRMSE of the NARMA10 task and the rank of $x[k]$ as $\\rho$ changes from 0 to 1 in increments of 0.02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_setup, seed_dataset = 1234, 5678\n",
    "sigma, phi = 0.05, 0.05  # vs \\in [0.0, 0.1]\n",
    "dims = [25, 50, 100]\n",
    "rhos = np.linspace(0.0, 1.0, 51)[1:-1]  # 0.02, ..., 0.98\n",
    "rhos_batch = reshape_rho(rhos)  # use `reshape_rhos`.\n",
    "dataset_info = dict(t_washout=100, t_train=2000, t_eval=1000)\n",
    "\n",
    "ts, us, ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "vs = sigma * us + phi\n",
    "\n",
    "nrmses, best_ids, ranks = [], [], []\n",
    "for dim in tqdm(dims):\n",
    "    w_in, net, w_out = create_setup(seed_setup, dim, rhos_batch, f=np.tanh)\n",
    "    x0 = np.zeros((rhos.shape[0], net.dim))\n",
    "    nrmse, xs = train_and_eval(x0, w_in, net, w_out, ts, vs, ys, time_info)\n",
    "    nrmses.append(nrmse[:, 0])\n",
    "    best_ids.append(np.argmin(nrmse[:, 0]))\n",
    "    xs_m = xs - xs.mean(axis=-2, keepdims=True)\n",
    "    ranks.append(np.linalg.matrix_rank(xs_m.swapaxes(-2, -1) @ xs_m))\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8, 8), gridspec_kw=dict(hspace=0.05))\n",
    "for dim, nrmse, best_id, rank in zip(dims, nrmses, best_ids, ranks, strict=False):\n",
    "    ax[0].plot(rhos, rank / dim, label=r\"$N={}$\".format(dim))\n",
    "    ax[1].plot(rhos, nrmse, label=r\"$N={}$\".format(dim))\n",
    "    ax[1].scatter(rhos[best_id], nrmse[best_id], s=100.0, marker=\"*\")\n",
    "ax[0].set_xticklabels([])\n",
    "ax[0].set_ylabel(r\"$r/N$\")\n",
    "ax[1].set_xlabel(r\"$\\rho$\")\n",
    "ax[1].set_ylabel(r\"NRMSE (best: $\\bigstar$)\")\n",
    "ax[1].legend(frameon=False)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "NARMA10は10次程度の過去入力の成分を有するため、比較的 $\\rho$ が $1$ に近く、長く入力情報を保持できる領域が最適であるとわかります。\n",
    "\n",
    "[en]: #\n",
    "Because NARMA10 requires past input components with order of about 10, the optimal range of $\\rho$ is where it is relatively close to 1, allowing the reservoir to keep past inputs for a longer period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "#### 漏れ率\n",
    "\n",
    "[en]: #\n",
    "#### Leaky rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "その他に重要なESNのパラメータとして $a$ が挙げられます。\n",
    "スペクトル半径 $\\rho$ と同様に過去状態をどの程度保持するかを制御するパラメータで $[0, 1]$の範囲の値を取ります。\n",
    "特に $a=1$ のケースでは線形項のない離散ESNと、 $a=0$ では定数 $x[k]=\\mathrm{const.}$ と等価になります。\n",
    "\n",
    "下記のような連続時間上で定義される連続ESNとその近似の文脈で漏れ率 $a$ はしばしば登場します。\n",
    "\n",
    "[en]: #\n",
    "Another important parameter in ESNs is $a$, a parameter that controls the extent to which past states are retained (similar to the spectral radius).\n",
    "It takes values in the range $[0, 1]$.\n",
    "In particular, the case of $a=1$ is equivalent to a discrete ESN without the linear term, and the case of $a=0$ is equivalent to a constant $x[k]=\\mathrm{const.}$\n",
    "\n",
    "The leaky rate $a$ often appears in the context of continuous ESNs defined in continuous time and their approximations, as shown below:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\tau \\dfrac{dx}{dt}(t) = -x(t) + \\tanh\\left(\\rho W^\\mathrm{rec} x(t) + W^\\mathrm{in} v(t)\\right)\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "ここで $\\tau \\in \\mathbb{R}$ は時定数です。\n",
    "この式をオイラー法により離散方程式として以下の近似式で表現されます。\n",
    "\n",
    "[en]: #\n",
    "where $\\tau \\in \\mathbb{R}$ is a time constant.\n",
    "By approximating this equation using the Euler method, it can now be expressed as the following discrete equation.\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "x(t+\\Delta t) = \\left(1-\\dfrac{\\Delta t}{\\tau}\\right) x(t) + \\dfrac{\\Delta t}{\\tau}\\tanh\\left(\\rho W^\\mathrm{rec} x(t) + W^\\mathrm{in} v(t + \\Delta t)\\right)\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "$t := k \\Delta t, a=\\Delta t / \\tau$ により冒頭の式と等価になります。\n",
    "上記の連続ESNの文脈では $a=0.1$ がよく見られますが、もちろんその限りではありません。\n",
    "$\\rho$ 同様に様々な $a$ に対して一度に $x[k]$ をサンプリングする`reshape_lr`を以下の設問で実装しましょう。\n",
    "\n",
    "[en]: #\n",
    "With $t := k \\Delta t, a=\\Delta t / \\tau$, this equation becomes equivalent to the original equation.\n",
    "In the context of continuous ESNs, $a=0.1$ is commonly seen, but is not necessarily limited to this value.\n",
    "Similar to $\\rho$, let's implement the function `reshape_lr` that allows the simultaneous sampling of $x[k]$ for multiple values of $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.3.\n",
    "\n",
    "[ja]: #\n",
    "長さ$k$ の配列として漏れ率 $A = \\left(a_0, a_1,~\\ldots,~a_{k-1} \\right) $ が与えられる。\n",
    "同じ入力 $u[k]$ と $W^\\mathrm{rec}$ に対して漏れ率のみを $a_{i}$ に変えたときのESNの時系列 $x_{i}[k]$ をまとめてサンプリングできるように、`reshape_lr`を完成させよ。\n",
    "\n",
    "[en]: #\n",
    "An array of leaky rates $A = \\left(a_0, a_1,~\\ldots,~a_{k-1} \\right) $ with length $k$ is given.\n",
    "Fill in the blanks in the following code to complete `reshape_lr`, which enables the simultaneous sampling of $x_{i}[k]$ when only the leaky rate $a_{i}$ is changed for the same input $u[k]$ and $W^\\mathrm{rec}$.\n",
    "[END]: #\n",
    "\n",
    "- `reshape_lr`\n",
    "  - Argument(s):\n",
    "    - `lr`: `np.ndarray`\n",
    "      - `shape`: `(k,)`\n",
    "  - Return(s):\n",
    "    - `lr_new`: `np.ndarray`\n",
    "      - `shape`: `(k, 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_lr(lr):\n",
    "    lr_new = lr[:, None]  # RIGHT\n",
    "    return lr_new\n",
    "\n",
    "\n",
    "test_func(reshape_lr, \"03_03\")\n",
    "# show_solution(\"03_03\", \"reshape_lr\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "次のコードは $a$ を $10^{-2}$ から $1$ (離散ESN) まで変化させたときの NARMA10タスクの NRMSEを描画します。\n",
    "スペクトル半径を $\\rho=0.5$ に設定し、内部結合がもたらす記憶特性への寄与を小さくしています。\n",
    "\n",
    "[en]: #\n",
    "The next code plots the NRMSE of NARMA10 tasks when $a$ is changed from $10^{-2}$ to $1$ (discrete ESN).\n",
    "Note that the spectral radius is set to $\\rho=0.5$, to reduce the contribution of internal connections on the reservoir's memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_setup, seed_dataset = 1234, 5678\n",
    "sigma, phi = 0.05, 0.05  # vs \\in [0.0, 0.1]\n",
    "dims = [25, 50, 100]\n",
    "rho = 0.5\n",
    "leaky_rates = np.logspace(-1, 0, 21)\n",
    "lr_batch = reshape_lr(leaky_rates)  # Use `reshape_lr`.\n",
    "dataset_info = dict(t_washout=100, t_train=2000, t_eval=1000)\n",
    "\n",
    "ts, us, ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "vs = sigma * us + phi\n",
    "\n",
    "nrmses, best_ids = [], []\n",
    "for dim in tqdm(dims):\n",
    "    w_in, net, w_out = create_setup(seed_setup, dim, rho, f=np.tanh, a=lr_batch)\n",
    "    x0 = np.zeros((leaky_rates.shape[0], net.dim))\n",
    "    nrmse, xs = train_and_eval(x0, w_in, net, w_out, ts, vs, ys, time_info)\n",
    "    nrmses.append(nrmse[:, 0])\n",
    "    best_ids.append(np.argmin(nrmse[:, 0]))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "for dim, nrmse, best_id in zip(dims, nrmses, best_ids, strict=False):\n",
    "    ax.plot(leaky_rates, nrmse, label=r\"$N={}$\".format(dim))\n",
    "    ax.scatter(leaky_rates[best_id], nrmse[best_id], s=100.0, marker=\"*\")\n",
    "ax.set_xlabel(r\"$a$\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_ylabel(r\"NRMSE (best: $\\bigstar$)\")\n",
    "ax.legend(frameon=False)\n",
    "ax.grid(True, which=\"minor\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.4. (Advanced)\n",
    "\n",
    "[ja]: #\n",
    "- NARMA10よりも時定数の短いタスク (NARMA2)を試し、$\\rho$ や $a$ 依存性がどのように変化するか確かめよ。\n",
    "- オイラー法は[1次のルンゲ・クッタ (Runge-Kutta; RK)法](https://en.wikipedia.org/wiki/List_of_Runge%E2%80%93Kutta_methods) として一般化される。上記の連続時間上のESNの微分方程式に対して、より高次なRK法 (例えばRK4) を適用し離散化したESNを実装せよ。\n",
    "\n",
    "[en]: #\n",
    "- Try experiments with tasks requiring shorter time constants (such as NARMA10), and observe how the dependency on $\\rho$ and $a$ changes.\n",
    "- Euler's method can be generalized as a [1st-order Runge-Kutta (RK) method](https://en.wikipedia.org/wiki/List_of_Runge%E2%80%93Kutta_methods). Apply higher-order RK methods (e.g., RK4) to the differential equation of ESN in continuous time to implement a discretized ESN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "### 4. データセットのサイズ\n",
    "\n",
    "[en]: #\n",
    "### 4. Dataset size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "データセットのサイズ $T_\\textrm{train}$ の影響を確認しましょう。\n",
    "一般に学習データの時系列が長いほどより信頼性の高い評価が可能になります。\n",
    "以下のコードは $(\\sigma, \\phi, N, \\rho) = (0.05, 0.05, 50, 0.9)$ に対して  $T_\\textrm{train}= (100,200,400,800,1600,3200)$ と変化させたときのNARMA10のNRMSEの分布 (デフォルトでは100個の $(W^\\mathrm{in}, W^\\mathrm{rec})$ をランダムに生成) を描画します (完了まで数分程度かかる場合があります。あまりに時間がかかる場合は`sample_num`や`dim`を小さくしてください)。\n",
    "\n",
    "[en]: #\n",
    "Let's check the effect of the dataset size $T_\\textrm{train}$.\n",
    "In general, more accurate evaluations become possible as the time series for training data gets longer.\n",
    "The following code plots the NRMSE of NARMA10 tasks for different lengths of training data, $T_\\textrm{train}= (100, 200, 400, 800, 1600, 3200)$.\n",
    "Other parameters are set to $(\\sigma, \\phi, N, \\rho) = (0.05, 0.05, 50, 0.9)$, and 100 samples of $(W^\\mathrm{in}, W^\\mathrm{rec})$ are generated randomly and evaluated by default.\n",
    "It may take a few minutes to complete.\n",
    "If it takes too long, try reducing the value of `sample_num` or `dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = 100\n",
    "seed_setup, seed_dataset = 1234, 5678\n",
    "t_trains = [100, 200, 400, 800, 1600, 3200]\n",
    "sigma, phi = 0.05, 0.05  # vs \\in [0.0, 0.1]\n",
    "dim, rho = 50, 0.9\n",
    "dataset_info = dict(t_washout=100, t_train=max(t_trains), t_eval=1000)\n",
    "\n",
    "ts, us, ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "w_in_ws, net_ws = [], []\n",
    "for seed in trange(sample_num):  # Create setups for `sample_num` times.\n",
    "    w_in, net, _w_out = create_setup(seed_setup + seed, dim, rho, f=np.tanh)\n",
    "    w_in_ws.append(w_in.weight)\n",
    "    net_ws.append(net.weight)\n",
    "\n",
    "w_in_batch = Linear(1, dim)\n",
    "w_in_batch.weight = np.array(w_in_ws)[:, None, :, :]  # [*bs, 1, N, 1] ([1] -> [*bs, 1, N])\n",
    "net_batch = ESN(dim, sr=rho)\n",
    "net_batch.weight = np.array(net_ws)  # [bs, N, N] ([*bs, 1, N] -> [*bs, 1, N])\n",
    "w_out = BatchLRReadout(dim, 1)\n",
    "\n",
    "vs = sigma * us + phi\n",
    "x0 = np.zeros((sample_num, 1, net.dim))\n",
    "xs = sample_dynamics(x0, w_in_batch, net_batch, ts, vs, display=True)\n",
    "nrmse_dict = {}\n",
    "for t_train in t_trains:\n",
    "    time_info[\"t_washout\"] = dataset_info[\"t_washout\"] + max(t_trains) - t_train\n",
    "    nrmse = eval_nrmse(xs, ys, w_out, time_info=time_info)[:, 0, 0]\n",
    "    nrmse_dict[t_train] = nrmse\n",
    "    print(\"t_train={}: {:.3e}±{:.3e} (#sample={})\".format(t_train, nrmse.mean(), nrmse.std(), nrmse.size))\n",
    "\n",
    "nrmse_aves = [nrmse.mean() for nrmse in nrmse_dict.values()]\n",
    "nrmse_stds = [nrmse.std() for nrmse in nrmse_dict.values()]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.errorbar(t_trains, nrmse_aves, nrmse_stds)\n",
    "ax.set_xlabel(r\"$T_\\mathrm{train}$\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_ylabel(\"NRMSE\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.grid(True, which=\"both\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "$T_\\mathrm{train}$ が小さい時、学習精度が悪くかつその分散が大きく不安定です。\n",
    "これは最大ランクとなる $N \\geq T_\\mathrm{train}$ の条件**のみ**では不十分で、より長い時間のサンプルの必要性を示唆しています。\n",
    "実際 $T_\\mathrm{train}$ は $N$ よりずっと大きい値が望ましく、経験的には10倍以上の長さがしばしば採用されます。\n",
    "\n",
    "[en]: #\n",
    "When $T_\\mathrm{train}$ is too small, the performance is poor and unstable, with a large variance.\n",
    "This suggests that the condition $N \\geq T_\\mathrm{train}$ for achieving the maximum rank is not sufficient, and that longer time samples are **also** needed.\n",
    "In practice, it is desirable for $T_\\mathrm{train}$ to be much larger than $N$.\n",
    "Empirically, a length more than 10 times $N$ is often used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4.1. (Advanced)\n",
    "\n",
    "[ja]: #\n",
    "- $N$ を大きくして同様に $T_\\mathrm{train}$ とNRMSEの関係を調べよ。\n",
    "- 他のタスクに関しても同様に調査せよ。\n",
    "\n",
    "[en]: #\n",
    "- Increase $N$, and investigate the relationship between $T_\\mathrm{train}$ and the NRMSE in a similar way.\n",
    "- Conduct similar investigations for other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "### 5. 時系列の長さが短いときの対処法\n",
    "\n",
    "[en]: #\n",
    "### 5. Addressing short time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "> *\"I remember my friend Johnny von Neumann used to say, with four parameters I can fit an elephant, and with five I can make him wiggle his trunk.\"*,\n",
    "> by [Enrico Fermi, 1953](https://en.wikipedia.org/wiki/Von_Neumann%27s_elephant)\n",
    ">\n",
    "> *ジョニーは言っていたよ、『パラメータが4つあったら象だって描けるし、5つあればその鼻さえ動かせる』と。* (エンリコ・フェルミ 1953)\n",
    "\n",
    "前節では $T_\\mathrm{train}$ の大きさの重要性を示しました。\n",
    "しかしながら現実にはリザバーの次元数 $N$ に対して十分なサイズの学習データを確保できない状況にしばしば遭遇します。\n",
    "典型的には物理リザバーの研究のように、物性固有の時定数やセンサーのサンプリング周波数によって十分大きな $T_\\mathrm{train}$ を確保できない状況が考えられます。\n",
    "逆にあまりに高次元な設定、すなわち $N$ が大きすぎて要求される $T_\\mathrm{train}$ が大きいが、その計算や保存が現実的ではない場合もありえます。\n",
    "\n",
    "このような状況で注意しなければならないのは **過学習 (overfitting)** と呼ばれる現象の発生です。\n",
    "すなわち学習データに過適合しすぎて、評価データに対する汎化性能が低い状況を指します。\n",
    "ここではそのような過学習を避けるテクニックとして **リッジ回帰** (ridge regression) ならびに **赤池情報量規準** (Akaike Information Criteria; AIC) を用いたリッジ回帰のパラメータの自動調整を紹介します。\n",
    "\n",
    "[en]: #\n",
    "> *\"I remember my friend Johnny von Neumann used to say, with four parameters I can fit an elephant, and with five I can make him wiggle his trunk.\"*\n",
    "> — [Enrico Fermi, 1953](https://en.wikipedia.org/wiki/Von_Neumann%27s_elephant)\n",
    "\n",
    "In the previous section, we highlighted the importance of large $T_\\mathrm{train}$.\n",
    "However, in reality, it can be difficult to obtain training data with sufficient length relative to the reservoir dimensionality $N$.\n",
    "This typically occurs in research on physical reservoirs, where the intrinsic time constants of physical systems or the sampling frequency of sensors limit the size of $T_\\mathrm{train}$.\n",
    "Conversely, it is also possible to encounter very high-dimensional settings where $N$ is so large that the required $T_\\mathrm{train}$ becomes impractically large, making computation or data storage infeasible.\n",
    "\n",
    "In such situations, it is crucial to avoid the phenomenon called **overfitting**.\n",
    "This refers to the situation where the model fits the training data too closely, resulting in poor generalization performance on evaluation (validation) data.\n",
    "Here, we introduce some techniques to avoid overfitting: **ridge regression** and the automatic adjustment of the ridge parameter using **Akaike information criteria (AIC)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "#### リッジ回帰\n",
    "\n",
    "[en]: #\n",
    "#### Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "リッジ回帰は最小二乗問題におけるコスト関数 $\\mathrm{RSS}(X, Y, w):=\\|Xw - Y\\|^2$ に正則化項を追加して、パラメータ $w$ の大きさ ( $L^2$ ノルム) に制限を加える手法です。\n",
    "リッジパラメータ $\\lambda(>0)$ を用いてそのコスト関数 $\\mathcal{L}^\\mathrm{ridge}$ は以下の式で定義されます。\n",
    "\n",
    "[en]: #\n",
    "Ridge regression is a method that adds a regularization term to the cost function $\\mathrm{RSS}(X, Y, w):=\\|Xw - Y\\|^2$ of the least squares problem, imposing a constraint on the size ($L^2$ norm) of the weight parameter $w$.\n",
    "With the ridge parameter $\\lambda(>0)$, the new cost function $\\mathcal{L}^\\mathrm{ridge}$ is defined by the following equation:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}^\\mathrm{ridge}(X, Y, w, \\lambda) & := \\mathrm{RSS}(X, Y, w) + \\lambda \\|w\\|^2\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "ここでこのコスト関数を最小化する $\\hat{w}^\\mathrm{ridge}$ は以下の式より線形回帰同様にワンショット (one-shot) で求まります。\n",
    "\n",
    "[en]: #\n",
    "where the optimal $\\hat{w}^\\mathrm{ridge}$ that minimizes this cost function can be obtained in one-shot (similar to linear regression) using the following equation:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{w}^\\mathrm{ridge}(X, Y, \\lambda):&=\\mathrm{arg}\\min_{w} \\mathcal{L}^\\mathrm{ridge}(X, Y, w, \\lambda) \\\\\n",
    "&=(X^\\top X + \\lambda I)^{-1}X^\\top Y\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "グラム行列 $X^\\top X$ は半正定値行列 (すべての固有値が0以上) なので $X^\\top X + \\lambda I$ は正定値行列で常に逆行列が存在します。\n",
    "したがって通常の線型回帰よりもリッジ回帰は数値的に安定といえます。\n",
    "\n",
    "[en]: #\n",
    "The Gram matrix $X^\\top X$ is a positive semi-definite matrix (all eigenvalues are non-negative), so $X^\\top X + \\lambda I$ is a positive definite matrix and always has an inverse.\n",
    "Therefore, ridge regression is numerically more stable than ordinary linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5.1.\n",
    "\n",
    "[ja]: #\n",
    "`Linear`を継承し、バイアス項を加えた予測変数 $\\tilde{X}=[1 : X]\\in\\mathbb{R}^{...\\times T\\times (N+1)}$、予測対象変数 $Y\\in \\mathbb{R}^{... \\times T \\times D}$ ならびにリッジパラメータ$\\lambda \\in \\mathbb{R}^{+}$ に対して、 $\\mathcal{L}^\\mathrm{ridge}(\\tilde{X}, Y, w)$ を最小化する $\\hat{w}^\\mathrm{ridge}\\in\\mathbb{R}^{...\\times(N+1)\\times D}$ を計算し、その重みとバイアスを更新する `RidgeReadout` を完成させよ。\n",
    "なお`BatchLRReadout`の実装を参考にせよ。\n",
    "\n",
    "[en]: #\n",
    "Fill in the blanks of the following code to complete `RidgeReadout`, inheriting from `Linear`.\n",
    "This class takes the predictor sequence $\\tilde{X}=[1 : X]\\in\\mathbb{R}^{...\\times T\\times (N+1)}$ (with additional bias term), target sequence $Y\\in \\mathbb{R}^{... \\times T \\times D}$, and ridge parameter $\\lambda \\in \\mathbb{R}^{+}$ as arguments.\n",
    "It calculates $\\hat{w}^\\mathrm{ridge}\\in\\mathbb{R}^{...\\times(N+1)\\times D}$ that minimizes  $\\mathcal{L}^\\mathrm{ridge}(\\tilde{X}, Y, w)$, and then updates its weights and biases.\n",
    "Refer to the implementation of `BatchLRReadout` as well.\n",
    "\n",
    "[END]: #\n",
    "\n",
    "- `RidgeReadout.train`\n",
    "  - Argument(s):\n",
    "    - `x`: `np.ndarray`\n",
    "      - `shape`: `(..., time_steps, input_dim)`\n",
    "    - `y`: `np.ndarray`\n",
    "      - `shape`: `(..., time_steps, output_dim)`\n",
    "  - Returns(s):\n",
    "    - `self.weight`: `np.ndarray`\n",
    "      - `shape`: `(..., output_dim, input_dim)`\n",
    "    - `self.bias`: `np.ndarray`\n",
    "      - `shape`: `(..., 1, output_dim)`\n",
    "\n",
    "[ja]: #\n",
    "  - Operation(s):\n",
    "      - `self.weight`の更新\n",
    "      - `self.bias`の更新\n",
    "\n",
    "[en]: #\n",
    "  - Operation(s):\n",
    "      - Update `self.weight` with the obtained weight.\n",
    "      - Update `self.bias` with the obtained bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeReadout(Linear):\n",
    "    def __init__(self, *args, lmbd: float = 0.0, **kwargs):\n",
    "        super(RidgeReadout, self).__init__(*args, **kwargs)\n",
    "        self.lmbd = lmbd\n",
    "\n",
    "    def train(self, x: np.ndarray, y: np.ndarray):\n",
    "        assert (x.ndim > 1) and (x.shape[-1] == self.input_dim)\n",
    "        assert (y.ndim > 1) and (y.shape[-1] == self.output_dim)\n",
    "        x_biased = np.ones((*x.shape[:-1], x.shape[-1] + 1), dtype=self.dtype)\n",
    "        x_biased[..., 1:] = x\n",
    "        # RIGHT_B Implement ridge regression to obtain `self.weight` and `self.bias`.\n",
    "        xtx = x_biased.swapaxes(-2, -1) @ x_biased\n",
    "        xty = x_biased.swapaxes(-2, -1) @ y\n",
    "        sol = np.matmul(np.linalg.pinv(xtx + self.lmbd * np.eye(x.shape[-1] + 1)), xty)\n",
    "        self.weight = sol[..., 1:, :].swapaxes(-2, -1)\n",
    "        self.bias = sol[..., :1, :]\n",
    "        # RIGHT_E\n",
    "        return self.weight, self.bias\n",
    "\n",
    "\n",
    "def solution(dim_in, dim_out, x_train, y_train, x_eval, lmbd):\n",
    "    # DO NOT CHANGE HERE.\n",
    "    readout = RidgeReadout(dim_in, dim_out, lmbd=lmbd)\n",
    "    readout.train(x_train, y_train)\n",
    "    return readout(x_eval)\n",
    "\n",
    "\n",
    "test_func(solution, \"05_01\")\n",
    "# show_solution(\"05_01\", \"RidgeReadout\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "次のコードは先程の $T_\\mathrm{train}$ を変更するコードで `BatchLRReadout` を `RidgeReadout` に置き換えてNRMSEを描画するコードです。\n",
    "$\\lambda = 0$ の代入でリッジ回帰が線型回帰と等価になる点に注意ください。\n",
    "\n",
    "[en]: #\n",
    "The next cell includes the previous code that plots NRMSE for various values of $T_\\mathrm{train}$, but here, `BatchLRReadout` is replaced with `RidgeReadout`.\n",
    "Note that substituting $\\lambda = 0$ makes the ridge regression equivalent to linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = 100\n",
    "seed_setup, seed_dataset = 1234, 5678\n",
    "t_trains = [50, 100, 200, 400, 800, 1600, 3200]\n",
    "ridge_parameters = [0.0, 1.0, 1e-2, 1e-4, 1e-6, 1e-8]\n",
    "sigma, phi = 0.05, 0.05  # vs \\in [0.0, 0.1]\n",
    "dim, rho = 50, 0.9\n",
    "dataset_info = dict(t_washout=100, t_train=max(t_trains), t_eval=1000)\n",
    "\n",
    "ts, us, ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "w_in_ws, net_ws = [], []\n",
    "for seed in trange(sample_num):  # Create setups for `sample_num` times.\n",
    "    w_in, net, _w_out = create_setup(seed_setup + seed, dim, rho, f=np.tanh)\n",
    "    w_in_ws.append(w_in.weight)\n",
    "    net_ws.append(net.weight)\n",
    "\n",
    "w_in_batch = Linear(1, dim)\n",
    "w_in_batch.weight = np.array(w_in_ws)[:, None, :, :]  # [bs, 1, N, 1] (-> [bs, 1, N])\n",
    "net_batch = ESN(dim, sr=rho)\n",
    "net_batch.weight = np.array(net_ws)  # [bs, N, N] ([bs, 1, N] -> [bs, 1, N])\n",
    "w_out = RidgeReadout(dim, 1)  # Use ridge regression.\n",
    "\n",
    "vs = sigma * us + phi\n",
    "x0 = np.zeros((sample_num, 1, net.dim))\n",
    "xs = sample_dynamics(x0, w_in_batch, net_batch, ts, vs, display=True)\n",
    "nrmse_dict = {lmbd: {} for lmbd in ridge_parameters}\n",
    "for lmbd, t_train in tqdm(list(itertools.product(ridge_parameters, t_trains))):\n",
    "    time_info[\"t_washout\"] = dataset_info[\"t_washout\"] + max(t_trains) - t_train\n",
    "    w_out.lmbd = lmbd\n",
    "    nrmse = eval_nrmse(xs, ys, w_out, time_info=time_info)[:, 0, 0]\n",
    "    nrmse_dict[lmbd][t_train] = nrmse\n",
    "    # print('λ={:.2e}, t_train={}: {:.3e}±{:.3e} (#sample={})'.format(\n",
    "    #     lmbd, t_train, nrmse.mean(), nrmse.std(), nrmse.size))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "for lmbd in ridge_parameters:\n",
    "    nrmse_aves = [nrmse.mean() for nrmse in nrmse_dict[lmbd].values()]\n",
    "    nrmse_stds = [nrmse.std() for nrmse in nrmse_dict[lmbd].values()]\n",
    "    if lmbd == 0.0:\n",
    "        plot_kws = dict(label=r\"LR ($\\lambda=0$)\", color=\"k\", ls=\":\")\n",
    "    else:\n",
    "        plot_kws = dict(label=r\"Ridge ($\\lambda=10^{{{:.0f}}}$)\".format(np.log10(lmbd)))\n",
    "    ax.errorbar(t_trains, nrmse_aves, nrmse_stds, **plot_kws)\n",
    "ax.set_xlabel(r\"$T_\\mathrm{train}$\")\n",
    "ax.set_ylabel(\"NRMSE\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend(loc=\"upper right\", borderaxespad=0, ncol=1, frameon=False)\n",
    "ax.grid(True, which=\"both\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "特に $T_\\mathrm{train} \\approx N $ 周辺で 線型回帰が不安定になるのに対し、Ridge ( $\\lambda=10^{-4}, 10^{-6}$ )における精度は安定しています。\n",
    "また $T_\\mathrm{train} \\gg N$ でも 線型回帰とほぼ同等の精度と安定性を有します。\n",
    "一方で $\\lambda$ が大きい ( $\\lambda=10^{0}, 10^{-2}$ ) と精度は安定するものの、線型回帰に大きく劣る結果になっています。\n",
    "この現象は過学習の対義語で**学習不足 (underfitting)** と呼ばれます。\n",
    "逆に $\\lambda$ を小さくしすぎる ( $\\lambda=10^{-8}$ ) と線形回帰同様の不安定性、すなわち過学習が $T_\\mathrm{train} \\approx N$ で発生します。\n",
    "\n",
    "このように リッジパラメータ $\\lambda$ の適切な選択は、学習の精度や安定性の意味でとても重要です。\n",
    "\n",
    "[en]: #\n",
    "Linear regression becomes unstable around $T_\\mathrm{train} \\approx N$, but the accuracy of ridge regression ($\\lambda=10^{-4}, 10^{-6}$) remains stable.\n",
    "Furthermore, even when $T_\\mathrm{train} \\gg N$, ridge regression achieves almost the same accuracy and stability as linear regression.\n",
    "On the other hand, when $\\lambda$ is large ($\\lambda=10^{0}, 10^{-2}$), the accuracy remains stable but is significantly worse than linear regression.\n",
    "This phenomenon is called **underfitting**, the opposite of overfitting.\n",
    "Additionally, choosing values of $\\lambda$ that are too small ($\\lambda=10^{-8}$) causes instability to occur around $T_\\mathrm{train} \\approx N$, i.e., overfitting, similar to linear regression.\n",
    "\n",
    "Thus, the appropriate selection of the ridge parameter $\\lambda$ is very important for learning accuracy and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "#### AICを用いたリッジパラメータ $\\lambda$ の自動調整\n",
    "\n",
    "[en]: #\n",
    "#### Automatic adjustment of ridge parameter $\\lambda$ based on AIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "リッジ回帰ではリッジパラメータの導入により、$T_\\mathrm{train}$ が小さいケースでもより安定な学習が可能になりました。\n",
    "一方で先述のとおり、このリッジパラメータ $\\lambda$ はハイパーパラメータの一種であり、その調整が別途必要です。\n",
    "特に $\\lambda$ は任意の正の実数を取りうるので、その探索範囲は無限大におよびその取り扱いはかなり厄介です。\n",
    "そこでこの $\\lambda$ を自動的に決定するためにAICと呼ばれる指標を導入します。\n",
    "AICはモデルサイズと残差誤差のバランスをとり、妥当なモデルを構築するための指標 (情報量規準) で機械学習の分野で広く使用されます。\n",
    "リッジ回帰の場合は以下の式で計算されます<sup>[2]</sup>。\n",
    "\n",
    "[en]: #\n",
    "In ridge regression, introduction of the ridge parameter allows for more stable learning even in cases where $T_\\mathrm{train}$ is small.\n",
    "However, this ridge parameter $\\lambda$ is a hyperparameter and requires separate tuning.\n",
    "Specifically, since $\\lambda$ can take any positive real number, its search range is infinite, making its tuning quite troublesome.\n",
    "As a solution, we introduce an AIC to automatically determine $\\lambda$.\n",
    "AIC is a measure that balances model size and residual error, and is widely used in the field of machine learning to construct reasonable models.\n",
    "In the case of ridge regression, AIC can be calculated using the following equation:\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{AIC}(X, Y, \\lambda) :&= T \\ln\\left(\\mathrm{RSS}(X, Y, \\hat{w}^\\mathrm{ridge}(X, Y, \\lambda))\\right) + \\mathrm{df}(X, \\lambda) \\\\\n",
    "&= T\\ln\\left(\\|X\\hat{w}^\\mathrm{ridge}(X, Y, \\lambda) - Y\\|^2\\right) + \\mathrm{df}(X, \\lambda)\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "ここで$\\mathrm{df}(X, \\lambda)$はモデルの自由度を表し以下の式で計算されます。\n",
    "\n",
    "[en]: #\n",
    "where $\\mathrm{df}(X, \\lambda)$ is the effective degrees of freedom of the model, given by\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{df}(X, \\lambda) :&= \\mathrm{tr}[X(X^\\top X + \\lambda I)^{-1}X^\\top] \\\\\n",
    "&= \\sum_{i=0}^{N-1}\\frac{\\sigma_{i}^2}{\\sigma_{i}^2+\\lambda}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "$\\{\\sigma_i \\}_{i=0}^{N-1}$ は $X \\in \\mathbb{R}^{T\\times N}$ を特異値分解した際の特異値です。\n",
    "AICの最小化により妥当な $\\lambda$ を選択できます。\n",
    "一方で式より $0<\\mathrm{df} \\leq N$ となります。\n",
    "したがって $\\lambda$ の代わりに予め $\\mathrm{df}$ の候補 $\\{\\mathrm{df}_k \\}$を用意し、$\\mathrm{df}_k = \\mathrm{df}(X, \\lambda_k)$ となる $\\lambda_k$ を逆算し、$\\mathrm{AIC}(X, Y, \\lambda_k)$ が最小となる $\\lambda_k$ を選ぶ指針を取れます。\n",
    "このような手順で、無限大まで及んだ $\\lambda$ の候補を $\\mathrm{df}$ の範囲に置き換えて、探索範囲を狭められます。\n",
    "\n",
    "以下、詳細なアルゴリズムを記載します。\n",
    "\n",
    "[en]: #\n",
    "$\\{\\sigma_i \\}_{i=0}^{N-1}$ are the singular values when singular value decomposition is applied to $X \\in \\mathbb{R}^{T\\times N}$.\n",
    "By minimizing AIC, an appropriate $\\lambda$ can be obtained.\n",
    "Also, $0<\\mathrm{df} \\leq N$ can be inferred from the equation.\n",
    "\n",
    "Therefore, instead of directly searching for $\\lambda$, we can prepare a set of candidate degrees of freedom $\\{\\mathrm{df}_k \\}$, calculate $\\lambda_k$ such that $\\mathrm{df}_k = \\mathrm{df}(X, \\lambda_k)$ for each candidate of $\\mathrm{df}$, and select $\\lambda_k$ that minimizes $\\mathrm{AIC}(X, Y, \\lambda_k)$.\n",
    "This approach allows us to search within a limited range of $\\mathrm{df}$ candidates, instead of within the whole infinite range of $\\lambda$.\n",
    "\n",
    "Below, we write out the detailed algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "1. $\\{\\mathrm{df}_k\\}$ の候補を作成します。\n",
    "典型的には整数値 $\\{1,~\\ldots,~N\\}$ のみを候補として選択します。\n",
    "2. ニュートン法により$f(\\lambda_k)=0$の解 $\\lambda_k$ を求めます。\n",
    "3. $\\lambda_k$ を用いて $\\mathrm{AIC}(X, Y, \\lambda_k)$ を計算します。\n",
    "4. $\\mathrm{AIC}(X, Y, \\lambda_k)$ を最小化する $k$ ならびに $\\lambda_k$ を求めます。\n",
    "\n",
    "ただし$f(\\lambda_k)$は以下の式で定義されます\n",
    "\n",
    "[en]: #\n",
    "1. Create a list of candidates for $\\{\\mathrm{df}_k\\}$. Typically, only integer values $\\{1,~\\ldots,~N\\}$ are selected as candidates.\n",
    "2. Use Newton's method to solve for $\\lambda_k$ such that $f(\\lambda_k)=0$.\n",
    "3. Compute  $\\mathrm{AIC}(X, Y, \\lambda_k)$ using $\\lambda_k$.\n",
    "4. Find the $k$ and $\\lambda_k$ that minimizes $\\mathrm{AIC}(X, Y, \\lambda_k)$.\n",
    "\n",
    "Here, the function $f(\\lambda_k)$ is defined as\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(\\lambda_k) = \\mathrm{df}_k - \\sum_{i=0}^{N-1}\\frac{\\sigma_{i}^2}{\\sigma_{i}^2+\\lambda_k}\n",
    ".\\end{align*}\n",
    "$$\n",
    "\n",
    "[ja]: #\n",
    "またニュートン法では以下の式で与えられる導関数 $\\dfrac{df}{d\\lambda_k}$ を使用します。\n",
    "\n",
    "[en]: #\n",
    "Also, the derivative $\\dfrac{df}{d\\lambda_k}$ used in Newton's method is given by\n",
    "\n",
    "[END]: #\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\dfrac{df}{d\\lambda_k}(\\lambda_k) = \\sum_{i=0}^{N-1}\\frac{\\sigma_{i}^2}{\\left(\\sigma_{i}^2+\\lambda_k\\right)^2}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5.2.\n",
    "\n",
    "[ja]: #\n",
    "上記のアルゴリズムを実装するため、穴埋めを実装し`calc_df_and_lambda`・`calc_aic`・`AutoRidgeReadout`を完成させよ。\n",
    "\n",
    "[en]: #\n",
    "Fill in the blanks of the following code to complete `calc_df_and_lambda`, `calc_aic`, and `AutoRidgeReadout`, implementing the algorithm described above.\n",
    "\n",
    "[END]: #\n",
    "- `AutoRidgeReadout.train`\n",
    "  - Argument(s):\n",
    "    - `x`: `np.ndarray`\n",
    "      - `shape`: `(..., time_steps, input_dim)`\n",
    "    - `y`: `np.ndarray`\n",
    "      - `shape`: `(..., time_steps, output_dim)`\n",
    "  - Return(s):\n",
    "    - `self.weight`: `np.ndarray`\n",
    "      - `shape`: `(..., output_dim, input_dim)`\n",
    "    - `self.bias`: `np.ndarray`\n",
    "      - `shape`: `(..., 1, output_dim)`\n",
    "    - `*misc`\n",
    "\n",
    "[ja]: #\n",
    "  - Operation(s):\n",
    "      - `self.weight`の更新 ( $\\mathrm{AIC}$ を最小化する重み )\n",
    "      - `self.bias`の更新 ( $\\mathrm{AIC}$ を最小化するバイアス )\n",
    "      - `self.lmbd`の更新　( $\\mathrm{AIC}$ を最小化する $\\lambda$ )\n",
    "\n",
    "[en]: #\n",
    "  - Operation(s):\n",
    "      - Update `self.weight` with the obtained weight.\n",
    "      - Update `self.bias` with the obtained bias.\n",
    "      - Update `self.lmbd` with the obtained $\\lambda$ minimizing $\\mathrm{AIC}$.\n",
    "\n",
    "[END]: #\n",
    "\n",
    "[tips]: #\n",
    "- [`scipy.optimize`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.newton.html)\n",
    "\n",
    "[/tips]: #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_df_and_lambda(xs: np.array, df_max: int = None, num_cand: int = None, lmbd0=1e-12):\n",
    "    batch_size, dim = xs.shape[:-2], xs.shape[-1]\n",
    "    if df_max is None:\n",
    "        df_max = dim\n",
    "    if num_cand is None:\n",
    "        num_cand = df_max\n",
    "\n",
    "    _left, sigma, _right = np.linalg.svd(xs)\n",
    "    sigma2 = (sigma**2)[..., None, :]  # [*bs, 1, dim]\n",
    "    dfs = np.linspace(0, df_max, num_cand + 1)[1:]  # Candidates for degrees of freedom -> [num_cand]\n",
    "    init_cond = np.full((*batch_size, num_cand), lmbd0)  # Initial condition for λ -> [*bs, num_cand]\n",
    "\n",
    "    def func(lmbd):\n",
    "        # BEGIN `lmbd`: [*bs, num_cand], `sigma2`: [*bs, 1, dim]\n",
    "        return dfs - np.sum(sigma2 / (lmbd[..., None] + sigma2), axis=-1)\n",
    "        # END\n",
    "\n",
    "    def fprime(lmbd):\n",
    "        # BEGIN `lmbd`: [*bs, num_cand], `sigma2`: [*bs, 1, dim]\n",
    "        return np.sum(sigma2 / (lmbd[..., None] + sigma2) ** 2, axis=-1)\n",
    "        # END\n",
    "\n",
    "    # RIGHT_B Solve f(λ) = 0 for using Newton's method. The output `lmbds` should be of shape [*bs, num_cand].\n",
    "    lmbds = scipy.optimize.newton(func, init_cond, fprime)\n",
    "    # RIGHT_E\n",
    "    lmbds[lmbds < 0] = 0  # Remove negative λ due to numerical errors.\n",
    "    return dfs, lmbds\n",
    "\n",
    "\n",
    "def calc_aic(xs, ys, **kwargs):\n",
    "    assert xs.shape[-2] == ys.shape[-2]\n",
    "    *batch_size, length, dim_in = xs.shape\n",
    "    dfs, lmbds = calc_df_and_lambda(xs, **kwargs)  # dfs: [num_cand], lmbds: [*bs, num_cand]\n",
    "    xs = xs[..., None, :, :]  # [*bs, 1, length, dim_in]\n",
    "    ys = ys[..., None, :, :]  # [*bs, 1, length, dim_in]\n",
    "    xtx = xs.swapaxes(-2, -1) @ xs  # X^T X: [*bs, 1, dim_in, dim_in]\n",
    "    xty = xs.swapaxes(-2, -1) @ ys  # X^T Y: [*bs, 1, dim_in, dim_out]\n",
    "    # RIGHT_B Ridge regression with λ -> [*bs, num_cand, dim_in, dim_out]\n",
    "    sol = np.matmul(np.linalg.pinv(xtx + lmbds[..., None, None] * np.eye(dim_in)), xty)\n",
    "    # RIGHT_E\n",
    "    rss = np.square(xs @ sol - ys).sum(axis=(-2, -1))  # RIGHT RSS: [*bs, num_cand]\n",
    "    aics = length * np.log(rss) + dfs  # RIGHT AIC: [*bs, num_cand]\n",
    "    return dfs, lmbds, sol, rss, aics\n",
    "\n",
    "\n",
    "class AutoRidgeReadout(Linear):\n",
    "    def __init__(self, *args, lmbd: float = 0.0, **kwargs):\n",
    "        super(AutoRidgeReadout, self).__init__(*args, **kwargs)\n",
    "        self.lmbd = lmbd\n",
    "\n",
    "    def train(self, x: np.ndarray, y: np.ndarray, **kwargs):\n",
    "        assert (x.ndim > 1) and (x.shape[-1] == self.input_dim)\n",
    "        assert (y.ndim > 1) and (y.shape[-1] == self.output_dim)\n",
    "        x_biased = np.ones((*x.shape[:-1], x.shape[-1] + 1), dtype=self.dtype)\n",
    "        x_biased[..., 1:] = x\n",
    "        dfs, lmbds, sol, rss, aics = calc_aic(x_biased, y, **kwargs)\n",
    "        # dfs: [num_cand], lmbds: [*bs, num_cand]\n",
    "        # sol: [*bs, num_cand, dim_in, dim_out]\n",
    "        # rss: [*bs, num_cand], aics: [*bs, num_cand]\n",
    "        best_idx = np.argmin(aics, axis=-1)  # RIGHT Extract the index of the best solution minimizing AIC.\n",
    "        sol_best = sol[(*np.indices(best_idx.shape), best_idx)]  # RIGHT Choose the best solution minimizing AIC.\n",
    "        self.lmbd = lmbds[(*np.indices(best_idx.shape), best_idx)]  # RIGHT Save the best λ minimizing AIC.\n",
    "        self.weight = sol_best[..., 1:, :].swapaxes(-2, -1)  # RIGHT Update weight based on `sol_best`.\n",
    "        self.bias = sol_best[..., :1, :]  # RIGHT Update bias based on `sol_best`.\n",
    "        return self.weight, self.bias, dfs, lmbds, sol, rss, aics\n",
    "\n",
    "\n",
    "def solution(dim_in, dim_out, x_train, y_train, x_eval):\n",
    "    # DO NOT CHANGE HERE.\n",
    "    readout = AutoRidgeReadout(dim_in, dim_out)\n",
    "    readout.train(x_train, y_train)\n",
    "    return readout(x_eval)\n",
    "\n",
    "\n",
    "test_func(solution, \"05_02\")\n",
    "# show_solution(\"05_02\", \"calc_df_and_lambda\")  # Uncomment it to see the solution.\n",
    "# show_solution(\"05_02\", \"calc_aic\")  # Uncomment it to see the solution.\n",
    "# show_solution(\"05_02\", \"AutoRidgeReadout\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "$(\\sigma,\\phi,N,\\rho,T_\\mathrm{train})=(0.05,0.05,500,0.9,2000)$ のケースで実装した`AutoRidgeReadout`の効果を検証してみましょう (時間の短縮のため`num_cand = dim // 10` により $\\{\\mathrm{df}_k\\}=\\{10, 20,~\\ldots,~500\\}$ としています)。\n",
    "\n",
    "[en]: #\n",
    "Let's examine the effectiveness of `AutoRidgeReadout` with the case of $(\\sigma,\\phi,N,\\rho,T_\\mathrm{train})=(0.05,0.05,500,0.9,2000)$.\n",
    "To reduce computation time, we set `num_cand = dim // 10` and $\\{\\mathrm{df}_k\\}=\\{10, 20,~\\ldots,~500\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_setup, seed_dataset = 1234, 5678  # Please choose your favorite seeds.\n",
    "sigma, phi = 0.05, 0.05  # vs \\in [0.0, 0.1]\n",
    "dim, rho = 500, 0.9\n",
    "dataset_info = dict(t_washout=100, t_train=2000, t_eval=1000)\n",
    "\n",
    "ts, us, ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "vs = sigma * us + phi\n",
    "\n",
    "w_in, net, w_out = create_setup(seed_setup, dim, rho, f=np.tanh, cls=AutoRidgeReadout)\n",
    "\n",
    "x0 = np.zeros(net.dim)\n",
    "xs = sample_dynamics(x0, w_in, net, ts, vs, display=True)\n",
    "nrmse, _weight, _bias, dfs, lmbds, sol, rss, aics = eval_nrmse(\n",
    "    xs, ys, w_out, time_info, return_out=True, df_max=dim, num_cand=dim // 10\n",
    ")\n",
    "\n",
    "best_idx = np.argmin(aics)\n",
    "rel_aics = aics - aics.min() + 1e0  # Normalize AICs for better visualization.\n",
    "y_out = w_out(xs)\n",
    "\n",
    "plot_length = 200\n",
    "fig = Figure(figsize=(8, 8))\n",
    "fig.create_grid(2, 1, height_ratios=(2, 1), hspace=0.4)\n",
    "ax0, ax1 = fig[0], fig[1]\n",
    "ax0.create_grid(2, 1, wspace=0.4)\n",
    "ax0[0].plot(dfs, lmbds)\n",
    "ax0[0].scatter(dfs[best_idx], lmbds[best_idx], s=100.0, marker=\"*\")\n",
    "ax0[0].set_yscale(\"log\")\n",
    "ax0[0].set_ylabel(r\"$\\lambda$\")\n",
    "ax0[0].set_xticklabels([])\n",
    "ax0[1].plot(dfs, rel_aics)\n",
    "ax0[1].scatter(dfs[best_idx], rel_aics[best_idx], s=100.0, marker=\"*\")\n",
    "ax0[1].set_ylabel(r\"relative $\\mathrm{AIC}$\")\n",
    "ax0[1].set_yscale(\"log\")\n",
    "info_str = r\"best: $\\mathrm{{df}}={:.0f},~\\lambda=10^{{{:.2f}}}$\".format(dfs[best_idx], np.log10(lmbds[best_idx]))\n",
    "ax0.set_title(info_str)\n",
    "ax0[1].set_xlabel(\"df\")\n",
    "\n",
    "ax1.set_xlabel(r\"$\\mathrm{{df}}$\")\n",
    "ax1.plot(ts[-plot_length:], ys[-plot_length:], color=\"k\", ls=\":\", lw=1.5)\n",
    "ax1.plot(ts[-plot_length:], y_out[-plot_length:], lw=1.5, color=\"red\")\n",
    "ax1.set_title(\"NRMSE={:.3e}\".format(nrmse[0]))\n",
    "ax1.set_ylabel(r\"$y[k]$ & $\\hat{y}[k]$\")\n",
    "ax1.set_xlabel(\"time steps\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "線型回帰 ( $\\lambda=0$ ) やUnderfitting ( $\\lambda=10^{-2}$ ) のケースと`AutoRidgeReadout` を比較してみましょう。\n",
    "\n",
    "[en]: #\n",
    "Try comparing the results of `AutoRidgeReadout` with linear regression ( $\\lambda=0$ ) and the underfitting case ( $\\lambda=10^{-2}$ )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = 20\n",
    "seed_setup, seed_dataset = 1234, 5678\n",
    "t_trains = [100, 200, 400, 800, 1600, 3200]\n",
    "sigma, phi = 0.05, 0.05  # vs \\in [0.0, 0.1]\n",
    "dim, rho = 50, 0.9\n",
    "dataset_info = dict(t_washout=100, t_train=max(t_trains), t_eval=1000)\n",
    "ridge_parameters = [0.0, 1e-2, None]  # None: `AutoRidgeReadout`\n",
    "\n",
    "ts, us, ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "w_in_ws, net_ws = [], []\n",
    "for seed in trange(sample_num):  # Create setups for `sample_num` times.\n",
    "    w_in, net, _w_out = create_setup(seed_setup + seed, dim, rho, f=np.tanh)\n",
    "    w_in_ws.append(w_in.weight)\n",
    "    net_ws.append(net.weight)\n",
    "\n",
    "w_in_batch = Linear(1, dim)\n",
    "w_in_batch.weight = np.array(w_in_ws)[:, None, :, :]  # [bs, 1, N, 1] (-> [bs, 1, N])\n",
    "net_batch = ESN(dim, sr=rho)\n",
    "net_batch.weight = np.array(net_ws)  # [bs, N, N] ([bs, 1, N] -> [bs, 1, N])\n",
    "x0 = np.zeros((sample_num, 1, net.dim))\n",
    "vs = sigma * us + phi\n",
    "xs = sample_dynamics(x0, w_in_batch, net_batch, ts, vs, display=True)\n",
    "\n",
    "nrmse_dict = {lmbd: {} for lmbd in ridge_parameters}\n",
    "lbmd_dict = {lmbd: {} for lmbd in ridge_parameters}\n",
    "for lmbd, t_train in tqdm(list(itertools.product(ridge_parameters, t_trains))):\n",
    "    time_info[\"t_washout\"] = dataset_info[\"t_washout\"] + max(t_trains) - t_train\n",
    "    if lmbd is None:\n",
    "        w_out = AutoRidgeReadout(dim, 1)\n",
    "        train_kws = dict(df_max=dim)\n",
    "    else:\n",
    "        w_out = RidgeReadout(dim, 1, lmbd=np.array(lmbd))\n",
    "        train_kws = {}\n",
    "    nrmse = eval_nrmse(xs, ys, w_out, time_info=time_info, **train_kws)\n",
    "    nrmse_dict[lmbd][t_train] = np.array(nrmse)\n",
    "    lbmd_dict[lmbd][t_train] = np.array(w_out.lmbd)\n",
    "    # print('t_train={}: λ: {:.3e}±{:.3e}, NRMSE: {:.3e}±{:.3e}'.format(\n",
    "    #     t_train, w_out.lmbd.mean(), w_out.lmbd.std(), nrmse.mean(), nrmse.std()))\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8, 8), gridspec_kw=dict(height_ratios=(1, 2), hspace=0.05))\n",
    "for lmbd in ridge_parameters:\n",
    "    nrmse_aves = [nrmse.mean() for nrmse in nrmse_dict[lmbd].values()]\n",
    "    nrmse_stds = [nrmse.std() for nrmse in nrmse_dict[lmbd].values()]\n",
    "    lmbd_aves = [lmbd.mean() for lmbd in lbmd_dict[lmbd].values()]\n",
    "    lmbd_stds = [lmbd.std() for lmbd in lbmd_dict[lmbd].values()]\n",
    "    if lmbd is None:\n",
    "        plot_kws = dict(label=r\"AutoRidge\", color=\"red\", ls=\"--\", lw=2.0)\n",
    "        ax[0].errorbar(t_trains, lmbd_aves, lmbd_stds, **plot_kws)\n",
    "    elif lmbd == 0.0:\n",
    "        plot_kws = dict(label=r\"LR ($\\lambda=0$)\", color=\"k\", ls=\":\")\n",
    "    else:\n",
    "        plot_kws = dict(label=r\"Ridge ($\\lambda=10^{{{:.0f}}}$)\".format(np.log10(lmbd)))\n",
    "    ax[1].errorbar(t_trains, nrmse_aves, nrmse_stds, **plot_kws)\n",
    "ax[0].set_xscale(\"log\")\n",
    "ax[0].set_xticklabels([])\n",
    "ax[0].set_ylabel(r\"$\\lambda$\")\n",
    "ax[0].grid(True, which=\"both\")\n",
    "ax[1].set_xlabel(r\"$T_\\mathrm{train}$\")\n",
    "ax[1].set_ylabel(\"NRMSE\")\n",
    "ax[1].set_xscale(\"log\")\n",
    "ax[1].set_yscale(\"log\")\n",
    "ax[1].legend(loc=\"upper right\", borderaxespad=0, ncol=1, frameon=False)\n",
    "ax[1].grid(True, which=\"both\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5.3. (Advanced)\n",
    "\n",
    "[ja]: #\n",
    "- $\\sigma$ や $\\rho$ を変化させて、`AutoRidgeReadout` のロバスト性を確認せよ。\n",
    "- 様々な $\\lambda$ による精度を `RidgeReadout` によって検証し、 `AutoRidgeReadout`のそれと比較せよ。\n",
    "- 必ずしも `AutoRidgeReadout`は評価誤差 (NRMSE) を最小にする $\\lambda$ を選択できるわけではない。この理由を考察せよ。\n",
    "- ベイズ情報量規準 (Bayesian Information Criterion; BIC) 等、他にも情報量規準は存在する。これら他の情報規準量を最小化するコードを実装し、AICと比較せよ。\n",
    "\n",
    "[en]: #\n",
    "- Test the robustness of `AutoRidgeReadout`, by changing values of $\\sigma$ or $\\rho$.\n",
    "- Validate the performance for various values of $\\lambda$ using `RidgeReadout`, and compare it with the performance of `AutoRidgeReadout`.\n",
    "- `AutoRidgeReadout` does not necessarily choose $\\lambda$ that minimizes the cost (NRMSE). Explain why.\n",
    "- There exist other information criteria, such as the Bayesian Information Criterion (BIC). Implement models that minimize these other critieria, and compare their performance with that of AIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "### 6. グリッドサーチ\n",
    "\n",
    "[en]: #\n",
    "### 6. Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "ここでは複数パラメータをまとめて探索しベストな組み合わせを得る**グリッドサーチ**の方法を学びます。\n",
    "グリッドサーチでは、予め与えられたハイパーパラメータの候補の**全て**の組み合わせを網羅的に探索されます。\n",
    "例えば、入力スケールとバイアス、スペクトル半径の3つのパラメータ $(\\sigma,\\phi,\\rho)$ のベストな組み合わせを探索したいとします。\n",
    "それぞれに関して以下に記される範囲を候補として考えます。\n",
    "\n",
    "[en]: #\n",
    "Let's learn the method of **grid search**, which explores multiple parameters simultaneously to find the best combination.\n",
    "In grid search, **all** combinations of hyperparameter candidate values are exhaustively searched.\n",
    "For example, let's suppose we want to find the best combination of three parameters $(\\sigma,\\phi,\\rho)$: input scale, bias, and spectral radius.\n",
    "The following parameter values are set as candidates:\n",
    "\n",
    "[END]: #\n",
    "- $\\sigma$ : $(0.05,0.10,0.15,~\\ldots~,1.00)$\n",
    "- $\\phi$ : $(0.00,0.05,0.10,~\\ldots~,1.00)$\n",
    "- $\\rho$ : $(0.6,0.7,0.8,0.9)$\n",
    "\n",
    "[ja]: #\n",
    "この際、$20\\times 21\\times 4=840$ 個の組み合わせが考えられ、グリッドサーチではその全てが検証されます。\n",
    "なおこのグリッドサーチでは先程より登場したバッチ処理が有効です。\n",
    "ここでもグリッドサーチに向けた、$u[k]$ ならびにESN初期値の指定方法を以下の設問で学びましょう。\n",
    "\n",
    "[en]: #\n",
    "In this case, there are $20\\times 21\\times 4=840$ total combinations, and all combinations are validated exhaustively in grid search.\n",
    "Here, the previously introduced batch processing is very effective.\n",
    "In the exercise below, let's learn how to specify the input $u[k]$ and the initial values of the ESN to conduct grid search efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6.1.\n",
    "\n",
    "[ja]: #\n",
    "入力時系列 $U=\\{u[k]\\} \\in \\mathbb{R}^{T\\times 1}$ 、\n",
    "長さ $k$ の入力スケール配列 $\\Sigma = (\\sigma_0$, $\\sigma_1,~\\ldots,~\\sigma_{k-1})$ 、\n",
    "長さ $l$ のバイアスを格納した配列 $\\Phi = (\\phi_0$, $\\phi_1,~\\ldots,~\\phi_{l-1})$\n",
    "ならびに 長さ $m$ のスペクトル半径を格納した配列 $\\Rho = (\\rho_0$, $\\rho_1,~\\ldots,~\\rho_{m-1})$ が与えられる。\n",
    "同じ $W^\\mathrm{in}, W^\\mathrm{rec}$ に対して 全部で $k\\times l\\times m$ 個の条件をまとめて $x[k] \\in \\mathbb{R}^{k\\times l\\times m\\times N}$ としてサンプリングするために $v[k]$ を生成し $\\Rho$ を変形する`create_grid_search_setup` を完成させよ。\n",
    "\n",
    "[en]: #\n",
    "The following arguments are given: input sequence $U=\\{u[k]\\} \\in \\mathbb{R}^{T\\times 1}$, an array of length $k$ containing input scaling $\\Sigma = (\\sigma_0$, $\\sigma_1,~\\ldots,~\\sigma_{k-1})$, an array of length $l$ containing biases $\\Phi = (\\phi_0$, $\\phi_1,~\\ldots,~\\phi_{l-1})$, an array of length $m$ containing spectral radii $\\Rho = (\\rho_0$, $\\rho_1,~\\ldots,~\\rho_{m-1})$.\n",
    "Fill in the blanks in the following code to complete `create_grid_search_setup`, a function that generates $v[k]$ and reshapes $\\Rho$ to collectively sample all $k\\times l\\times m$ conditions as $x[k] \\in \\mathbb{R}^{k\\times l\\times m\\times N}$, for the same weight intializations of $W^\\mathrm{in}, W^\\mathrm{rec}$ in each experiment.\n",
    "\n",
    "[END]: #\n",
    "- `create_grid_search_setup`\n",
    "  - Argument(s):\n",
    "    - `us`: `np.ndarray`\n",
    "      - `shape`: `(t, 1)`\n",
    "    - `sigma`: `np.ndarray`\n",
    "      - `shape`: `(k,)`\n",
    "    - `phi`: `np.ndarray`\n",
    "      - `shape`: `(l,)`\n",
    "    - `rho`: `np.ndarray`\n",
    "      - `shape`: `(m,)`\n",
    "  - Return(s):\n",
    "    - `vs`: `np.ndarray`\n",
    "      - `shape`: `(k, l, 1, t, 1)`\n",
    "    - `rho_new`: `np.ndarray`\n",
    "      - `shape`: `(m, 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid_search_setup(us, sigma, phi, rho):\n",
    "    vs = sigma[:, None, None, None, None] * us + phi[:, None, None, None]  # RIGHT\n",
    "    rho_new = rho[:, None]  # RIGHT\n",
    "    return vs, rho_new\n",
    "\n",
    "\n",
    "test_func(create_grid_search_setup, \"06_01\", multiple_output=True)\n",
    "# show_solution(\"06_01\", \"create_grid_search_setup\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "以下のコードは $840$ 個の組み合わせに対してまとめてNRMSEを評価するコードです (環境によっては描画まで数分かかったり、メモリ不足により実行できないかもしれません。あまりに時間がかかる場合は `dim` や `t_train`を小さくしたり、`sigmas`・`phis`・`rhos`の間隔を大きくしてください)。\n",
    "\n",
    "[en]: #\n",
    "The following code evaluates the NRMSE for all $840$ parameter combinations.\n",
    "Depending on your environment, it may take around 2 to 3 minutes to complete the plot.\n",
    "If it takes too long, consider reducing `dim` and `t_train`, or increasing the spacing between parameter values in `sigmas`, `phis`, and `rhos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_setup, seed_dataset = 1234, 5678\n",
    "dim = 50\n",
    "sigmas = np.linspace(0.0, 1.0, 21)[1:]  # [0.05, 0.10, ...., 1.00]\n",
    "phis = np.linspace(0.0, 1.0, 21)  # [0.0, 0.05,, ...., 1.00]\n",
    "rhos = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "dataset_info = dict(t_washout=100, t_train=2000, t_eval=1000)\n",
    "ts, us, ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "vs_batch, rhos_batch = create_grid_search_setup(us, sigmas, phis, rhos)  # Use `create_grid_search_setup`.\n",
    "\n",
    "w_in, net, w_out = create_setup(seed_setup, dim, rhos_batch, f=np.tanh)\n",
    "x0 = np.zeros((sigmas.shape[0], phis.shape[0], rhos.shape[0], net.dim))\n",
    "xs = sample_dynamics(x0, w_in, net, ts, vs_batch, display=True)\n",
    "nrmse = eval_nrmse(xs, ys, w_out, time_info)[..., 0]  # [k, l, m]\n",
    "\n",
    "top_num = 5\n",
    "best_ids = np.array(np.unravel_index(np.argsort(nrmse, axis=None)[:top_num], nrmse.shape)).T\n",
    "for idx, pos in enumerate(best_ids):\n",
    "    print(\n",
    "        \"#{}: NRMSE={:.2e}, σ={:.2e}, Φ={:.2e}, ρ={:.2e}\".format(\n",
    "            idx + 1, nrmse[(*pos,)], sigmas[pos[0]], phis[pos[1]], rhos[pos[2]]\n",
    "        )\n",
    "    )\n",
    "\n",
    "grid_num = (len(rhos) ** 0.5).__ceil__()\n",
    "fig = Figure(figsize=(6 * grid_num, 5 * grid_num))\n",
    "fig.create_grid(grid_num, grid_num, hspace=0.5, wspace=0.5)\n",
    "\n",
    "vmax = min(1.0, np.max(nrmse))\n",
    "vmin = max(0.1, np.min(nrmse))\n",
    "for idx, rho in enumerate(rhos):\n",
    "    fig[idx].plot_matrix(\n",
    "        nrmse[..., idx],\n",
    "        index=sigmas,\n",
    "        column=phis,\n",
    "        vmax=vmax,\n",
    "        vmin=vmin,\n",
    "        cmap=\"viridis\",\n",
    "        zscale=\"log\",\n",
    "        origin=\"lower\",\n",
    "    )\n",
    "    fig[idx].set_xlabel(r\"$\\phi$\")\n",
    "    fig[idx].set_ylabel(r\"$\\sigma$\")\n",
    "    fig[idx].set_title(r\"$\\rho={:.2f}$\".format(rho))\n",
    "\n",
    "px, py, idx = best_ids[0]\n",
    "fig[idx].scatter(py, px, s=200.0, marker=\"*\", color=\"magenta\")\n",
    "fig.suptitle(\n",
    "    r\"best NRMSE={:.2e}, $(\\sigma,\\phi,\\rho)=$({:g},{:g},{:g})\".format(\n",
    "        nrmse[(*best_ids[0],)], sigmas[px], phis[py], rhos[idx]\n",
    "    )\n",
    ")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "グリッドサーチは組み合わせの数に比例する計算量が発生します。\n",
    "したがってハイパーパラメータの数が増えると、計算が現実的でなかったりメモリ不足が発生する場合があります。\n",
    "一般的にはパラメータを粗く設定した後に、より詳細にパラメータを探索する戦略が取られる場合があります。\n",
    "\n",
    "様々な方法がありますが、最も簡便な例として以下のアルゴリズムが考えられます。\n",
    "1. パラメータの分割数 $p$ を決定\n",
    "2. 最適化するパラメータ $\\theta$ の最小値 $\\theta_\\mathrm{min}$ と最大値 $\\theta_\\mathrm{max}$ を指定\n",
    "3. 探索される $p$ 個の パラメータ $\\theta_{k} = \\theta_\\mathrm{min} + (k + 0.5)(\\theta_\\mathrm{max}-\\theta_\\mathrm{min})/p~(0 \\leq k < p)$ を用意\n",
    "4. 各 $\\theta_k$ に対して目的関数 $\\mathcal{L}(\\theta_k)$ を計算\n",
    "5. $\\mathcal{L}(\\theta_k)$ を最小化する $\\hat{k}:=\\mathrm{argmin}_k \\mathcal{L}(\\theta_k)$ を計算\n",
    "6. $\\theta_\\mathrm{min}\\leftarrow \\theta_\\mathrm{min}+\\hat{k}(\\theta_\\mathrm{max}-\\theta_\\mathrm{min})/p$\n",
    "7. $\\theta_\\mathrm{max}\\leftarrow \\theta_\\mathrm{min}+(\\hat{k}+1)(\\theta_\\mathrm{max}-\\theta_\\mathrm{min})/p$\n",
    "8. 3.に戻る\n",
    "\n",
    "無論このアルゴリズムは $\\mathcal{L}$ の値の変化が パラメータの分割間隔と比して十分に「滑らか」でないと機能しませんが、実装が簡単なので大雑把に良いパラメータの組み合わせを調査するのに使用できます。\n",
    "次の設問でこのアルゴリズムの重要な箇所を実装しましょう。\n",
    "\n",
    "[en]: #\n",
    "Grid search requires computation proportional to the number of parameter combinations.\n",
    "Therefore, as the number of hyperparameters increases, the calculations may become impractical or result in memory shortages.\n",
    "A common strategy is to first set the parameters coarsely and then search more finely.\n",
    "\n",
    "There are various methods, but the following algorithm is one simple example:\n",
    "1. Decide the number of divisions $p$ for the parameters.\n",
    "2. Specify the minimum value $\\theta_\\mathrm{min}$ and maximum value $\\theta_\\mathrm{max}$ of the parameter $\\theta$ to be optimized.\n",
    "3. Prepare $p$ parameter values to be searched, $\\theta_{k} = \\theta_\\mathrm{min} + (k + 0.5)(\\theta_\\mathrm{max}-\\theta_\\mathrm{min})/p~(0 \\leq k < p)$.\n",
    "4. Calculate the objective function $\\mathcal{L}(\\theta_k)$ for each $\\theta_k$.\n",
    "5. Obtain $\\hat{k}:=\\mathrm{argmin}_k \\mathcal{L}(\\theta_k)$ that minimizes $\\mathcal{L}(\\theta_k)$.\n",
    "6. Update $\\theta_\\mathrm{min}\\leftarrow \\theta_\\mathrm{min}+\\hat{k}(\\theta_\\mathrm{max}-\\theta_\\mathrm{min})/p$.\n",
    "7. Update $\\theta_\\mathrm{max}\\leftarrow \\theta_\\mathrm{min}+(\\hat{k}+1)(\\theta_\\mathrm{max}-\\theta_\\mathrm{min})/p$.\n",
    "8. Return to step 3.\n",
    "\n",
    "This algorithm works only when the change of $\\mathcal{L}$ is sufficiently smooth relative to the parameter division intervals.\n",
    "However, it is simple to implement and useful for a rough search of good parameter combinations.\n",
    "In the exercise below, let's implement the key parts of this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6.2.\n",
    "\n",
    "[ja]: #\n",
    "実数  $\\theta_\\mathrm{min}, \\theta_\\mathrm{max}$ と分割数を表す正の整数 $p$ が与えられる。\n",
    "長さ $p$ の配列 $\\{\\theta_{k}\\}$ を生成する`create_parameter_set`を実装せよ。\n",
    "\n",
    "[en]: #\n",
    "Real numbers $\\theta_\\mathrm{min}$ and $\\theta_\\mathrm{max}$, and a positive integer $p$ representing the number of parameter divisions are given.\n",
    "Fill in the blanks in the following code to complete `create_parameter_set`, a function that generates an array $\\{\\theta_{k}\\}$ of length $p$ containing candidate parameter values.\n",
    "\n",
    "[END]: #\n",
    "- `create_parameter_set`\n",
    "  - Argument(s):\n",
    "    - `th_min`: `float`\n",
    "    - `th_max`: `float`\n",
    "    - `num_split` : `int`\n",
    "  - Return(s):\n",
    "    - `th_new`: `np.ndarray`\n",
    "      - `shape`: `(num_split,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parameter_set(th_min, th_max, num_split):\n",
    "    th_new = th_min + ((np.arange(num_split) + 0.5) * (th_max - th_min)) / num_split  # RIGHT\n",
    "    return th_new\n",
    "\n",
    "\n",
    "test_func(create_parameter_set, \"06_02\")\n",
    "# show_solution(\"06_02\", \"create_parameter_set\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "以下のコードは 下記の範囲のパラメータを分割数 5 (`num_split`) で 10回 (`num_iteration`) 繰り返し探索します。\n",
    "\n",
    "[en]: #\n",
    "The following code searches the parameter ranges with 5 divisions (`num_split`) over 10 iterations (`num_iteration`).\n",
    "\n",
    "[END]: #\n",
    "- $\\sigma\\in (0.0, 1.0)$\n",
    "- $\\phi\\in (0.0, 1.0)$\n",
    "- $\\rho\\in (0.0, 2.0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_split = 5\n",
    "num_iteration = 10\n",
    "seed_setup, seed_dataset = 1234, 5678\n",
    "dim = 50\n",
    "parameter_range = dict(\n",
    "    sigma=(0.0, 1.0),\n",
    "    phi=(0.0, 1.0),\n",
    "    rho=(0.0, 2.0),\n",
    ")\n",
    "dataset_info = dict(t_washout=100, t_train=2000, t_eval=1000)\n",
    "ts, us, ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "\n",
    "for idx in range(num_iteration):\n",
    "    # print(parameter_range)\n",
    "    sigmas, phis, rhos = map(lambda v: create_parameter_set(*v, num_split), parameter_range.values())\n",
    "    # print(sigmas, phis, rhos)\n",
    "    vs_batch, rhos_batch = create_grid_search_setup(us, sigmas, phis, rhos)\n",
    "    w_in, net, w_out = create_setup(seed_setup, dim, rhos_batch, f=np.tanh)\n",
    "    x0 = np.zeros((sigmas.shape[0], phis.shape[0], rhos.shape[0], net.dim))\n",
    "    xs = sample_dynamics(x0, w_in, net, ts, vs_batch, display=False)\n",
    "    nrmse = eval_nrmse(xs, ys, w_out, time_info)[..., 0]  # [k, l, m]\n",
    "    best_ids = np.unravel_index(np.argmin(nrmse, axis=None), nrmse.shape)\n",
    "    print(\n",
    "        \"#{:>2}: NRMSE={:.6e}, σ={:g}, Φ={:g}, ρ={:g}\".format(\n",
    "            idx + 1, nrmse[(*best_ids,)], sigmas[best_ids[0]], phis[best_ids[1]], rhos[best_ids[2]]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    parameter_set_new = dict()\n",
    "    for pos, (key, param_range) in zip(best_ids, parameter_range.items(), strict=False):\n",
    "        vmin, vmax = param_range\n",
    "        vmin_new = vmin + (vmax - vmin) * pos / num_split\n",
    "        vmax_new = vmin + (vmax - vmin) * (pos + 1) / num_split\n",
    "        parameter_set_new[key] = (vmin_new, vmax_new)\n",
    "    parameter_range = parameter_set_new\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6.3. (Advanced)\n",
    "\n",
    "[ja]: #\n",
    "- $N$ や $p$ を大きくすると環境によってはメモリ不足が発生する場合がある。メモリ不足を回避しながら実行できるようにコードを改変せよ。\n",
    "- $N$ や $p$ を変化させこのアルゴリズムの長所と短所を考察せよ。\n",
    "- よりロバストなハイパーパラメータ探索の方法を調査し、実装に組み込め。\n",
    "\n",
    "[en]: #\n",
    "- Increasing $N$ or $p$ may cause memory shortage in some environments.\n",
    "Modify the code to avoid running out of memory.\n",
    "- Investigate the strengths and weaknesses of this algorithm by varying $N$ and $p$.\n",
    "- Explore more robust methods for hyperparameter search and incorporate them into the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6.4. (Advanced)\n",
    "\n",
    "[ja]: #\n",
    "- 今回学んだパラメータの探索方法を活用し、物理リザバーの研究に応用せよ。\n",
    "- 例えば空気圧人工筋肉 (Pneumatic Artificial Muscle; PAM)<sup>[3, 4]</sup> の研究で公開されているデータセットを用いて、今回のセットアップで検証せよ。\n",
    "\n",
    "[en]: #\n",
    "- Apply the parameter search methods learned here to physical reservoir research.\n",
    "- For example, use the current setup to validate the open dataset from pneumatic artificial muscle (PAM)<sup>[3, 4]</sup> research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ja]: #\n",
    "## 参考文献\n",
    "\n",
    "[en]: #\n",
    "## Reference(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Kubota, T., Takahashi, H., & Nakajima, K. (2021). *Unifying framework for information processing in stochastically driven dynamical systems*. Physical Review Research, 3(4), 043135. https://doi.org/10.1103/PhysRevResearch.3.043135\n",
    "\n",
    "[2] Goto, K., Nakajima, K., & Notsu, H. (2021). *Twin vortex computer in fluid flow*. New Journal of Physics, 23(6), 063051. https://doi.org/10.1088/1367-2630/ac024d\n",
    "\n",
    "[3] Sakurai, R., Nishida, M., Sakurai, H., Wakao, Y., Akashi, N., Kuniyoshi, Y., Minami, Y., & Nakajima, K. (2020). *Emulating a sensor using soft material dynamics: A reservoir computing approach to pneumatic artificial muscle*. 2020 3rd IEEE International Conference on Soft Robotics (RoboSoft), 710–717. https://doi.org/10.1109/RoboSoft48309.2020.9115974\n",
    "\n",
    "[4] Akashi, N., Yamaguchi, T., Tsunegi, S., Taniguchi, T., Nishida, M., Sakurai, R., Wakao, Y., & Nakajima, K. (2020). *Input-driven bifurcations and information processing capacity in spintronics reservoirs*. Physical Review Research, 2(4), 043303. https://doi.org/10.1103/PhysRevResearch.2.043303"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rc-bootcamp (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
